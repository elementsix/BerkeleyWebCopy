<!DOCTYPE html>
<!--==============================================================================
	           "GitHub HTML5 Pandoc Template" v1.2 — by Tristano Ajmone           
	==============================================================================
	(c) Tristano Ajmone, 2017, MIT License (MIT). Project's home repository:

	- https://github.com/tajmone/pandoc-goodies

	This template reuses source code taken from the following projects:

	- GitHub Markdown CSS: © Sindre Sorhus, MIT License (MIT):
	  https://github.com/sindresorhus/github-markdown-css

	- Primer CSS: © 2016 GitHub Inc., MIT License (MIT):
	  http://primercss.io/
	==============================================================================-->
<html>

<!-- Mirrored from www.eecs70.org/static/notes/n17.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:40:58 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Variance</title>
<style type="text/css">@font-face{font-family:octicons-link;src:url(data:font/woff;charset=utf-8;base64,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) format('woff')}.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;color:#24292e;font-family:-apple-system,system-ui,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:16px;line-height:1.5;word-wrap:break-word;box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto;padding:45px}.markdown-body .octicon{display:inline-block;fill:currentColor;vertical-align:text-bottom}.markdown-body a{background-color:transparent;-webkit-text-decoration-skip:objects;color:#0366d6;text-decoration:none}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body h1{margin:.67em 0}.markdown-body img{border-style:none}.markdown-body svg:not(:root){overflow:hidden}.markdown-body code,.markdown-body kbd,.markdown-body pre{font-family:monospace,monospace}.markdown-body input{font:inherit;margin:0;overflow:visible;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=checkbox]{box-sizing:border-box;padding:0}.markdown-body *{box-sizing:border-box}.markdown-body a:hover{text-decoration:underline}.markdown-body strong{font-weight:600}.markdown-body hr{box-sizing:content-box;overflow:hidden;background:0 0;border-bottom:1px solid #dfe2e5}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body table{border-spacing:0;border-collapse:collapse;display:block;width:100%;overflow:auto}.markdown-body td,.markdown-body th{padding:0}.markdown-body blockquote{margin:0}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body dd{margin-left:0}.markdown-body code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace}.markdown-body pre{font:12px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;word-wrap:normal}.markdown-body .pl-0{padding-left:0!important}.markdown-body .pl-1{padding-left:4px!important}.markdown-body .pl-2{padding-left:8px!important}.markdown-body .pl-3{padding-left:16px!important}.markdown-body .pl-4{padding-left:24px!important}.markdown-body .pl-5{padding-left:32px!important}.markdown-body .pl-6{padding-left:40px!important}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:0}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e1e4e8;border:0}.markdown-body blockquote{padding:0 1em;color:#6a737d;border-left:.25em solid #dfe2e5}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd{font-size:11px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#1b1f23;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1{padding-bottom:.3em;font-size:2em;border-bottom:1px solid #eaecef}.markdown-body h2{padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid #eaecef}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#6a737d}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #dfe2e5}.markdown-body table tr{background-color:#fff;border-top:1px solid #c6cbd1}.markdown-body table tr:nth-child(2n){background-color:#f6f8fa}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body code{padding:.2em 0;margin:0;font-size:85%;background-color:rgba(27,31,35,.05);border-radius:3px}.markdown-body code::after,.markdown-body code::before{letter-spacing:-.2em;content:"\00a0"}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f6f8fa;border-radius:3px}.markdown-body pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code::after,.markdown-body pre code::before{content:normal}.markdown-body .full-commit .btn-outline:not(:disabled):hover{color:#005cc5;border-color:#005cc5}.markdown-body kbd{display:inline-block;padding:3px 5px;font:11px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fcfcfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body :checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item+.task-list-item{margin-top:3px}.markdown-body .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown-body hr{border-bottom-color:#eee}.flash{position:relative;padding:16px;color:#246;background-color:#e2eef9;border:1px solid #bac6d3;border-radius:3px}.flash p:last-child{margin-bottom:0}.flash-messages{margin-bottom:24px}.flash-warn{color:#4c4a42;background-color:#fff9ea;border-color:#dfd8c2}.flash-error{color:#911;background-color:#fcdede;border-color:#d2b2b2}.flash-success{color:#22662c;background-color:#e2f9e5;border-color:#bad3be}.flash-plain{color:#4c4a42;background-color:#f5f5f5;border-color:#c1c1c1}.figure{text-align:center;}</style>
  <style type="text/css">code{white-space: pre;}</style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {equationNumbers: {autoNumber: "AMS"}}
    });
  </script>
  <script src="../../../cdn.mathjax.org/mathjax/latest/MathJax2ba6.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title">Variance</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#variance">Variance</a><ul>
<li><a href="#examples">Examples</a></li>
</ul></li>
<li><a href="#independent-random-variables">Independent Random Variables</a><ul>
<li><a href="#example">Example</a></li>
</ul></li>
</ul>
</nav>
<h1 id="variance" class="unnumbered">Variance</h1>
<p>We have seen in the previous note that if we toss a coin <span class="math inline">\(n\)</span> times with bias <span class="math inline">\(p\)</span>, then the expected number of heads is <span class="math inline">\(np\)</span>. What this means is that if we repeat the experiment multiple times, where in each experiment we toss the coin <span class="math inline">\(n\)</span> times, then on average we get <span class="math inline">\(np\)</span> heads. But in any single experiment, the number of heads observed can be any value between <span class="math inline">\(0\)</span> and <span class="math inline">\(n\)</span>. What can we say about how far off we are from the expected value? That is, what is the typical deviation of the number of heads from <span class="math inline">\(np\)</span>?</p>
<p><span><strong>Random Walk</strong></span></p>
<p>Let us consider a simpler setting that is equivalent to tossing a fair coin <span class="math inline">\(n\)</span> times, but is more amenable to analysis. Suppose we have a particle that starts at position <span class="math inline">\(0\)</span> and performs a random walk. At each time step, the particle moves either one step to the right or one step to the left with equal probability, and the move at each time step is independent of all other moves. We think of these random moves as taking place according to whether a fair coin comes up heads or tails. The expected position of the particle after <span class="math inline">\(n\)</span> moves is back at <span class="math inline">\(0\)</span>, but how far from <span class="math inline">\(0\)</span> should we typically expect the particle to end up at?</p>
<p>Denoting a right-move by <span class="math inline">\(+1\)</span> and a left-move by <span class="math inline">\(-1\)</span>, we can describe the probability space here as the set of all sequences of length <span class="math inline">\(n\)</span> over the alphabet <span class="math inline">\(\{\pm 1\}\)</span>, each having equal probability <span class="math inline">\(2^{-n}\)</span>. Let the r.v. <span class="math inline">\(X\)</span> denote the position of the particle (relative to our starting point <span class="math inline">\(0\)</span>) after <span class="math inline">\(n\)</span> moves. Thus, we can write <a name="eq:randomwalk"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
X=X_1+X_2+\cdots +X_n,\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(1)</span></span>  where <span class="math inline">\(X_i=+1\)</span> if the <span class="math inline">\(i\)</span>-th move is to the right and <span class="math inline">\(X_i = -1\)</span> otherwise.</p>
<p>Now obviously we have <span class="math inline">\({\mathbb{E}}[X]=0\)</span>. The easiest way to see this is to note that <span class="math inline">\({\mathbb{E}}[X_i]=(1/2)\times 1+(1/2)\times(-1)=0\)</span>, so by linearity of expectation <span class="math inline">\({\mathbb{E}}[X]=\sum_{i=1}^n {\mathbb{E}}[X_i]=0\)</span>. But of course this is not very informative, and is due to the fact that positive and negative deviations from <span class="math inline">\(0\)</span> cancel out.</p>
<p>What we are really asking is: What is the expected value of <span class="math inline">\(|X|\)</span>, the <span><em>distance</em></span> of the particle from <span class="math inline">\(0\)</span>? Rather than consider the r.v. <span class="math inline">\(|X|\)</span>, which is a little difficult to work with due to the absolute value operator, we will instead look at the r.v. <span class="math inline">\(X^2\)</span>. Notice that this also has the effect of making all deviations from <span class="math inline">\(0\)</span> positive, so it should also give a good measure of the distance from <span class="math inline">\(0\)</span>. However, because it is the <span><em>squared</em></span> distance, we will need to take a square root at the end.</p>
<p>We will now show that the expected square distance after <span class="math inline">\(n\)</span> steps is equal to <span class="math inline">\(n\)</span>:</p>
<p><span id="proposition:1" class="pandoc-numbering-text proposition"><strong>Proposition 1</strong></span></p>
<p><em>For the random variable <span class="math inline">\(X\)</span> defined in</em> Equation <a href="#eq:randomwalk">1</a>, <em>we have <span class="math inline">\({\mathbb{E}}[X^2] = n\)</span>.</em></p>
<p><em>Proof</em>. We use the expression Equation <a href="#eq:randomwalk">1</a> and expand the square: <span class="math display">\[\begin{aligned} {\mathbb{E}}[X^2]&amp;={\mathbb{E}}[(X_1+X_2+\cdots+X_n)^2]\cr
                 &amp;={\mathbb{E}}\!\!\left[\sum_{i=1}^n X_i^2 + \sum_{i\ne j} X_iX_j\right]\cr
                 &amp;=\sum_{i=1}^n {\mathbb{E}}[X_i^2] + \sum_{i\ne j}{\mathbb{E}}[X_iX_j]\end{aligned}\]</span> In the last line we have used linearity of expectation. To proceed, we need to compute <span class="math inline">\({\mathbb{E}}[X_i^2]\)</span> and <span class="math inline">\({\mathbb{E}}[X_iX_j]\)</span> (for <span class="math inline">\(i\ne j\)</span>). Let’s consider first <span class="math inline">\(X_i^2\)</span>. Since <span class="math inline">\(X_i\)</span> can take on only values <span class="math inline">\(\pm 1\)</span>, clearly <span class="math inline">\(X_i^2=1\)</span> always, so <span class="math inline">\({\mathbb{E}}[X_i^2]=1\)</span>. What about <span class="math inline">\({\mathbb{E}}[X_iX_j]\)</span>? Well, <span class="math inline">\(X_iX_j=+1\)</span> when <span class="math inline">\(X_i=X_j=+1\)</span> or <span class="math inline">\(X_i=X_j=-1\)</span>, and otherwise <span class="math inline">\(X_iX_j=-1\)</span>. Therefore, <span class="math display">\[\begin{aligned}{\mathbb{P}}[X_i X_j = 1]
    &amp;= {\mathbb{P}}[(X_i=X_j=+1) \vee (X_i=X_j=-1)] \cr
    &amp;= {\mathbb{P}}[X_i=X_j=+1] \:+\: {\mathbb{P}}[X_i=X_j=-1] \cr
    &amp;= {\mathbb{P}}[X_i = +1] \times {\mathbb{P}}[X_j = +1] \:+\: {\mathbb{P}}[X_i = -1] \times {\mathbb{P}}[X_j = -1] \cr
    &amp;= \frac{1}{4}+\frac{1}{4} 
    = \frac{1}{2},\end{aligned}\]</span> where in the above calculation we used the fact that the events <span class="math inline">\(X_i=+1\)</span> and <span class="math inline">\(X_j=+1\)</span> are independent, and similarly the events <span class="math inline">\(X_i=-1\)</span> and <span class="math inline">\(X_j=-1\)</span> are independent. Thus <span class="math inline">\({\mathbb{P}}[X_iX_j=-1] = 1/2\)</span> as well, and hence <span class="math inline">\({\mathbb{E}}[X_iX_j]=0\)</span>.</p>
<p>Plugging these values into the above equation gives <span class="math display">\[{\mathbb{E}}[X^2] = \sum_{i=1}^n 1 + \sum_{i\ne j} 0 = n,\]</span> as claimed. <span class="math inline">\(\square\)</span></p>
<p>So we see that <em>our expected squared distance from <span class="math inline">\(0\)</span> is <span class="math inline">\(n\)</span></em>. One interpretation of this is that we might expect to be a distance of about <span class="math inline">\(\sqrt{n}\)</span> away from <span class="math inline">\(0\)</span> after <span class="math inline">\(n\)</span> steps. However, we have to be careful here: we <span><strong>cannot</strong></span> simply argue that <span class="math inline">\({\mathbb{E}}[|X|]=\sqrt{{\mathbb{E}}[X^2]}=\sqrt{n}\)</span>. (Why not?) We will see later in the lecture how to make precise deductions about <span class="math inline">\(|X|\)</span> from knowledge of <span class="math inline">\({\mathbb{E}}[X^2]\)</span>.</p>
<p>For the moment, however, let’s agree to view <span class="math inline">\({\mathbb{E}}[X^2]\)</span> as an intuitive measure of “spread” of the r.v. <span class="math inline">\(X\)</span>. In fact, for a more general r.v. with expectation <span class="math inline">\({\mathbb{E}}[X]=\mu\)</span>, what we are really interested in is <span class="math inline">\({\mathbb{E}}[(X-\mu)^2]\)</span>, the expected squared distance <span><em>from the mean</em></span>. In our random walk example, we had <span class="math inline">\(\mu=0\)</span>, so <span class="math inline">\({\mathbb{E}}[(X-\mu)^2]\)</span> just reduces to <span class="math inline">\({\mathbb{E}}[X^2]\)</span>.</p>
<p><span id="definition:1" class="pandoc-numbering-text definition"><strong>Definition 1</strong> <em>(Variance)</em></span></p>
<p><em>For a r.v. <span class="math inline">\(X\)</span> with expectation <span class="math inline">\({\mathbb{E}}[X]=\mu\)</span>, the variance of <span class="math inline">\(X\)</span> is defined to be <span class="math display">\[{\operatorname{var}}(X)={\mathbb{E}}[(X-\mu)^2].\]</span> The square root <span class="math inline">\(\sigma(X) :=\sqrt{{\operatorname{var}}(X)}\)</span> is called the standard deviation of <span class="math inline">\(X\)</span>.</em></p>
<p>The point of the standard deviation is merely to “undo” the squaring in the variance. Thus the standard deviation is “on the same scale as” the r.v. itself. Since the variance and standard deviation differ just by a square, it really doesn’t matter which one we choose to work with as we can always compute one from the other immediately. We shall usually use the variance. For the random walk example above, we have that <span class="math inline">\({\operatorname{var}}(X)=n\)</span>, and the standard deviation of <span class="math inline">\(X\)</span>, <span class="math inline">\(\sigma(X)\)</span>, is <span class="math inline">\(\sqrt{n}\)</span>.</p>
<p>The following easy observation gives us a slightly different way to compute the variance that is simpler in many cases.</p>
<p><span id="thm:var" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></p>
<p><em>For a r.v. <span class="math inline">\(X\)</span> with expectation <span class="math inline">\({\mathbb{E}}[X]=\mu\)</span>, we have <span class="math inline">\({\operatorname{var}}(X)={\mathbb{E}}[X^2]-\mu^2\)</span>.</em></p>
<p><em>Proof</em>. From the definition of variance, we have <span class="math display">\[{\operatorname{var}}(X)={\mathbb{E}}[(X-\mu)^2]={\mathbb{E}}[X^2-2\mu X+\mu^2]
                               ={\mathbb{E}}[X^2]-2\mu{\mathbb{E}}[X]+\mu^2
                               ={\mathbb{E}}[X^2]-\mu^2.\]</span> In the third step above, we used linearity of expectation. Moreover, note that <span class="math inline">\(\mu = {\mathbb{E}}[X]\)</span> is a constant, so <span class="math inline">\({\mathbb{E}}[\mu X] = \mu {\mathbb{E}}[X] = \mu^2\)</span> and <span class="math inline">\({\mathbb{E}}[\mu^2] = \mu^2\)</span>. <span class="math inline">\(\square\)</span></p>
<p>Another important property that will come in handy is the following: For any random variable <span class="math inline">\(X\)</span> and constant <span class="math inline">\(c\)</span>, we have <span class="math display">\[{\operatorname{var}}(cX)=c^2{\operatorname{var}}(X).\]</span> The proof is simple and left as an exercise.</p>
<h2 id="examples" class="unnumbered">Examples</h2>
<p>Let’s see some examples of variance calculations.</p>
<ol style="list-style-type: decimal">
<li><p><span><strong>Fair die.</strong></span> Let <span class="math inline">\(X\)</span> be the score on the roll of a single fair die. Recall from the previous note that <span class="math inline">\({\mathbb{E}}[X]=7/2\)</span>. So we just need to compute <span class="math inline">\({\mathbb{E}}[X^2]\)</span>, which is a routine calculation: <span class="math display">\[{\mathbb{E}}[X^2] = {1\over 6}\left(1^2+2^2+3^2+4^2+5^2+6^2\right)={{91}\over 6}.\]</span> Thus from <a href="#thm:var">Theorem 1</a>, <span class="math display">\[{\operatorname{var}}(X)={\mathbb{E}}[X^2]-({\mathbb{E}}[X])^2 = {{91}\over 6} - {{49}\over 4}
                             = {{35}\over{12}}.\]</span></p></li>
<li><p><span><strong>Uniform distribution.</strong></span> More generally, if <span class="math inline">\(X\)</span> is a uniform random variable on the set <span class="math inline">\(\{1,\dots,n\}\)</span>, so <span class="math inline">\(X\)</span> takes on values <span class="math inline">\(1, \ldots, n\)</span> with equal probability <span class="math inline">\(1/n\)</span>, then the mean, variance and standard deviation of <span class="math inline">\(X\)</span> are given by: <a name="eq:uniform"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[\begin{aligned}
{\mathbb{E}}[X] = \frac{n+1}{2}, \qquad {\operatorname{var}}(X) = \frac{n^2-1}{12}, \qquad \sigma(X) = \sqrt{\frac{n^2-1}{12}}.\end{aligned}\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(2)</span></span>  You should verify these as an exercise.</p></li>
<li><p><span><strong>Biased coin.</strong></span> Let <span class="math inline">\(X\)</span> the the number of Heads in <span class="math inline">\(n\)</span> tosses of a biased coin with Heads probability <span class="math inline">\(p\)</span> (i.e., <span class="math inline">\(X\)</span> has the binomial distribution with parameters <span class="math inline">\(n,p\)</span>). We already know that <span class="math inline">\({\mathbb{E}}[X]=np\)</span>. Writing as usual <span class="math inline">\(X=X_1+X_2+\cdots +X_n\)</span>, where <span class="math inline">\(X_i=\begin{cases} 1&amp;\text{if $i$th toss is Head;}\\ 0&amp;\text{otherwise}\end{cases}\)</span> we have <span class="math display">\[\begin{aligned}{\mathbb{E}}[X^2]&amp;={\mathbb{E}}[(X_1+X_2+\cdots+X_n)^2]\cr
                 &amp;=\sum_{i=1}^n {\mathbb{E}}[X_i^2] + \sum_{i\ne j}{\mathbb{E}}[X_iX_j]\cr
                 &amp;=(n\times p) + (n(n-1)\times p^2)\cr
                 &amp;=n^2p^2 + np(1-p).\end{aligned}\]</span> In the third line here, we have used the facts that <span class="math inline">\({\mathbb{E}}[X_i^2]=p\)</span>, and that</p>
<p><span class="math display">\[{\mathbb{E}}[X_iX_j]= {\mathbb{P}}[X_i=X_j = 1]= {\mathbb{P}}[X_i = 1] \cdot {\mathbb{P}}[X_j = 1] = p^2,\]</span></p>
<p>(since <span class="math inline">\(X_i=1\)</span> and <span class="math inline">\(X_j=1\)</span> are independent events). Note that there are <span class="math inline">\(n(n-1)\)</span> pairs <span class="math inline">\(i,j\)</span> with <span class="math inline">\(i\ne j\)</span>.</p>
<p>Finally, we get that <span class="math inline">\({\operatorname{var}}(X)={\mathbb{E}}[X^2]-({\mathbb{E}}[X])^2=np(1-p)\)</span>. Notice that in fact <span class="math inline">\({\operatorname{var}}(X)=\sum_i{\operatorname{var}}(X_i)\)</span>, and the same was true in the random walk example. This is in fact no coincidence. We will explore for what kinds of random variables this is true later in the next lecture.</p>
<p>As an example, for a fair coin the expected number of Heads in <span class="math inline">\(n\)</span> tosses is <span class="math inline">\(n/2\)</span>, and the standard deviation is <span class="math inline">\(\sqrt{n}/2\)</span>. Note that since the maximum number of Heads is <span class="math inline">\(n\)</span>, the standard deviation is much less than this maximum number for large <span class="math inline">\(n\)</span>. This is in contrast to the previous example of the uniformly distributed random variable, where the standard deviation <span class="math display">\[\sigma(X) = \sqrt{(n^2-1)/12} \approx n/\sqrt{12}\]</span> is of the same order as the largest value <span class="math inline">\(n\)</span>. In this sense, the spread of a binomially distributed r.v. is much smaller than that of a uniformly distributed r.v.</p></li>
<li><p><span><strong>Poisson distribution.</strong></span> What is the variance of a Poisson r.v. <span class="math inline">\(X\)</span>? <span class="math display">\[{\mathbb{E}}[X^2] = \sum_{i=0}^\infty i^2 e^{-\lambda}{{\lambda^i}\over{i!}}
            = \lambda\sum_{i=1}^\infty i e^{-\lambda}{{\lambda^{i-1}}\over{(i-1)!}}
            = \lambda\left(\sum_{i=1}^\infty (i-1) e^{-\lambda}{{\lambda^{i-1}}\over{(i-1)!}} + \sum_{i=1}^\infty e^{-\lambda}{{\lambda^{i-1}}\over{(i-1)!}}\right)
            = \lambda(\lambda + 1).\]</span> [Check you follow each of these steps. In the last step, we have noted that the two sums are respectively <span class="math inline">\({\mathbb{E}}[X]\)</span> and <span class="math inline">\(\sum_i{\mathbb{P}}[X=i] = 1\)</span>.]</p>
<p>Finally, we get <span class="math inline">\({\operatorname{var}}(X)={\mathbb{E}}[X^2]-({\mathbb{E}}[X])^2 = \lambda\)</span>. So, for a Poisson random variable, the expectation and variance are equal.</p></li>
<li><p><span><strong>Number of fixed points.</strong></span> Let <span class="math inline">\(X\)</span> be the number of fixed points in a random permutation of <span class="math inline">\(n\)</span> items (i.e., the number of students in a class of size <span class="math inline">\(n\)</span> who receive their own homework after shuffling). We saw in the previous note that <span class="math inline">\({\mathbb{E}}[X]=1\)</span>, regardless of <span class="math inline">\(n\)</span>. To compute <span class="math inline">\({\mathbb{E}}[X^2]\)</span>, write <span class="math inline">\(X=X_1+X_2+\cdots+X_n\)</span>, where <span class="math inline">\(X_i=1\)</span> if <span class="math inline">\(i\)</span> is a fixed point, and <span class="math inline">\(X_i = 0\)</span> otherwise. Then as usual we have <a name="eq:eq1"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   {\mathbb{E}}[X^2] = \sum_{i=1}^n {\mathbb{E}}[X_i^2] + \sum_{i\ne j}{\mathbb{E}}[X_iX_j].\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(3)</span></span>  Since <span class="math inline">\(X_i\)</span> is an indicator r.v., we have that <span class="math inline">\({\mathbb{E}}[X_i^2]={\mathbb{P}}[X_i=1]=1/n\)</span>. Since both <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are indicators, we can compute <span class="math inline">\({\mathbb{E}}[X_iX_j]\)</span> as follows: <span class="math display">\[{\mathbb{E}}[X_iX_j] = {\mathbb{P}}[X_iX_j = 1] = {\mathbb{P}}[X_i=1\wedge X_j=1] = {\mathbb{P}}[\text{both $i$ and $j$
                                                 are fixed points}]
                                        = {1\over{n(n-1)}}.\]</span> Make sure that you understand the last step here. Plugging this into Equation <a href="#eq:eq1">3</a> we get <span class="math display">\[{\mathbb{E}}[X^2] =  \sum_{i=1}^n \frac{1}{n} + \sum_{i\ne j}\frac{1}{n(n-1)} = \left(n\times {1\over n}\right) + \left(n(n-1)\times{1\over{n(n-1)}}\right) = 1+1 = 2.\]</span> Thus <span class="math inline">\({\operatorname{var}}(X)={\mathbb{E}}[X^2]-({\mathbb{E}}[X])^2 = 2-1 = 1\)</span>. That is, the variance and the mean are both equal to <span class="math inline">\(1\)</span>. Like the mean, the variance is also independent of <span class="math inline">\(n\)</span>. Intuitively at least, this means that it is unlikely that there will be more than a small number of fixed points even when the number of items, <span class="math inline">\(n\)</span>, is very large.</p></li>
</ol>
<h1 id="independent-random-variables" class="unnumbered">Independent Random Variables</h1>
<p>Independence for random variables is defined in analogous fashion to independence for events:</p>
<p><span id="definition:2" class="pandoc-numbering-text definition"><strong>Definition 2</strong></span></p>
<p><em>Random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> on the same probability space are said to be <span><em>independent</em></span> if the events <span class="math inline">\(X=a\)</span> and <span class="math inline">\(Y=b\)</span> are independent for all values <span class="math inline">\(a,b\)</span>. Equivalently, the joint distribution of independent r.v.’s decomposes as <span class="math display">\[{\mathbb{P}}[X=a,Y=b] = {\mathbb{P}}[X=a]{\mathbb{P}}[Y=b] \quad \forall a,b.\]</span></em></p>
<p>Mutual independence of more than two r.v.’s is defined similarly. A very important example of independent r.v.’s is indicator r.v.’s for independent events. Thus, for example, if <span class="math inline">\(\{X_i\}\)</span> are indicator r.v.’s for the <span class="math inline">\(i\)</span>-th toss of a coin being Heads, then the <span class="math inline">\(X_i\)</span> are mutually independent r.v.’s.</p>
<p>One of the most important and useful facts about variance is if a random variable <span class="math inline">\(X\)</span> is the sum of <span><em>independent</em></span> random variables <span class="math inline">\(X = X_1 + \cdots + X_n\)</span>, then its variance is the sum of the variances of the individual r.v.’s. In particular, if the individual r.v.’s <span class="math inline">\(X_i\)</span> are identically distributed (i.e., they have the same distribution), then <span class="math inline">\({\operatorname{var}}(X) = \sum_i {\operatorname{var}}(X_i) = n \cdot {\operatorname{var}}(X_1)\)</span>. This means that the standard deviation is <span class="math inline">\(\sigma(X) = \sqrt{n} \cdot \sigma(X_1)\)</span>. Note that by contrast, the expected value is <span class="math inline">\({\mathbb{E}}[X] = n \cdot {\mathbb{E}}[X_1]\)</span>. Intuitively this means that whereas the average value of <span class="math inline">\(X\)</span> grows proportionally to <span class="math inline">\(n\)</span>, the spread of the distribution grows proportionally to <span class="math inline">\(\sqrt{n}\)</span>, which is much smaller than <span class="math inline">\(n\)</span>. In other words the distribution of <span class="math inline">\(X\)</span> tends to concentrate around its mean.</p>
<p>Let us now formalize these ideas. First, we have the following result which states that the expected value of the product of two independent random variables is equal to the product of their expected values.</p>
<p><span id="thm:indepexp" class="pandoc-numbering-text thm"><strong>Theorem 2</strong></span></p>
<p><em>For <span><em>independent</em></span> random variables <span class="math inline">\(X,Y\)</span>, we have <span class="math inline">\({\mathbb{E}}[XY]={\mathbb{E}}[X]{\mathbb{E}}[Y]\)</span>.</em></p>
<p><em>Proof</em>. We have <span class="math display">\[\begin{aligned}
   {\mathbb{E}}[XY] &amp; = &amp; \sum_a\sum_b ab\times{\mathbb{P}}[X=a, Y=b] \\
                  &amp;=&amp; \sum_a\sum_b ab\times{\mathbb{P}}[X=a]\times{\mathbb{P}}[Y=b] \\
                  &amp;=&amp; \left(\sum_a a\times{\mathbb{P}}[X=a]\right)\times \left(\sum_b b\times{\mathbb{P}}[Y=b]\right)\\
                  &amp;=&amp; {\mathbb{E}}[X]\times{\mathbb{E}}[Y],\end{aligned}\]</span> as required. In the second line here we made crucial use of independence. <span class="math inline">\(\square\)</span></p>
<p>We now use the above theorem to conclude the nice property that the variance of the sum of independent random variables is equal to the sum of their variances.</p>
<p><span id="thm:varsum" class="pandoc-numbering-text thm"><strong>Theorem 3</strong></span></p>
<p><em>For <span><em>independent</em></span> random variables <span class="math inline">\(X,Y\)</span>, we have <span class="math display">\[{\operatorname{var}}(X+Y) = {\operatorname{var}}(X) + {\operatorname{var}}(Y).\]</span></em></p>
<p><em>Proof</em>. From the alternative formula for variance in <a href="#thm:var">Theorem 1</a>, we have, using linearity of expectation extensively, <span class="math display">\[\begin{aligned}
   {\operatorname{var}}(X+Y) &amp;=&amp; {\mathbb{E}}[(X+Y)^2] - {\mathbb{E}}[X+Y]^2\\
                      &amp;=&amp; {\mathbb{E}}[X^2]+{\mathbb{E}}[Y^2]+2{\mathbb{E}}[XY] - ({\mathbb{E}}[X]+{\mathbb{E}}[Y])^2\\
                      &amp;=&amp; ({\mathbb{E}}[X^2]-{\mathbb{E}}[X]^2) + ({\mathbb{E}}[Y^2]-{\mathbb{E}}[Y]^2) + 2({\mathbb{E}}[XY]-{\mathbb{E}}[X]{\mathbb{E}}[Y])\\
                      &amp;=&amp; {\operatorname{var}}(X) + {\operatorname{var}}(Y) + 2({\mathbb{E}}[XY]-{\mathbb{E}}[X]{\mathbb{E}}[Y]).\end{aligned}\]</span> Now <span><em>because <span class="math inline">\(X,Y\)</span> are independent</em></span>, by <a href="#thm:indepexp">Theorem 2</a> the final term in this expression is zero. Hence we get our result. <span class="math inline">\(\square\)</span></p>
<p><span><strong>Note:</strong></span> The expression <span class="math inline">\({\mathbb{E}}[XY]-{\mathbb{E}}[X]{\mathbb{E}}[Y]\)</span> appearing in the above proof is called the <span><em>covariance</em></span> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and is a measure of the dependence between <span class="math inline">\(X,Y\)</span>. It is zero when <span class="math inline">\(X,Y\)</span> are independent.</p>
<p>It is very important to remember that <span><strong>neither</strong></span> of these two results is true in general, without the assumption that <span class="math inline">\(X,Y\)</span> are independent. As a simple example, note that even for a <span class="math inline">\(0\)</span>-<span class="math inline">\(1\)</span> r.v. <span class="math inline">\(X\)</span> with <span class="math inline">\({\mathbb{P}}[X=1]=p\)</span>, <span class="math inline">\({\mathbb{E}}[X^2]=p\)</span> is not equal to <span class="math inline">\({\mathbb{E}}[X]^2=p^2\)</span> (because of course <span class="math inline">\(X\)</span> and <span class="math inline">\(X\)</span> are not independent!). This is in contrast to the case of the expectation, where we saw that the expectation of a sum of r.v.’s is the sum of the expectations of the individual r.v.’s <span><em>always</em></span>.</p>
<h2 id="example" class="unnumbered">Example</h2>
<p>Let’s return to our motivating example of a sequence of <span class="math inline">\(n\)</span> coin tosses. Let <span class="math inline">\(X\)</span> the the number of Heads in <span class="math inline">\(n\)</span> tosses of a biased coin with Heads probability <span class="math inline">\(p\)</span> (i.e., <span class="math inline">\(X\)</span> has the binomial distribution with parameters <span class="math inline">\(n,p\)</span>). As usual, we write <span class="math inline">\(X=X_1+X_2+\cdots +X_n\)</span>, where <span class="math inline">\(X_i=1\)</span> if the <span class="math inline">\(i\)</span>-th toss is Head, and <span class="math inline">\(X_i = 0\)</span> otherwise.</p>
<p>We already know <span class="math inline">\({\mathbb{E}}[X]= \sum_{i=1}^n {\mathbb{E}}[X_i] = np\)</span>. We can compute <span class="math inline">\({\operatorname{var}}(X_i) = {\mathbb{E}}(X_i^2) - {\mathbb{E}}(X_i)^2 = p - p^2 = p(1-p)\)</span>. Since the <span class="math inline">\(X_i\)</span>’s are independent, by <a href="#thm:varsum">Theorem 3</a> we get <span class="math inline">\({\operatorname{var}}(X) = \sum_{i=1}^n {\operatorname{var}}(X_i) = np(1-p)\)</span>.</p>
<p>As an example, for a fair coin (<span class="math inline">\(p = 1/2\)</span>) the expected number of Heads in <span class="math inline">\(n\)</span> tosses is <span class="math inline">\(n/2\)</span>, and the standard deviation is <span class="math inline">\(\sqrt{n/4} = \sqrt{n}/2\)</span>. Note that since the maximum number of Heads is <span class="math inline">\(n\)</span>, the standard deviation is much less than this maximum number for large <span class="math inline">\(n\)</span>. This is in contrast to the previous example of the uniformly distributed random variable Equation <a href="#eq:uniform">2</a>, where the standard deviation <span class="math inline">\(\sigma(X) ~=~ \sqrt{(n^2 - 1)/12} ~\approx~ n/\sqrt{12}\)</span> is of the same order as the largest value <span class="math inline">\(n\)</span>. In this sense, the spread of a binomially distributed r.v. is much smaller than that of a uniformly distributed r.v.</p>
</article>
</body>

<!-- Mirrored from www.eecs70.org/static/notes/n17.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:40:58 GMT -->
</html>
