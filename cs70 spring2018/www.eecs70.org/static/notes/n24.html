<!DOCTYPE html>
<!--==============================================================================
	           "GitHub HTML5 Pandoc Template" v1.2 — by Tristano Ajmone           
	==============================================================================
	(c) Tristano Ajmone, 2017, MIT License (MIT). Project's home repository:

	- https://github.com/tajmone/pandoc-goodies

	This template reuses source code taken from the following projects:

	- GitHub Markdown CSS: © Sindre Sorhus, MIT License (MIT):
	  https://github.com/sindresorhus/github-markdown-css

	- Primer CSS: © 2016 GitHub Inc., MIT License (MIT):
	  http://primercss.io/
	==============================================================================-->
<html>

<!-- Mirrored from www.eecs70.org/static/notes/n24.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:42:38 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Markov Chains</title>
<style type="text/css">@font-face{font-family:octicons-link;src:url(data:font/woff;charset=utf-8;base64,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) format('woff')}.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;color:#24292e;font-family:-apple-system,system-ui,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:16px;line-height:1.5;word-wrap:break-word;box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto;padding:45px}.markdown-body .octicon{display:inline-block;fill:currentColor;vertical-align:text-bottom}.markdown-body a{background-color:transparent;-webkit-text-decoration-skip:objects;color:#0366d6;text-decoration:none}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body h1{margin:.67em 0}.markdown-body img{border-style:none}.markdown-body svg:not(:root){overflow:hidden}.markdown-body code,.markdown-body kbd,.markdown-body pre{font-family:monospace,monospace}.markdown-body input{font:inherit;margin:0;overflow:visible;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=checkbox]{box-sizing:border-box;padding:0}.markdown-body *{box-sizing:border-box}.markdown-body a:hover{text-decoration:underline}.markdown-body strong{font-weight:600}.markdown-body hr{box-sizing:content-box;overflow:hidden;background:0 0;border-bottom:1px solid #dfe2e5}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body table{border-spacing:0;border-collapse:collapse;display:block;width:100%;overflow:auto}.markdown-body td,.markdown-body th{padding:0}.markdown-body blockquote{margin:0}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body dd{margin-left:0}.markdown-body code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace}.markdown-body pre{font:12px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;word-wrap:normal}.markdown-body .pl-0{padding-left:0!important}.markdown-body .pl-1{padding-left:4px!important}.markdown-body .pl-2{padding-left:8px!important}.markdown-body .pl-3{padding-left:16px!important}.markdown-body .pl-4{padding-left:24px!important}.markdown-body .pl-5{padding-left:32px!important}.markdown-body .pl-6{padding-left:40px!important}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:0}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e1e4e8;border:0}.markdown-body blockquote{padding:0 1em;color:#6a737d;border-left:.25em solid #dfe2e5}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd{font-size:11px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#1b1f23;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1{padding-bottom:.3em;font-size:2em;border-bottom:1px solid #eaecef}.markdown-body h2{padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid #eaecef}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#6a737d}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #dfe2e5}.markdown-body table tr{background-color:#fff;border-top:1px solid #c6cbd1}.markdown-body table tr:nth-child(2n){background-color:#f6f8fa}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body code{padding:.2em 0;margin:0;font-size:85%;background-color:rgba(27,31,35,.05);border-radius:3px}.markdown-body code::after,.markdown-body code::before{letter-spacing:-.2em;content:"\00a0"}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f6f8fa;border-radius:3px}.markdown-body pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code::after,.markdown-body pre code::before{content:normal}.markdown-body .full-commit .btn-outline:not(:disabled):hover{color:#005cc5;border-color:#005cc5}.markdown-body kbd{display:inline-block;padding:3px 5px;font:11px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fcfcfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body :checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item+.task-list-item{margin-top:3px}.markdown-body .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown-body hr{border-bottom-color:#eee}.flash{position:relative;padding:16px;color:#246;background-color:#e2eef9;border:1px solid #bac6d3;border-radius:3px}.flash p:last-child{margin-bottom:0}.flash-messages{margin-bottom:24px}.flash-warn{color:#4c4a42;background-color:#fff9ea;border-color:#dfd8c2}.flash-error{color:#911;background-color:#fcdede;border-color:#d2b2b2}.flash-success{color:#22662c;background-color:#e2f9e5;border-color:#bad3be}.flash-plain{color:#4c4a42;background-color:#f5f5f5;border-color:#c1c1c1}.figure{text-align:center;}</style>
  <style type="text/css">code{white-space: pre;}</style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {equationNumbers: {autoNumber: "AMS"}}
    });
  </script>
  <script src="../../../cdn.mathjax.org/mathjax/latest/MathJax2ba6.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title">Markov Chains</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#finite-markov-chains">Finite Markov Chains</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#a-first-example">A First Example</a></li>
<li><a href="#a-second-example">A Second Example</a></li>
<li><a href="#finite-markov-chains-1">Finite Markov Chains</a></li>
<li><a href="#balance-equations">Balance Equations</a></li>
<li><a href="#fraction-of-time-in-states">Fraction of Time in States</a></li>
<li><a href="#hitting-time">Hitting Time</a></li>
<li><a href="#probability-of-a-before-b">Probability of <span class="math inline">\(A\)</span> before <span class="math inline">\(B\)</span></a></li>
<li><a href="#appendix-1-calculations">Appendix 1: Calculations</a></li>
<li><a href="#appendix-2-some-proofs">Appendix 2: Some Proofs</a></li>
</ul>
</nav>
<h1 id="finite-markov-chains" class="unnumbered">Finite Markov Chains</h1>
<p>These notes explain the theory of finite Markov chains. For CS 70, we do not cover the proofs that are discussed in <a href="#appendix-2-some-proofs">Appendix 2</a>.</p>
<h1 id="introduction" class="unnumbered">Introduction</h1>
<p>Markov chains are models of random motion in a finite or countable set. These models are powerful because they capture a vast array of systems that we encounter in applications. Yet, the models are simple in that many of their properties can often be determined using elementary matrix algebra. In this course, we limit the discussion to the case of finite Markov chains, i.e., motions in a finite set.</p>
<p>Imagine the following scenario. You flip a fair coin until you get two consecutive ‘heads’. How many times do you have to flip the coin, on average? You roll a balanced six-sided die until the sum of the last two rolls is <span class="math inline">\(8\)</span>. How many times do you have to roll the die, on average?</p>
<p>As another example, say that you play a game of ‘heads or tails’ using a biased coin that yields ‘heads’ with probability <span class="math inline">\(0.48\)</span>. You start with <span class="math inline">\(\$10\)</span>. At each step, if the flip yields ‘heads’, you earn <span class="math inline">\(\$1\)</span>. Otherwise, you lose <span class="math inline">\(\$1\)</span>. What is the probability that you reach <span class="math inline">\(\$100\)</span> before <span class="math inline">\(\$0\)</span>? How long does it take until you reach either <span class="math inline">\(\$100\)</span> or <span class="math inline">\(\$0\)</span>?</p>
<p>You try to go up a ladder that has <span class="math inline">\(20\)</span> rungs. At each time step, you succeed in going up by one rung with probability <span class="math inline">\(0.9\)</span>. Otherwise, you fall back to the ground. How many time steps does it take you to reach the top of the ladder, on average?</p>
<p>You look at a web page, then select randomly one of the links on that page, with equal probabilities. You then repeat on the next page you visit, and so on. As you keep browsing the web in that way, what fraction of the time do you open a given page? How long does it take until you reach a particular page? How likely is it that you visit a given page before another given page?</p>
<p>These questions can be answered using the methods of Markov chains, as we explain in these notes.</p>
<h1 id="a-first-example" class="unnumbered">A First Example</h1>
<div class="figure">
<img src="n24-f1.png" alt="Figure 1: A simple Markov chain." id="fig:f1" style="width:30.0%" />
<p class="caption">Figure 1: A simple Markov chain.</p>
</div>
<p>Figure <a href="#fig:f1">1</a> illustrates a simple Markov chain. It describe a random motion in the set <span class="math inline">\(\{0, 1\}\)</span>. The position at time <span class="math inline">\(n = 0, 1, 2, \ldots\)</span> is <span class="math inline">\(X_n \in \{0, 1\}\)</span>. We call <span class="math inline">\(X_n\)</span> the <span><em>state</em></span> of the Markov chain at step (or time) <span class="math inline">\(n\)</span>. The set <span class="math inline">\(\{0, 1\}\)</span> is the <span><em>state space</em></span>, i.e., the set of possible values of the state. The motion, i.e., the time evolution, of <span class="math inline">\(X_n\)</span> follows the following rules. One is given a number <span class="math inline">\(a \in [0, 1]\)</span> and two nonnegative numbers <span class="math inline">\(\pi_0(0)\)</span> and <span class="math inline">\(\pi_0(1)\)</span> that add up to <span class="math inline">\(1\)</span>. Then, <a name="eq:e1"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
{\mathbb{P}}[X_0 = 0] = \pi_0(0) \text{ and } {\mathbb{P}}[X_0 = 1] = \pi_0(1).\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(1)</span></span>  That is, the <span><em>initial</em></span> state <span class="math inline">\(X_0\)</span> is equal to <span class="math inline">\(0\)</span> with probability <span class="math inline">\(\pi_0(0)\)</span>, otherwise it is <span class="math inline">\(1\)</span>. Then for <span class="math inline">\(n \geq 0\)</span>, <a name="eq:e2"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[\begin{aligned}
{\mathbb{P}}[X_{n+1} = 0 \mid X_n = 0, X_{n-1}, \ldots, X_0] &amp;= 1 - a \\
{\mathbb{P}}[X_{n+1} = 1 \mid X_n = 0, X_{n-1}, \ldots, X_0] &amp;= a \\
{\mathbb{P}}[X_{n+1} = 0 \mid X_n = 1, X_{n-1}, \ldots, X_0] &amp;= a \\
{\mathbb{P}}[X_{n+1} = 1 \mid X_n = 1, X_{n-1}, \ldots, X_0] &amp;= 1 - a. \end{aligned}\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(2)</span></span>  Figure <a href="#fig:f1">1</a> summarizes the rules in Equation <a href="#eq:e2">2</a>. These rules specify the <span><em>transition probabilities</em></span> of the Markov chain. The first two rules specify that if the Markov chain is in state <span class="math inline">\(0\)</span> at step <span class="math inline">\(n\)</span>, then at the next step it stays in state <span class="math inline">\(0\)</span> with probability <span class="math inline">\(1 - a\)</span> and it moves to state <span class="math inline">\(1\)</span> with probability <span class="math inline">\(a\)</span>, independently of what happened in the previous steps. Thus, the Markov chain may have been in state <span class="math inline">\(0\)</span> for a long time prior to step <span class="math inline">\(n\)</span>, or it may have just moved into state <span class="math inline">\(0\)</span>, but the probability of staying in state <span class="math inline">\(0\)</span> one more step is <span class="math inline">\(1 - a\)</span> in those different cases. The last two rules are similar. Figure <a href="#fig:f1">1</a> is called the <span><em>state transition diagram</em></span> of the Markov chain. It captures the transition probabilities in a graphical form.</p>
<p>In a sense, the Markov chain is <span><em>amnesic</em></span>: at step <span class="math inline">\(n\)</span>, it forgets what it did before getting to the current state and its future steps only depend on that current state. Here is one way to think of the rules of motion. When the Markov chain gets to state <span class="math inline">\(0\)</span>, it flips a coin. If the outcome is <span class="math inline">\(H\)</span>, which occurs with probability <span class="math inline">\(a\)</span>, then it goes to state <span class="math inline">\(1\)</span>; otherwise, it stays in state <span class="math inline">\(0\)</span> one more step. The situation is similar when the Markov chain gets to state <span class="math inline">\(1\)</span>.</p>
<p>We define the transition probability matrix <span class="math inline">\(P\)</span> by <span class="math inline">\(P(0, 0) = 1 - a, P(0, 1) = a, P(1, 0) = a, P(1, 1) = 1 - a.\)</span> That is <span class="math display">\[P = 
\begin{bmatrix}
1 - a &amp; a \\
a &amp; 1 - a
\end{bmatrix}
.\]</span> Hence, <span class="math display">\[{\mathbb{P}}[ X_{n+1} = j \mid X_n = i, X_{n-1}, \ldots , X_0] = P(i, j), \text{ for } n \geq 0 \text{ and } i, j \in \{0, 1\}.\]</span></p>
<div class="figure">
<img src="n24-f2.png" alt="Figure 2: Simulations of the two-state Markov chain." id="fig:f2" style="width:50.0%" />
<p class="caption">Figure 2: Simulations of the two-state Markov chain.</p>
</div>
<p>Figure <a href="#fig:f2">2</a> shows some simulations of the Markov chain with different values of <span class="math inline">\(a\)</span>. When <span class="math inline">\(a = 0.1\)</span>, it is unlikely that the state of the Markov chain changes in one step. As the figure shows, the Markov chain spends many steps in one state before switching. For larger values of <span class="math inline">\(a\)</span>, the state of the Markov chain changes more frequently. Note that, by symmetry, over the long term the Markov chain spends half of the time in each state.</p>
<h1 id="a-second-example" class="unnumbered">A Second Example</h1>
<div class="figure">
<img src="n24-f3.png" alt="Figure 3: A five-state Markov chain. The outgoing arrows are equally likely." id="fig:f3" style="width:30.0%" />
<p class="caption">Figure 3: A five-state Markov chain. The outgoing arrows are equally likely.</p>
</div>
<p>Figure <a href="#fig:f3">3</a> shows the state transition diagram of a small web browsing experiment. Each state in the figure represents a web page. The arrows out of a state correspond to links on the page that point to other pages. The transition probabilities are not indicated on the figure, but the model is that each outgoing link is equally likely. The figure corresponds to the following probability transition matrix: <span class="math display">\[P =
\begin{bmatrix}
0 &amp; 1/2 &amp; 0 &amp; 1/2 &amp; 0  \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0  \\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\
1/3 &amp; 1/3 &amp; 0 &amp; 0 &amp; 1/3  \\
0 &amp; 1/2 &amp; 1/2 &amp; 0 &amp; 0  \\
\end{bmatrix}
.\]</span></p>
<div class="figure">
<img src="n24-f4.png" alt="Figure 4: Simulation of the five-state Markov chain." id="fig:f4" style="width:50.0%" />
<p class="caption">Figure 4: Simulation of the five-state Markov chain.</p>
</div>
<p>Figure <a href="#fig:f4">4</a> shows a simulation of the five-state Markov chain.</p>
<h1 id="finite-markov-chains-1" class="unnumbered">Finite Markov Chains</h1>
<p>One defines a general finite Markov chain as follows. The <span><em>state space</em></span> is <span class="math inline">\({\cal X} = \{1, 2, \ldots, K\}\)</span> for some finite <span class="math inline">\(K\)</span>. The <span><em>transition probability matrix</em></span> <span class="math inline">\(P\)</span> is a <span class="math inline">\(K \times K\)</span> matrix such that <span class="math display">\[P(i, j) \geq 0, \forall i, j \in {\cal X}\]</span> and <span class="math display">\[\sum_{j = 1}^K P(i, j) = 1, \forall i \in {\cal X}.\]</span> The <span><em>initial distribution</em></span> is a vector <span class="math inline">\(\pi_0 = \{\pi_0(i), i \in {\cal X}\}\)</span> where <span class="math inline">\(\pi_0(i) \geq 0\)</span> for all <span class="math inline">\(i \in {\cal X}\)</span> and <span class="math inline">\(\sum_{i \in {\cal X}} \pi_0(i) = 1\)</span>.</p>
<p>One then defines the random sequence <span class="math inline">\(\{X_n, n = 0, 1, 2, \ldots \}\)</span> by <span class="math display">\[\begin{aligned}
{\mathbb{P}}[X_0 = i] &amp;= \pi_0(i), i \in {\cal X} \\
{\mathbb{P}}[X_{n+1} = j \mid X_n = i, X_{n-1}, \ldots, X_0] &amp;= P(i, j), \forall n \geq 0, \forall i, j \in {\cal X} . \end{aligned}\]</span></p>
<p>Note that <span class="math display">\[\begin{aligned}
&amp;{\mathbb{P}}[X_0 = i_0, X_1 = i_1, \ldots, X_n = i_n] \\
&amp;\qquad = {\mathbb{P}}[X_0 = i_0] {\mathbb{P}}[X_1 = i_1 \mid X_0 = i_0] {\mathbb{P}}[X_2 = i_2 \mid X_0 = i_0, X_1 = i_1 ] \cdots {\mathbb{P}}[X_n = i_n \mid X_0 = i_0, \ldots, X_{n-1} = i_{n-1}] \\
&amp;\qquad = \pi_0(i_0) P(i_0, i_1) \cdots P(i_{n-1}, i_n).\end{aligned}\]</span></p>
<p>Consequently, <span class="math display">\[\begin{aligned}
{\mathbb{P}}[X_n = i_n] &amp;= \sum_{i_0, \dotsc, i_{n-1}} {\mathbb{P}}[X_0 = i_0, X_1 = i_1, \ldots, X_n = i_n]  \\
&amp; = \sum_{i_0, \dotsc, i_{n-1}} \pi_0(i_0) P(i_0, i_1) \cdots P(i_{n-1}, i_n) \\
&amp; = \pi_0 P^n (i_n)\end{aligned}\]</span> where the last expression is component <span class="math inline">\(i_n\)</span> of the product of the row vector <span class="math inline">\(\pi_0\)</span> times the <span class="math inline">\(n\)</span>-th power of the matrix <span class="math inline">\(P\)</span>.</p>
<p>Thus, if we designate by <span class="math inline">\(\pi_n\)</span> the distribution of <span class="math inline">\(X_n\)</span>, so that <span class="math inline">\({\mathbb{P}}[X_n = i] = \pi_n(i)\)</span>, then the last derivation proves the following result.</p>
<p><span id="thm:s1" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></p>
<p><em>One has <span class="math display">\[
\pi_n = \pi_0 P^n, n \geq 0.\]</span> In particular, if <span class="math inline">\(\pi_0(i) = 1\)</span> for some <span class="math inline">\(i\)</span>, then <span class="math inline">\(\pi_n(j) = P^n(i, j) = {\mathbb{P}}[X_n = j \mid X_0 = i]\)</span>.</em></p>
<p>For the two-state Markov chain, one can verify that <span class="math display">\[
P^n = 
\begin{bmatrix}
1 - a &amp; a \\
a &amp; 1 - a
\end{bmatrix}
^n = 
\begin{bmatrix}
1/2 + (1/2)(1 - 2a)^n &amp; 1/2 - (1/2)(1 - 2a)^n \\
1/2 - (1/2)(1 - 2a)^n &amp; 1/2 + (1/2)(1 - 2a)^n
\end{bmatrix}
.\]</span> Note that if <span class="math inline">\(0 &lt; a &lt; 1\)</span>, <a name="eq:c1"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[P^n \to 
\begin{bmatrix}
1/2 &amp; 1/2 \\
1/2 &amp; 1/2
\end{bmatrix}
, \text{ as } n \to \infty.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(3)</span></span>  Consequently, for <span class="math inline">\(0 &lt; a &lt; 1\)</span>, one has <span class="math inline">\(\pi_n = \pi_0 P^n \to \begin{bmatrix}1/2 &amp; 1/2\end{bmatrix}\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p>
<h1 id="balance-equations" class="unnumbered">Balance Equations</h1>
<p>The following definition introduces the important notion of invariant distribution.</p>
<p><span id="definition:1" class="pandoc-numbering-text definition"><strong>Definition 1</strong></span></p>
<p><em>A distribution <span class="math inline">\(\pi\)</span> is <span><em>invariant</em></span> for the transition probability matrix <span class="math inline">\(P\)</span> if it satisfies the following <span><em>balance equations</em></span>:</em> <a name="eq:be"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\pi = \pi P.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(4)</span></span> </p>
<p>The relevance of this definition is stated in the next result.</p>
<p><span id="thm:s2" class="pandoc-numbering-text thm"><strong>Theorem 2</strong></span></p>
<p><em>One has <span class="math display">\[\pi_n = \pi_0, \forall n \geq 0\]</span> if and only if <span class="math inline">\(\pi_0\)</span> is invariant.</em></p>
<p><em>Proof</em>. If <span class="math inline">\(\pi_n = \pi_0\)</span> for all <span class="math inline">\(n \geq 0\)</span>, then <span class="math inline">\(\pi_0 = \pi_1 = \pi_0 P\)</span>, so that <span class="math inline">\(\pi_0\)</span> satisfies Equation <a href="#eq:be">4</a> and is thus invariant.</p>
<p>If <span class="math inline">\(\pi_0 P = \pi_0\)</span>, then <span class="math inline">\(\pi_1 = \pi_0 P = \pi_0\)</span>. Also, if <span class="math inline">\(\pi_n = \pi_0\)</span>, then <span class="math inline">\(\pi_{n+1} = \pi_n P = \pi_0 P = \pi_0\)</span>. <span class="math inline">\(\square\)</span></p>
<p>For instance, in the case of the two-state Markov chain, the balance equations are <span class="math display">\[\begin{aligned}
\pi(0)&amp; = \pi(0) (1 - a) + \pi(1) a \\
\pi(1)&amp; = \pi(0)a + \pi(1) (1 - a).\end{aligned}\]</span> Each of these two equations is equivalent to <span class="math display">\[\pi(0) = \pi(1).\]</span> Thus, the two equations are redundant. If we add the condition that the components of <span class="math inline">\(\pi\)</span> add up to one, we find that the only solution is <span class="math inline">\(\begin{bmatrix} \pi(0) &amp; \pi(1) \end{bmatrix} = \begin{bmatrix} 1/2 &amp; 1/2 \end{bmatrix}\)</span>, which is not surprising in view of symmetry.</p>
<p>For the five-state Markov chain, the balance equations are <span class="math display">\[\begin{bmatrix} \pi(A) &amp; \pi(B) &amp; \pi(C) &amp; \pi(D) &amp; \pi(E) \end{bmatrix} = \begin{bmatrix} \pi(A) &amp; \pi(B) &amp; \pi(C) &amp; \pi(D) &amp; \pi(E) \end{bmatrix}
\begin{bmatrix}
0 &amp; 1/2 &amp; 0 &amp; 1/2 &amp; 0  \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0  \\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\
1/3 &amp; 1/3 &amp; 0 &amp; 0 &amp; 1/3  \\
0 &amp; 1/2 &amp; 1/2 &amp; 0 &amp; 0  \\
\end{bmatrix}
.\]</span> Once again, these five equations in the five unknowns are redundant. They do not determine <span class="math inline">\(\pi\)</span> uniquely. However, if we add the condition that the components of <span class="math inline">\(\pi\)</span> add up to one, then we find that the solution is unique and given by (see the appendix for the calculations) <a name="eq:c2"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\begin{bmatrix} \pi(A) &amp; \pi(B) &amp; \pi(C) &amp; \pi(D) &amp; \pi(E)\end{bmatrix} = \frac{1}{39} \begin{bmatrix} 12 &amp; 9 &amp; 10 &amp; 6 &amp; 2 \end{bmatrix}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(5)</span></span>  Thus, in this web-browsing examples, page <span class="math inline">\(A\)</span> is visited most often, then page <span class="math inline">\(C\)</span>, then page <span class="math inline">\(B\)</span>. A Google search would return the pages in order of most frequent visits, i.e., in the order <span class="math inline">\(A, C, B, D, E\)</span>. This ranking of the pages is called <span><em>PageRank</em></span> and can be determined by solving the balance equations. (In fact, the actual ranking by Google combines the estimate of <span class="math inline">\(\pi\)</span> with other factors.)</p>
<p>How many invariant distributions does a Markov chain have? We have seen that for the two examples, the answer was one. However, that is not generally the case. For instance, consider the two-state Markov chain with <span class="math inline">\(a = 0\)</span> instead of <span class="math inline">\(0 &lt; a &lt; 1\)</span> as we assumed previously. This Markov chain does not change state. Its transition probability matrix is <span class="math inline">\(P = I\)</span> where <span class="math inline">\(I\)</span> denotes the identity matrix. Since <span class="math inline">\(\pi I = \pi\)</span> for any vector <span class="math inline">\(\pi\)</span>, we see that any distribution is invariant for this Markov chain. This is intuitively clear since the Markov chain does not change state, so that the distribution of the state does not change.</p>
<p>However, we show in <a href="#thm:t1">Theorem 3</a> that a simple condition guarantees the uniqueness of the invariant distribution.</p>
<h1 id="fraction-of-time-in-states" class="unnumbered">Fraction of Time in States</h1>
<p>How much time does a Markov chain spend in state <span class="math inline">\(i\)</span>, in the long term? That is, what is the long term fraction of time that <span class="math inline">\(X_n = i\)</span>? We can write this long term fraction of time as <span class="math display">\[\lim_{n \to \infty} \frac{1}{n} \sum_{m= 0}^{n-1} 1\{X_m = i\}.\]</span> To understand this expression, note that <span class="math inline">\(\sum_{m= 0}^{n-1} 1\{X_m = i\}\)</span> counts the number of steps among steps <span class="math inline">\(\{0, 1, \ldots, n - 1\}\)</span> that <span class="math inline">\(X_m = i\)</span>. Thus, <span class="math inline">\(n^{-1} \sum_{m= 0}^{n-1} 1\{X_m = i\}\)</span> is the fraction of time among the first <span class="math inline">\(n\)</span> steps that <span class="math inline">\(X_m = i\)</span>. By taking the limit, we obtain the long term fraction of time that <span class="math inline">\(X_m = i\)</span>.</p>
<p>To study this long term fraction of time, we need one property: irreducibility.</p>
<p><span id="definition:2" class="pandoc-numbering-text definition"><strong>Definition 2</strong> <em>(Irreducible)</em></span></p>
<p><em>A Markov chain is <span><em>irreducible</em></span> if it can go from every state <span class="math inline">\(i\)</span> to every other state <span class="math inline">\(j\)</span>, possibly in multiple steps.</em></p>
<p>The two-state Markov chain is irreducible when <span class="math inline">\(0 &lt; a &lt; 1\)</span>, not when <span class="math inline">\(a = 0\)</span>. The Markov chain in Figure <a href="#fig:f3">3</a> is irreducible. Observe that a Markov chain is irreducible if and only if its state transition diagram is a directed graph with a single connected component. In this transition diagram, there is an arrow from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> if <span class="math inline">\(P(i, j) &gt; 0\)</span>.</p>
<p>Here is a remarkable result.</p>
<p><span id="thm:t1" class="pandoc-numbering-text thm"><strong>Theorem 3</strong></span></p>
<p><em>Consider a finite irreducible Markov chain with state space <span class="math inline">\({\cal X}\)</span> and transition probability matrix <span class="math inline">\(P\)</span>. Then, for any initial distribution <span class="math inline">\(\pi_0\)</span>,</em><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> <a name="eq:8"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\lim_{n \to \infty} \frac{1}{n} \sum_{m=0}^{n-1} 1\{X_m = i\} = \pi(i), \forall i \in {\cal X}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(8)</span></span>  <em>In</em> Equation <a href="#eq:8">8</a>, <em><span class="math inline">\(\pi = \{\pi(i), i \in {\cal X}\}\)</span> is an invariant distribution. Consequently, the invariant distribution exists and is unique.</em></p>
<p>We sketch the proof of the result in the appendix. Here, we outline the main points of the argument. Consider the Markov chain in Figure <a href="#fig:f3">3</a>. Assume that <span class="math inline">\(X_0 = A\)</span> and let <span class="math inline">\(T(A)\)</span> be the first time after <span class="math inline">\(0\)</span> that the Markov chain comes back to <span class="math inline">\(A\)</span>. This random time has some mean value <span class="math inline">\({\mathbb{E}}[T(A)]\)</span>. Let <span class="math inline">\(\pi(A) = 1/{\mathbb{E}}[T(A)]\)</span>. Thus, the time between two visits to <span class="math inline">\(A\)</span> has mean value <span class="math inline">\({\mathbb{E}}[T(A)]\)</span>, so that the fraction of time that the Markov chain is in state <span class="math inline">\(A\)</span> is <span class="math inline">\(\pi(A)\)</span>. Hence, over a long time <span class="math inline">\(n\)</span>, the Markov chain is in state <span class="math inline">\(A\)</span> for about <span class="math inline">\(n \pi(A)\)</span> steps. We define <span class="math inline">\(\pi(i)\)</span> in the same way for the other states. These fractions of time in the different states must add up to one. Also, we claim that <span class="math inline">\(\pi\)</span> satisfies the balance equations. To see this, note that over a large number <span class="math inline">\(n\)</span> of steps, the Markov chain visits <span class="math inline">\(D\)</span> and then <span class="math inline">\(A\)</span> about <span class="math inline">\(n \pi(D) P(D, A)\)</span> times. Indeed, it visits <span class="math inline">\(D\)</span> about <span class="math inline">\(n \pi(D)\)</span> times and each of these visits is followed by a visit to <span class="math inline">\(A\)</span> with probability <span class="math inline">\(P(D, A)\)</span>. Similarly, if visits <span class="math inline">\(C\)</span> and then <span class="math inline">\(A\)</span> about <span class="math inline">\(n \pi(C) P(C, A)\)</span> times. Thus, a general Markov chain visits some state <span class="math inline">\(j\)</span> and then state <span class="math inline">\(i\)</span> about <span class="math inline">\(n \pi(j) P(j, i)\)</span> times in <span class="math inline">\(n\)</span> steps, for <span class="math inline">\(j \in {\cal X}\)</span>. Now, the total number of visits to <span class="math inline">\(i\)</span> in <span class="math inline">\(n\)</span> steps is the total number of visits to some <span class="math inline">\(j\)</span> followed by a visit to <span class="math inline">\(i\)</span>. Hence, <span class="math inline">\(n \pi(i) = \sum_j n \pi(j) P(j, i)\)</span>, which shows that <span class="math inline">\(\pi\)</span> solves the balance equations.</p>
<p>Is it the case that <span class="math inline">\({\mathbb{P}}[X_n = i]\)</span> converges to some value as <span class="math inline">\(n\)</span> increases? A simple example shows that this does not have to be the case. Consider our two-state Markov chain and assume that <span class="math inline">\(a = 1\)</span>. This Markov chain keeps switching state, at every step. Thus, if <span class="math inline">\(X_0 = 0\)</span>, then <span class="math inline">\(X_1 = 1, X_2 = 0, X_3 = 1, X_4 = 0,\)</span> and so on. For this Markov chain, <span class="math inline">\({\mathbb{P}}[X_n = 0] = 1\)</span> when <span class="math inline">\(n\)</span> is even and <span class="math inline">\({\mathbb{P}}[X_n = 0] = 0\)</span> when <span class="math inline">\(n\)</span> is odd. Hence, <span class="math inline">\({\mathbb{P}}[X_n = 0]\)</span> keeps on oscillating between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> and does not converge. Such a Markov chain is said to be <span><em>periodic</em></span>. However, if <span class="math inline">\(a \in (0, 1)\)</span>, then our calculations after <a href="#thm:t1">Theorem 3</a> showed that <span class="math inline">\({\mathbb{P}}[X_n = 0] \to 1/2\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p>
<p>The following theorem generalizes this example.</p>
<p><span id="thm:t2" class="pandoc-numbering-text thm"><strong>Theorem 4</strong></span></p>
<p><em>Consider an irreducible Markov chain on <span class="math inline">\({\cal X}\)</span> with transition probability matrix <span class="math inline">\(P\)</span>. Define</em> <a name="eq:11"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
d(i) := \gcd\{n &gt; 0 \mid P^n(i, i) = {\mathbb{P}}[X_n = i \mid X_0 = i] &gt; 0 \}, i \in {\cal X}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(9)</span></span> </p>
<p>(a) <em>Then, <span class="math inline">\(d(i)\)</span> has the same value for all <span class="math inline">\(i \in {\cal X}\)</span>. If that value is <span class="math inline">\(1\)</span>, the Markov chain is said to be <span><em>aperiodic</em></span>. Otherwise, it is said to be <span><em>periodic with period</em></span> <span class="math inline">\(d\)</span>.</em></p>
<p>(b) <em>If the Markov chain is aperiodic, then</em> <a name="eq:12"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
{\mathbb{P}}[X_n = i] \to \pi(i), \forall i \in {\cal X}, \text{ as } n \to \infty.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(10)</span></span>  <em>In</em> Equation <a href="#eq:12">10</a>, <em><span class="math inline">\(\pi\)</span> is the unique invariant distribution.</em></p>
<p>To explain this theorem, we first need to clarify Equation <a href="#eq:11">9</a>. For a given state <span class="math inline">\(i\)</span>, the quantity <span class="math inline">\(d(i)\)</span> is the greatest common divisor or all the integers <span class="math inline">\(n &gt; 0\)</span> so that the Markov chain can go from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(i\)</span> in <span class="math inline">\(n\)</span> steps.</p>
<p>For instance, for the Markov chain in Figure <a href="#fig:f1">1</a>, assume that <span class="math inline">\(a = 1\)</span>. In that case, the Markov chain can go from state <span class="math inline">\(0\)</span> to state <span class="math inline">\(0\)</span> in <span class="math inline">\(n\)</span> steps for all <span class="math inline">\(n\)</span> in the set <span class="math inline">\(\{2, 4, 6, 8, \ldots \}\)</span>. Thus, <span class="math inline">\(d(0) = \gcd\{2, 4, 6, \ldots \} = 2.\)</span> Similarly, we find that <span class="math inline">\(d(1) = 2\)</span>. The Markov chain is irreducible and <a href="#thm:t2">Theorem 4</a> correctly predicted that <span class="math inline">\(d(0) = d(1)\)</span>. This Markov chain is periodic with period <span class="math inline">\(2\)</span>. If <span class="math inline">\(a \in (0, 1)\)</span>, then the Markov chain can go from state <span class="math inline">\(0\)</span> to state <span class="math inline">\(0\)</span> in any <span class="math inline">\(n \geq 1\)</span> steps. Thus, <span class="math inline">\(d(0) = \gcd\{1, 2, 3, \ldots \} = 1.\)</span> Similarly <span class="math inline">\(d(1) = 1\)</span>. The Markov chain is aperiodic and the theorem predicts that <span class="math inline">\(\pi_n(i) \to \pi(i) = 1/2\)</span>, as we had verified explicitly.</p>
<p>As another example, consider the Markov chain in Figure <a href="#fig:f3">3</a>. This Markov chain is irreducible. Is it aperiodic? Looking at the state transition diagram, we see that <span class="math display">\[d(A) = \gcd\{2, 3, \ldots \} = 1.\]</span> Indeed, the Markov chain can go from state <span class="math inline">\(A\)</span> to state <span class="math inline">\(A\)</span> in two steps (<span class="math inline">\(A \to D \to A\)</span>) and in three steps (<span class="math inline">\(A \to B \to C \to A\)</span>). Thus, the Markov chain is aperiodic. Just for fun, let us compute <span class="math inline">\(d(B)\)</span>. We find <span class="math inline">\(d(B) = \gcd\{3, 4, \ldots \} = 1\)</span>. Thus, <a href="#thm:t2">Theorem 4</a> implies that <span class="math inline">\(\pi_n(i) \to \pi(i)\)</span>. For instance, <span class="math inline">\({\mathbb{P}}[X_n = A] \to 12/39\)</span>. This is a powerful result because computing <span class="math inline">\(\pi_n\)</span> directly is not that simple!</p>
<p>We give the proof of the theorem in the appendix. Here are the key points of the argument when the Markov chain is aperiodic. Consider the Markov chain of Figure <a href="#fig:f3">3</a>. Note that <span class="math inline">\(S(E) := \{n &gt; 0 \mid P^n(E, E) &gt; 0\} = \{4, 5, 6, \ldots\}\)</span>. Thus, any <span class="math inline">\(n \geq n(E) := 4\)</span> is such that <span class="math inline">\(n \in S(E)\)</span>. In general, one can show that if <span class="math inline">\(d(i) = 1\)</span>, then there is some integer <span class="math inline">\(n(i)\)</span> so that <span class="math inline">\(\{n(i), n(i) + 1, \ldots\} \subset S(i)\)</span>. Note also that the Markov chain can go from state <span class="math inline">\(C\)</span> to <span class="math inline">\(E\)</span> is some finite number <span class="math inline">\(a\)</span> of steps (here, <span class="math inline">\(a = 3\)</span>). Also, it can go from <span class="math inline">\(E\)</span> to <span class="math inline">\(C\)</span> is <span class="math inline">\(b\)</span> steps (here, <span class="math inline">\(b = 1\)</span>). Hence, it can go from <span class="math inline">\(C\)</span> to <span class="math inline">\(C\)</span> in <span class="math inline">\(a + n + b\)</span> steps for any <span class="math inline">\(n \geq n(E)\)</span> by first going from <span class="math inline">\(C\)</span> to <span class="math inline">\(E\)</span> in <span class="math inline">\(a\)</span> steps, then from <span class="math inline">\(E\)</span> to <span class="math inline">\(E\)</span> in <span class="math inline">\(n\)</span> steps, then from <span class="math inline">\(E\)</span> to <span class="math inline">\(C\)</span> in <span class="math inline">\(b\)</span> steps. Thus, <span class="math inline">\(S(C)\)</span> contains two consecutive integers, so that its GCD <span class="math inline">\(d(C)\)</span> is equal to one. Similarly, <span class="math inline">\(d(j) = 1\)</span> for any state <span class="math inline">\(j\)</span> if there is some state <span class="math inline">\(i\)</span> with <span class="math inline">\(d(i) = 1\)</span>. Also, this argument shows that there is some integer <span class="math inline">\(k\)</span> so that the Markov chain can go from any state <span class="math inline">\(j\)</span> to some specific state <span class="math inline">\(i\)</span> in <span class="math inline">\(k\)</span> steps (the same <span class="math inline">\(k\)</span> for all <span class="math inline">\(j\)</span>). The next idea is a coupling argument. Imagine two independent versions of the Markov chain. The claim is that they must meet after some finite time. Indeed, every <span class="math inline">\(k\)</span> steps, they have a positive probability of meeting in state <span class="math inline">\(i\)</span>. Now, say that one version <span class="math inline">\(X_n\)</span> starts with the invariant distribution <span class="math inline">\(\pi\)</span> and the other, <span class="math inline">\(Z_n\)</span> starts with some arbitrary distribution <span class="math inline">\(\pi_0\)</span>. Define <span class="math inline">\(Y_n\)</span> so that it agrees with <span class="math inline">\(Z_n\)</span> until it meets with <span class="math inline">\(X_n\)</span> and thereafter stays glued to <span class="math inline">\(X_n\)</span>. The point is that <span class="math inline">\(Y_n\)</span> is a Markov chain with transition matrix <span class="math inline">\(P\)</span> and initial distribution <span class="math inline">\(\pi_0\)</span>. But, since it meets with <span class="math inline">\(X_n\)</span> after a finite time, we see that <span class="math inline">\({\mathbb{P}}[Y_n = i] \to {\mathbb{P}}[X_n = i] = \pi(i)\)</span>.</p>
<h1 id="hitting-time" class="unnumbered">Hitting Time</h1>
<p>Consider the Markov chain in Figure <a href="#fig:f3">3</a>. Assume it starts in state <span class="math inline">\(A\)</span>. What is the average number of steps until it reaches state <span class="math inline">\(E\)</span>? To calculate that average time, for <span class="math inline">\(i \in \{A, B, C, D, E\}\)</span>, define <span class="math inline">\(\beta (i)\)</span> to be the average time until the Markov chain reaches state <span class="math inline">\(E\)</span> given that it starts from state <span class="math inline">\(i\)</span>.</p>
<p>Thus, <span class="math inline">\(\beta(E) = 0\)</span> since it takes <span class="math inline">\(0\)</span> steps to reach <span class="math inline">\(E\)</span> when starting in state <span class="math inline">\(E\)</span>. We want to calculate <span class="math inline">\(\beta(A)\)</span>. However, it turns out that to calculate <span class="math inline">\(\beta(A)\)</span>, one also has to calculate <span class="math inline">\(\beta(B), \ldots, \beta(D)\)</span>. We do this by finding equations that these quantities satisfy. We then solve these equations.</p>
<p>We claim that <a name="eq:fse1"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\beta(A) = 1 + \frac{1}{2} \beta(B) + \frac{1}{2} \beta(D).\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(11)</span></span>  To see this, note that when the Markov chain starts in state <span class="math inline">\(A\)</span>, it stays there for one step. Then, with probability <span class="math inline">\(1/2\)</span> it moves to state <span class="math inline">\(B\)</span>. In that case, the average time untill it reaches <span class="math inline">\(E\)</span> is <span class="math inline">\(\beta(B)\)</span>. With probability <span class="math inline">\(1/2\)</span>, the Markov chain moves to state <span class="math inline">\(D\)</span> and then takes <span class="math inline">\(\beta(D)\)</span> steps, on average to reach <span class="math inline">\(E\)</span>. Thus, the time to reach <span class="math inline">\(E\)</span> starting from state <span class="math inline">\(A\)</span> is <span class="math inline">\(1\)</span> step plus an average of <span class="math inline">\(\beta(B)\)</span> steps with probability <span class="math inline">\(1/2\)</span> and an average of <span class="math inline">\(\beta(D)\)</span> steps with probability <span class="math inline">\(1/2\)</span>. Equation <a href="#eq:fse1">11</a> capture that decomposition of the time to reach <span class="math inline">\(E\)</span> starting from <span class="math inline">\(A\)</span>.</p>
<p>An identity similar to Equation <a href="#eq:fse1">11</a> can be written for every starting state. We find <span class="math display">\[\begin{aligned}
\beta(A) &amp;= 1 + \frac{1}{2} \beta(B) + \frac{1}{2} \beta(D) \\
\beta(B) &amp;= 1 + \beta(C) \\
\beta(C) &amp;= 1 + \beta(A) \\
\beta(D) &amp;= 1 + \frac{1}{3} \beta(A) + \frac{1}{3} \beta(B) + \frac{1}{3} \beta(E) \\
\beta(E) &amp;= 0.\end{aligned}\]</span> These equations are called the <span><em>first step equations</em></span> (FSE). Solving these equations, we find (see the appendix for the calculations) <a name="eq:c3"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\beta(A) = 17, \beta(B) = 19, \beta(C) = 18, \beta(D) = 13, \beta(E) = 0.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(12)</span></span> </p>
<p>Let us now consider a general finite Markov chain with transition probability matrix <span class="math inline">\(P\)</span> on the state space <span class="math inline">\({\cal X}\)</span>. Let <span class="math inline">\(A \subset {\cal X}\)</span> be a set of states. For each <span class="math inline">\(i \in {\cal X}\)</span>, let <span class="math inline">\(\beta(i)\)</span> be average number of steps until the Markov chain enters one of the states in <span class="math inline">\(A\)</span>, given that it starts in state <span class="math inline">\(i\)</span>.</p>
<p>Then one has <span class="math display">\[\begin{aligned}
\beta(i) &amp;= 0, \text{ if } i \in A \\
\beta(i) &amp;= 1 + \sum_{j \in {\cal X}} P(i, j) \beta(j).\end{aligned}\]</span> These equations are called the <span><em>first step equations</em></span> (FSE) for the average hitting time.</p>
<p>As another example, consider the Markov chain in Figure <a href="#fig:f1">1</a>. Let <span class="math inline">\(\beta(i)\)</span> be the average number of steps until the Markov chain enters state <span class="math inline">\(1\)</span>. The first step equations are <span class="math display">\[\begin{aligned}
\beta(0) &amp;= 1 + (1 - a) \beta (0) + a \beta(1) \\
\beta(1) &amp;= 0;\end{aligned}\]</span> Solving, we find <span class="math inline">\(\beta(0) = 1/a\)</span>. Note that the time to enter state <span class="math inline">\(1\)</span> starting from state <span class="math inline">\(0\)</span> is the number of times one has to flip a loaded coin with <span class="math inline">\({\mathbb{P}}[H] = a\)</span> until the first heads. This number of steps has a geometric distribution with parameter <span class="math inline">\(a\)</span>. Thus, we have rediscovered the fact that the mean value of a <span class="math inline">\(G(a)\)</span> random variable is <span class="math inline">\(1/a\)</span>.</p>
<div class="figure">
<img src="n24-f5.png" alt="Figure 5: Flipping a fair coin until two heads in row." id="fig:f5" style="width:50.0%" />
<p class="caption">Figure 5: Flipping a fair coin until two heads in row.</p>
</div>
<p>Say that you flip a fair coin repeatedly until you get two heads in a row. How many times do you have to flip the coin, on average? Figure <a href="#fig:f5">5</a> shows a state transition diagram that corresponds to that situation. The Markov chain starts in state <span class="math inline">\(S\)</span>. The state is <span class="math inline">\(H\)</span> or <span class="math inline">\(T\)</span> if the last coin flip was <span class="math inline">\(H\)</span> or <span class="math inline">\(T\)</span>, except that the state is <span class="math inline">\(E\)</span> if the last two flips where heads. For <span class="math inline">\(i \in \{S, T, H, E\}\)</span>, let <span class="math inline">\(\beta(i)\)</span> be the average number of steps until the Markov chain enters state <span class="math inline">\(E\)</span>. The first step equations are <span class="math display">\[\begin{aligned}
\beta(S) &amp;= 1 + \frac{1}{2} \beta(T) + \frac{1}{2}\beta(H) \\
\beta(T) &amp;= 1 + \frac{1}{2} \beta(T) + \frac{1}{2} \beta(H) \\
\beta(H) &amp;= 1 + \frac{1}{2} \beta(T) + \frac{1}{2} \beta(E) \\
\beta(E) &amp;= 0.\end{aligned}\]</span> Solving, we find <a name="eq:c4"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\beta(S) = 6.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(13)</span></span>  (See the appendix for the calculations.)</p>
<p>Now assume you roll a balanced six-sided die until the sum of the last two rolls is <span class="math inline">\(8\)</span>. The Markov chain that corresponds to this situation has a start state <span class="math inline">\(S\)</span>, a state <span class="math inline">\(i\)</span> for <span class="math inline">\(i \in \{1, 2, \ldots, 6\}\)</span> that indicates the value of the last roll, and an end state <span class="math inline">\(E\)</span> that the Markov chain enters when the sum of the last two rolls is <span class="math inline">\(8\)</span>. Thus, if the state of the Markov chain is <span class="math inline">\(5\)</span> and if the next roll is <span class="math inline">\(2\)</span>, then the new state is <span class="math inline">\(2\)</span>. However, if the next roll is <span class="math inline">\(3\)</span>, then the Markov chain enters state <span class="math inline">\(E\)</span>. The first step equations for the average time <span class="math inline">\(\beta(i)\)</span> it takes the Markov chain to enter state <span class="math inline">\(E\)</span> are as follows: <span class="math display">\[\begin{aligned}
\beta(S) &amp;= 1 + \sum_{i = 1}^6 \frac{1}{6} \beta(i) \\
\beta(i) &amp;= 1 + \sum_{j \text{ s.t. } i + j \neq 8} \frac{1}{6} \beta(j).\end{aligned}\]</span> Solving, we find <a name="eq:c5"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\beta(S) = 8.4.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(14)</span></span>  (See the appendix for the calculations.)</p>
<p>Consider now the <span class="math inline">\(20\)</span>-rung ladder. A man starts on the ground. At each step, he moves up one rung with probability <span class="math inline">\(p\)</span> and falls back to the ground otherwise. Let <span class="math inline">\(\beta(i)\)</span> be the average number of steps needed to reach the top rung, starting from rung <span class="math inline">\(i \in \{0, 1, \ldots, 20\}\)</span> where rung <span class="math inline">\(0\)</span> refers to the ground. The first step equations are <span class="math display">\[\begin{aligned}
\beta(i) &amp;= 1 + (1 - p)\beta(0) + p \beta(i + 1), i = 0, \ldots, 19\\
\beta(20) &amp;= 0.\end{aligned}\]</span> Solving, we find <a name="eq:c6"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\beta(0) = \frac{p^{-20} - 1}{1 - p}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(15)</span></span>  (See the appendix for the calculations.) For instance, if <span class="math inline">\(p = 0.9\)</span>, then <span class="math inline">\(\beta(0) \approx 72\)</span>. Also, if <span class="math inline">\(p = 0.8\)</span>, then <span class="math inline">\(\beta(0) \approx 429\)</span>. The morale of the story is that you have to be careful on a ladder.</p>
<p>Assume we play a game of heads-or-tails with a coin such that <span class="math inline">\({\mathbb{P}}[H] = p\)</span>. For every heads, your fortune increases by <span class="math inline">\(1\)</span> and for every tails, it decreases by <span class="math inline">\(1\)</span>. The initial fortune is <span class="math inline">\(m\)</span>. Let <span class="math inline">\(\beta(n)\)</span> be the average time until the fortune reaches the value <span class="math inline">\(0\)</span> or the value <span class="math inline">\(M\)</span> where <span class="math inline">\(M &gt; m\)</span>. The first step equations are <span class="math display">\[\begin{aligned}
\beta(n) &amp;= 1 + (1 - p) \beta(n-1) + p \beta(n+1), \mbox{ for } n = 1, \ldots, M -1 \\
\beta(0) &amp;= \beta(M) = 0.\end{aligned}\]</span> Solving, we find <a name="eq:c7"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\beta(n) = n (1 - 2p)^{-1} -  \frac{M(1 - 2p)^{-1}}{1 - \rho^M} (1 - \rho^n).\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(16)</span></span>  (See the appendix for the calculations.)</p>
<h1 id="probability-of-a-before-b" class="unnumbered">Probability of <span class="math inline">\(A\)</span> before <span class="math inline">\(B\)</span></h1>
<p>Let <span class="math inline">\(X_n\)</span> be a finite Markov chain with state space <span class="math inline">\({\cal X}\)</span> and transition probability matrix <span class="math inline">\(P\)</span>. Let also <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two disjoint subsets of <span class="math inline">\({\cal X}\)</span>. We want to determine the probability <span class="math inline">\(\alpha(i)\)</span> that, starting in state <span class="math inline">\(i\)</span>, the Markov chain enters one of the states in <span class="math inline">\(A\)</span> before one of the states in <span class="math inline">\(B\)</span>.</p>
<p>The first step equations for <span class="math inline">\(\alpha(i)\)</span> are <span class="math display">\[\begin{aligned}
\alpha(i) &amp;= \sum_j P(i, j) \alpha(j), \forall i \notin A \cup B \\
\alpha(i) &amp;= 1, \forall i \in A \\
\alpha(i) &amp;= 0, \forall i \in B.\end{aligned}\]</span></p>
<p>To see why the first set of equations hold, we observe that the event that the Markov chain enters <span class="math inline">\(A\)</span> before <span class="math inline">\(B\)</span> starting from <span class="math inline">\(i\)</span> is partitioned into the events that it does so by first moving to state <span class="math inline">\(j\)</span>, for all possible value of <span class="math inline">\(j\)</span>. Now, the probability that it enters <span class="math inline">\(A\)</span> before <span class="math inline">\(B\)</span> starting from <span class="math inline">\(i\)</span> after moving first to <span class="math inline">\(j\)</span> is the probability that it enters <span class="math inline">\(A\)</span> before <span class="math inline">\(B\)</span> starting from <span class="math inline">\(j\)</span>, because the Markov chain is amnesic. The second and third sets of equations are obvious.</p>
<p>As an illustration, consider again the game of heads and tails and let <span class="math inline">\(\alpha(n)\)</span> be the probability that your fortune reaches <span class="math inline">\(M\)</span> before <span class="math inline">\(0\)</span> when starting from <span class="math inline">\(n\)</span> with <span class="math inline">\(0 \leq n \leq M\)</span>. The first step equations are <span class="math display">\[\begin{aligned}
\alpha(n) &amp;= (1 - p) \alpha(n-1) + p \alpha(n+1), 0 &lt; n &lt; M \\
\alpha(M) &amp;= 1 \\
\alpha(0) &amp;= 0.\end{aligned}\]</span> Solving these equations, we find <a name="eq:c8"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\alpha(n) = \frac{1 - \rho^n}{1 - \rho^M}\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(17)</span></span>  where <span class="math inline">\(\rho := (1 - p)p^{-1}\)</span>. (See the appendix for the calculations.) For instance, with <span class="math inline">\(p = 0.48\)</span> and <span class="math inline">\(M = 100\)</span>, we find that <span class="math inline">\(\alpha(10) \approx 4 \times 10^{-4}\)</span>, which is sobering when contemplating a trip to Las Vegas. Note that for each gambler who plays this game, the Casino makes <span class="math inline">\(\$10.00\)</span> with probability <span class="math inline">\(1 - 4 \times 10^{-4}\)</span> and loses <span class="math inline">\(\$990.00\)</span> with probability <span class="math inline">\(4 \times 10^{-4}\)</span>, so that the expected gain of the Casino per gambler is approximately <span class="math inline">\((1 - 4 \times 10^{-4}) \times \$10.00 - 4 \times 10^{-4} \times \$990.00 \approx \$9.60\)</span>. Observe that the probability of winning in one step is <span class="math inline">\(48\%\)</span>, so that if the gamble did bet everything on a single game and stopped after one step, the Casino would only make <span class="math inline">\(0.52 \times \$10.00 - 0.48 \times \$10.00 = \$0.40\)</span> on average per gambler, instead of <span class="math inline">\(\$9.60\)</span>. The morale of the story is: don’t push your luck!</p>
<h1 id="appendix-1-calculations" class="unnumbered">Appendix 1: Calculations</h1>
<p>This section presents the details of the calculations of this note.</p>
<ul>
<li>Equation <a href="#eq:c1">3</a></li>
</ul>
<p>By symmetry, we can write <span class="math display">\[P^n =
\begin{bmatrix}
1 - \alpha_n &amp; \alpha_n \\
\alpha_n &amp; 1 - \alpha_n
\end{bmatrix}
\]</span> for some <span class="math inline">\(\alpha_n\)</span> that we determine below. Note that <span class="math inline">\(\alpha_1 = a\)</span>. Also, <span class="math display">\[P^{n+1} = 
\begin{bmatrix}
1 - \alpha_{n+1} &amp; \alpha_{n+1} \\
\alpha_{n+1} &amp; 1 - \alpha_{n+1}
\end{bmatrix}
= P P^n = 
\begin{bmatrix}
1 - a &amp; a\\
a &amp; 1 - a
\end{bmatrix}
\begin{bmatrix}
1 - \alpha_n &amp; \alpha_n \\
\alpha_n &amp; 1 - \alpha_n
\end{bmatrix}
.\]</span> Consequently, by looking at component <span class="math inline">\((0, 1)\)</span> of this product, <span class="math display">\[\alpha_{n+1} = (1 - a) \alpha_n + a (1 - \alpha_n) = a + (1 - 2a) \alpha_n.\]</span> Let us try a solution of the form <span class="math inline">\(\alpha_n = b + c \lambda^n\)</span>. We need <span class="math display">\[\alpha_{n+1} = b + c \lambda^{n+1} = a + (1 - 2a) \alpha_n = a + (1 - 2a)[b + c \lambda^n] = a + (1 - 2a)b + (1 - 2a)c \lambda^n.\]</span> Matching the terms, we see that this identity holds if <span class="math display">\[b = a + (1 -2a)b \text{ and } \lambda = 1 - 2a.\]</span> The first equation gives <span class="math inline">\(b = 1/2\)</span>. Hence, <span class="math inline">\(\alpha_n = 1/2 + c(1 - 2a)^n\)</span>. To find <span class="math inline">\(c\)</span>, we use the fact that <span class="math inline">\(\alpha_1 = a\)</span>, so that <span class="math inline">\(1/2 + c(1 - 2a) = a\)</span>, which yields <span class="math inline">\(c = - 1/2\)</span>.</p>
<p>Hence, <span class="math inline">\(\alpha_n = 1/2 - (1/2)(1 - 2a)^n\)</span>.</p>
<ul>
<li>Equation <a href="#eq:c2">5</a></li>
</ul>
<p>The balance equations are <span class="math inline">\(\pi = \pi P\)</span>.</p>
<p>We know that the equations do not determine <span class="math inline">\(\pi\)</span> uniquely. Let us choose arbitrarily <span class="math inline">\(\pi(A) = 1\)</span>. We then solve for the other components of <span class="math inline">\(\pi\)</span> and we renormalize later. We can ignore any equation we choose. Let us ignore the first one. The new equations are <span class="math display">\[\begin{bmatrix} \pi(B) &amp; \pi(C) &amp; \pi(D) &amp; \pi(E) \end{bmatrix} = \begin{bmatrix} 1 &amp; \pi(B) &amp; \pi(C) &amp; \pi(D) &amp; \pi(E) \end{bmatrix}
\begin{bmatrix}
1/2 &amp; 0 &amp; 1/2 &amp; 0  \\
0 &amp; 1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0 &amp; 0  \\
1/3 &amp; 0 &amp; 0 &amp; 1/3  \\
1/2 &amp; 1/2 &amp; 0 &amp; 0  \\
\end{bmatrix}
.\]</span> Equivalently, <span class="math display">\[\begin{bmatrix} \pi(B) &amp; \pi(C) &amp; \pi(D) &amp; \pi(E) \end{bmatrix} = \begin{bmatrix} 1/2 &amp; 0 &amp; 1/2 &amp; 0 \end{bmatrix} + \begin{bmatrix} \pi(B) &amp; \pi(C) &amp; \pi(D) &amp; \pi(E) \end{bmatrix}
\begin{bmatrix}
0 &amp; 1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0 &amp; 0  \\
1/3 &amp; 0 &amp; 0 &amp; 1/3  \\
1/2 &amp; 1/2 &amp; 0 &amp; 0  \\
\end{bmatrix}
.\]</span> By inspection, we see that <span class="math inline">\(\pi(D) = 1/2\)</span>, then <span class="math inline">\(\pi(E) = (1/3) \pi(D) = 1/6\)</span>, then <span class="math inline">\(\pi(B) = 1/2 + (1/3) \pi(D) + (1/2) \pi(E) = 1/2 + 1/6 + 1/12 = 3/4\)</span>. Finally, <span class="math inline">\(\pi(C) = \pi(B) + (1/2) \pi(E) = 3/4 + 1/12 = 5/6.\)</span> The components <span class="math inline">\(\pi(A) + \cdots + \pi(E)\)</span> add up to <span class="math inline">\(1 + 3/4 + 5/6 + 1/2 + 1/6 = 39/12.\)</span> To normalize, we multiply each component by <span class="math inline">\(12/39\)</span> and we get <span class="math display">\[\pi = \begin{bmatrix} 12/39 &amp; 9/39 &amp; 10/39 &amp; 6/39 &amp; 2/39 \end{bmatrix}.\]</span></p>
<p>We could have proceeded differently and observed that our identity implies that <span class="math display">\[\begin{bmatrix} \pi(B) &amp; \pi(C) &amp; \pi(D) &amp; \pi(E) \end{bmatrix}
\begin{bmatrix}
1 &amp; -1 &amp; 0 &amp; 0  \\
0 &amp; 1 &amp; 0 &amp; 0  \\
- 1/3 &amp; 0 &amp; 1 &amp; - 1/3  \\
- 1/2 &amp; -1/2 &amp; 0 &amp; 1  \\
\end{bmatrix}
 = \begin{bmatrix} 1/2 &amp; 0 &amp; 1/2 &amp; 0\end{bmatrix} .\]</span> Hence, <span class="math display">\[\begin{bmatrix} \pi(B)&amp; \pi(C)&amp; \pi(D)&amp; \pi(E)\end{bmatrix} = \begin{bmatrix}1/2&amp; 0&amp; 1/2&amp; 0\end{bmatrix}
\begin{bmatrix}
1 &amp; -1 &amp; 0 &amp; 0  \\
0 &amp; 1 &amp; 0 &amp; 0  \\
- 1/3 &amp; 0 &amp; 1 &amp; - 1/3  \\
- 1/2 &amp; -1/2 &amp; 0 &amp; 1  \\
\end{bmatrix}
^{-1}.\]</span> This procedure is a systematic way to solve the balance equations by computer.</p>
<ul>
<li>Equation <a href="#eq:c3">12</a></li>
</ul>
<p>Using the third equation in the second, we find <span class="math inline">\(\beta(B) = 2 + \beta(A)\)</span>. The fourth equation then gives <span class="math inline">\(\beta(D) = 1 + (1/3) \beta(A) + (1/3)(2 + \beta(A)) = 5/3 + (2/3) \beta(A)\)</span>. The first equation then gives <span class="math inline">\(\beta(A) = 1 + (1/2)(2 + \beta(A)) + (1/2)[5/3 + (2/3)\beta(A)] = 17/6 + (5/6) \beta(A)\)</span>. Hence, <span class="math inline">\((1/6) \beta(A) = 17/6\)</span>, so that <span class="math inline">\(\beta(A) = 17\)</span>. Consequently, <span class="math inline">\(\beta(B) = 19\)</span> and <span class="math inline">\(\beta(D) = 5/3 + 34/3 = 13\)</span>. Finally, <span class="math inline">\(\beta(C) = 18\)</span>.</p>
<ul>
<li>Equation <a href="#eq:c4">13</a></li>
</ul>
<p>The last two equations give <span class="math inline">\(\beta(H) = 1 + (1/2)\beta(T)\)</span>. If we substitute this expression in the second equation, we get <span class="math inline">\(\beta(T) = 1 + (1/2) \beta(T) + (1/2)[1 + (1/2) \beta(T)]\)</span>, or <span class="math inline">\(\beta(T) = 3/2 + (3/4) \beta(T)\)</span>. Hence, <span class="math inline">\(\beta(T) = 6\)</span>. Consequently, <span class="math inline">\(\beta(H) = 1 + (1/2)6 = 4\)</span>. Finally, <span class="math inline">\(\beta(S) = 1 + (1/2)6 + (1/2)4 = 6\)</span>.</p>
<ul>
<li>Equation <a href="#eq:c5">14</a></li>
</ul>
<p>Let us write the equations explicitly: <span class="math display">\[\begin{aligned}
\beta(S) &amp;= 1 + \frac{1}{6} [\beta(1) + \beta(2) + \beta(3) + \beta(4) + \beta(5) + \beta(6)] \\
\beta(1) &amp;= 1 + \frac{1}{6} [\beta(1) + \beta(2) + \beta(3) + \beta(4) + \beta(5) + \beta(6)] \\
\beta(2) &amp;= 1 + \frac{1}{6} [\beta(1) + \beta(2) + \beta(3) + \beta(4) + \beta(5)] \\
\beta(3) &amp;= 1 + \frac{1}{6} [\beta(1) + \beta(2) + \beta(3) + \beta(4) + \beta(6)] \\
\beta(4) &amp;= 1 + \frac{1}{6} [\beta(1) + \beta(2) + \beta(3) + \beta(5) + \beta(6)] \\
\beta(5) &amp;= 1 + \frac{1}{6} [\beta(1) + \beta(2) + \beta(4) + \beta(5) + \beta(6)] \\
\beta(6) &amp;= 1 + \frac{1}{6} [\beta(1) + \beta(3) + \beta(4) + \beta(5) + \beta(6)].\end{aligned}\]</span> These equations are symmetric in <span class="math inline">\(\beta(2), \ldots, \beta(6)\)</span>. Let then <span class="math inline">\(\gamma = \beta(2) = \cdots = \beta(6)\)</span>. Also, <span class="math inline">\(\beta(1) = \beta(S)\)</span>. Thus, we can write these equations as <span class="math display">\[\begin{aligned}
\beta(S) &amp;= 1 + \frac{1}{6}[\beta(S) + 5 \gamma] = 1 +  \frac{1}{6} \beta(S) + \frac{5}{6} \gamma \\
\gamma   &amp;= 1 + \frac{1}{6}[ \beta(S) + 4 \gamma] = 1 + \frac{1}{6} \beta(S) + \frac{2}{3} \gamma.\end{aligned}\]</span> The first equation gives <span class="math display">\[\beta(S) = \frac{6}{5} + \gamma.\]</span> The second yields <span class="math display">\[\gamma = 3 + \frac{1}{2} \beta(S).\]</span> Substituting the next to last identity into the last one gives <span class="math display">\[\gamma = 3 + \frac{1}{2}\left[\frac{6}{5} + \gamma\right] = \frac{18}{5} + \frac{1}{2} \gamma.\]</span> Hence, <span class="math display">\[\gamma = \frac{36}{5}\]</span> and, consequently, <span class="math display">\[\beta(S) = \frac{6}{5} + \frac{36}{5} = \frac{42}{5} = 8.4.\]</span></p>
<ul>
<li>Equation <a href="#eq:c6">15</a></li>
</ul>
<p>Let us look for a solution of the form <span class="math inline">\(\beta(i) = a + b \lambda^i\)</span>. Then <span class="math display">\[a + b \lambda^i = 1 + (1 - p) (a + b) + p [a + b \lambda^{i+1}] = 1 + (1 - p) (a + b) + pa + bp \lambda^{i +1}.\]</span> This identity holds if <span class="math display">\[a = 1 + (1 - p)(a + b) + pa \text{ and } \lambda = p^{-1},\]</span> i.e., <span class="math display">\[b = - (1 - p)^{-1} \text{ and } \lambda = p^{-1}.\]</span> Then, <span class="math display">\[\beta(i) = a - (1 - p)^{-1} p^{-i}.\]</span> Since <span class="math inline">\(\beta(20) = 0\)</span>, we need <span class="math display">\[0 = a - (1 - p)^{-1} p^{- 20},\]</span> so that <span class="math inline">\(a = (1 - p)^{-1} p^{- 20}\)</span> and <span class="math display">\[\beta(i) =  (1 - p)^{-1} p^{- 20} - (1 - p)^{-1} p^{-i} = \frac{p^{-20} - p^{-i}}{1 - p}.\]</span></p>
<ul>
<li>Equation <a href="#eq:c7">16</a></li>
</ul>
<p>Let us make a little detour into the solution of such difference equations. Assume you have a function <span class="math inline">\(g(n)\)</span> such that <span class="math display">\[g(n) = 1 + (1 - p) g(n-1) + pg(n - 1)\]</span> and two functions <span class="math inline">\(\beta(n) = h(n)\)</span> and <span class="math inline">\(\beta(n) = k(n)\)</span> such that <span class="math display">\[\beta(n) = (1 - p) \beta(n-1) + p \beta(n+1).\]</span> Then, for any two constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, we note that <span class="math inline">\(\beta(n) := g(n) + a \cdot h(n) + b \cdot k(n)\)</span> satisfies <span class="math display">\[\beta(n) = 1 + (1 - p) \beta(n-1) + p \beta(n+1).\]</span> We can then choose the two constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> to make sure that <span class="math inline">\(0 = \beta(0) = \beta(M)\)</span>.</p>
<p>To find <span class="math inline">\(g(n)\)</span>, we try <span class="math inline">\(g(n) = \alpha n\)</span>. We need <span class="math display">\[\alpha n = 1 + (1 - p) \alpha (n - 1) + p \alpha (n + 1) = 1 + \alpha n - (1 - p) \alpha + p \alpha = \alpha n + 1 - \alpha + 2p \alpha.\]</span> Thus, we need <span class="math inline">\(1 - \alpha + 2 p \alpha = 0\)</span>, i.e., <span class="math inline">\(\alpha = (1 - 2p)^{-1}\)</span>.</p>
<p>To find solutions to <span class="math inline">\(\beta(n) = (1 - p) \beta(n-1) + p \beta(n+1)\)</span>, we try <span class="math inline">\(\beta(n) = \lambda^n\)</span>. Then, <span class="math display">\[\lambda^n = (1 - p) \lambda^{n-1} + p \lambda^{n+1}.\]</span> With <span class="math inline">\(n = 1\)</span>, this gives <span class="math display">\[\lambda = (1 - p) + p \lambda^2.\]</span> Hence, <span class="math display">\[p \lambda^2 - \lambda + (1 - p) = 0.\]</span> The solutions of this quadratic equation are <span class="math display">\[\lambda = \frac{ 1 \pm \sqrt{1 - 4p(1 - p)} }{2p} = \frac{1 \pm (1 - 2p)}{2p} = 1 \text{ and } \rho := (1 - p)p^{-1}.\]</span> Thus, we find two such solutions that correspond to these two values of <span class="math inline">\(\lambda\)</span>: <span class="math display">\[h(n) := 1 \mbox{ and } k(n) := \rho^n.\]</span> We now choose the two parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> so that <span class="math inline">\(\beta(n) = g(n) + a h(n) + b k(n)\)</span> satisfies the two conditions <span class="math inline">\(\beta(0) = \beta(M) = 0\)</span>. This should give us two equations in the two unknowns <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>These equations are <span class="math display">\[0  = \beta(0) = g(0) + a h(0) + b k(0) = 0 + a \times 1 + b \times 1 = a + b\]</span> and <span class="math display">\[0 = \beta(M) = g(M) + a h(M) + b k(m) = M (1 - 2p)^{-1} + a \times 1 + b \times \rho^M.\]</span> The first equation gives <span class="math inline">\(b = - a\)</span>, so that the second implies <span class="math display">\[0 = M(1 - 2p)^{-1} + a(1 - \rho^M).\]</span> Hence, <span class="math display">\[a = - \frac{M(1 - 2p)^{-1}}{1 - \rho^M}.\]</span> Finally, <span class="math display">\[\beta(n) = n (1 - 2p)^{-1} -  \frac{M(1 - 2p)^{-1}}{1 - \rho^M} (1 - \rho^n).\]</span></p>
<ul>
<li>Equation <a href="#eq:c8">17</a></li>
</ul>
<p>In the calculation of Equation <a href="#eq:c7">16</a>, we found two solutions to Equation <a href="#eq:c8">17</a>: <span class="math display">\[\alpha(n) = 1 \text{ and } \alpha(n) = \rho^n\]</span> with <span class="math inline">\(\rho = (1 - p)p^{-1}\)</span>. Hence, for any two constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, a solution is <span class="math inline">\(\alpha(n) = a + b \rho^n\)</span>. We now choose <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> so that <span class="math inline">\(\alpha(0) = 0\)</span> and <span class="math inline">\(\alpha(M) = 1\)</span>. That is, <span class="math display">\[0 = a + b \text{ and } 1 = a + b \rho^M.\]</span> Thus, <span class="math inline">\(b = - a\)</span> and <span class="math display">\[1 = a(1 - \rho^M), \text{ i.e., } a = (1 - \rho^M)^{-1}.\]</span> Hence, <span class="math display">\[\alpha(n) = a + b \rho^n = a(1 - \rho^n) = \frac{1 - \rho^n}{1 - \rho^M}.\]</span></p>
<h1 id="appendix-2-some-proofs" class="unnumbered">Appendix 2: Some Proofs</h1>
<p><em>Proof Sketch of <a href="#thm:t1">Theorem 3</a></em>. A formal proof of <a href="#thm:t1">Theorem 3</a> is a bit complicated. However, we can sketch the argument to justify the result.</p>
<p>First let us explain why Equation <a href="#eq:8">8</a> implies that the invariant distribution is unique. Assume that <span class="math inline">\(\nu = \{\nu(i), i \in {\cal X}\}\)</span> is an invariant distribution and choose <span class="math inline">\(\pi_0 = \nu\)</span>. Then <span class="math inline">\({\mathbb{P}}[X_n = i] = \nu(i)\)</span> for all <span class="math inline">\(n \geq 0\)</span>. Call <span class="math inline">\(Y_n\)</span> the fraction in Equation <a href="#eq:8">8</a>. Note that <span class="math inline">\({\mathbb{E}}[Y_n] = \nu(i)\)</span> for all <span class="math inline">\(n\)</span>, because <span class="math inline">\({\mathbb{E}}[1\{X_m = i\}] = {\mathbb{P}}[X_m = i] = \nu(i)\)</span>. Now, Equation <a href="#eq:8">8</a> says that <span class="math inline">\(Y_n \to \pi(i)\)</span>. We claim that this implies that <span class="math inline">\({\mathbb{E}}[Y_n] \to \pi(i)\)</span>, so that <span class="math inline">\(\nu(i) \to \pi(i)\)</span>, which implies that <span class="math inline">\(\nu(i) = \pi(i)\)</span>. To prove the claim, we use the fact that <span class="math inline">\(Y_n \to \pi(i)\)</span> implies that, for any <span class="math inline">\(\epsilon &gt; 0\)</span>, there is some <span class="math inline">\(n\)</span> large enough so that <span class="math inline">\({\mathbb{P}}[|Y_m - \pi(i)| \leq \epsilon] \geq 1 - \epsilon\)</span> for all <span class="math inline">\(m \geq n\)</span>. But then, because <span class="math inline">\(Y_m \in [0, 1]\)</span>, we see that <span class="math inline">\({\mathbb{E}}[|Y_m - \pi(i)|] \leq \epsilon(1 - \epsilon) + \epsilon\)</span>, so that <span class="math inline">\(|{\mathbb{E}}[Y_m] - \pi(i)| \leq {\mathbb{E}}[|Y_m - \pi(i)|] \leq 2 \epsilon\)</span>. This shows that <span class="math inline">\({\mathbb{E}}[Y_m] \to \pi(i)\)</span>.</p>
<p>The second step is to note that all the states must be <span><em>recurrent</em></span>, which means that the Markov chain visits them infinitely often. Indeed, at least one state, say state <span class="math inline">\(i\)</span>, must be recurrent since there are only finitely many states. Consider any other state <span class="math inline">\(j\)</span>. Every time that the Markov chain visits <span class="math inline">\(i\)</span>, it has a positive probability <span class="math inline">\(p\)</span> of visiting <span class="math inline">\(j\)</span> before coming back to <span class="math inline">\(i\)</span>. Otherwise, the Markov chain would never visit <span class="math inline">\(j\)</span> when starting from <span class="math inline">\(i\)</span>, which would contradict its irreducibility. Since the Markov chain visits <span class="math inline">\(i\)</span> infinitely often, it also must visit <span class="math inline">\(j\)</span> infinitely often, in the same way that if you flip a coin with <span class="math inline">\({\mathbb{P}}[H] = p &gt; 0\)</span> forever, you must see an infinite number of <span class="math inline">\(H\)</span>s.</p>
<p>The third step is to observe that the times <span class="math inline">\(T(i, 1), T(i, 2), \ldots\)</span> between successive visits to one state <span class="math inline">\(i\)</span> are independent and identically distributed, because the motion of the Markov chain starts afresh whenever it enters state <span class="math inline">\(i\)</span>. By the law of large number, <span class="math inline">\((T(i, 1) + \cdots + T(i, n))/n \to {\mathbb{E}}[T(i, 1)]\)</span> as <span class="math inline">\(n \to \infty\)</span>. Hence, <span class="math inline">\(n/(T(i, 1) + \cdots + T(i, n)) \to \pi(i) := 1/{\mathbb{E}}[T(i, 1)]\)</span>. That is, the rate of visits of state <span class="math inline">\(i\)</span> is <span class="math inline">\(\pi(i)\)</span>.</p>
<p>The fourth step is to show that <span class="math inline">\(\pi(i) &gt; 0\)</span> for all <span class="math inline">\(i\)</span>. Indeed, <span class="math inline">\(\pi(i) &gt; 0\)</span> for at least one state <span class="math inline">\(i\)</span>, otherwise the rate of visits of all the states would be zero, which is not possible since these rates of visit add up to one. Also, if the Markov chain visits state <span class="math inline">\(i\)</span> with rate <span class="math inline">\(\pi(i) &gt; 0\)</span>, then it visits state <span class="math inline">\(j\)</span> with at least rate <span class="math inline">\(\pi(i)p &gt; 0\)</span> because it visits <span class="math inline">\(j\)</span> with probability <span class="math inline">\(p\)</span> between two visits to <span class="math inline">\(i\)</span>. Hence, <span class="math inline">\(\pi(j) &gt; 0\)</span> for all <span class="math inline">\(j\)</span>.</p>
<p>The fifth step is to show that <span class="math inline">\(\pi(i)\)</span> satisfies the balance equations. We saw that, during a large number <span class="math inline">\(n\)</span> of steps, the Markov chain visits state <span class="math inline">\(j\)</span> approximately <span class="math inline">\(n \pi(j)\)</span> times. Consider then a given state <span class="math inline">\(i\)</span>. Since that state is visited with probability <span class="math inline">\(P(j, i)\)</span> after each visit to state <span class="math inline">\(j\)</span>, state <span class="math inline">\(i\)</span> should be visited approximately <span class="math inline">\(n \pi(j) P(j, i)\)</span> times immediately after the <span class="math inline">\(n \pi(j)\)</span> visits to state <span class="math inline">\(j\)</span>. If we sum over all the states <span class="math inline">\(j\)</span>, we see that state <span class="math inline">\(i\)</span> should be visited approximately <span class="math inline">\(n \sum_j \pi(j) P(j, i)\)</span> times over <span class="math inline">\(n\)</span> steps. But we know that this number of visits is approximately <span class="math inline">\(n \pi(i)\)</span>. Hence, it must be that <span class="math inline">\(n \sum_j \pi(j)P(j, i) \approx n \pi(i)\)</span>, i.e., that <span class="math inline">\(\pi(i) = \sum_j \pi(j) P(j, i)\)</span>. These are the balance equations. <span class="math inline">\(\square\)</span></p>
<p><em>Proof of <a href="#thm:t2">Theorem 4</a></em>. We give the proof in the aperiodic case, i.e., when there is some state <span class="math inline">\(i\)</span> such that <span class="math inline">\(d(i) = 1\)</span>.</p>
<p>Define <span class="math inline">\(S(i) = \{n &gt; 0 \mid P^n(i, i) &gt; 0\}\)</span>. We fist show that there is some integer <span class="math inline">\(n(i)\)</span> so that <span class="math inline">\(n \in S(i)\)</span> if <span class="math inline">\(n \geq n(i)\)</span>. Note that if <span class="math inline">\(\gcd(S(i)) = 1\)</span>, then there must be <span class="math inline">\(a, b \in S(i)\)</span> with <span class="math inline">\(\gcd\{a, b\} = 1\)</span>. Using Euclid’s extended GCD algorithm, we find integers <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> so that <span class="math inline">\(m a + n b = \gcd\{a, b\} = 1\)</span>. Let <span class="math inline">\(m^+ = \max\{0, m\}, n^+ = \max\{0, n\}, m^- = m + m^+, n^- = n + n^+\)</span>. Then <span class="math inline">\((m^+ - m^-)a + (n^+ - n^-)b = 1\)</span> and we note that <span class="math inline">\(k := m^- a + n^- b\)</span> and <span class="math inline">\(k+1 = m^+a + n^+ b\)</span> are both in <span class="math inline">\(S(i)\)</span>. Now, if <span class="math inline">\(n \geq k^2\)</span>, then one can write <span class="math inline">\(n = ak + b\)</span> for some <span class="math inline">\(b \in \{0, 1, \ldots, k -1\}\)</span> and some <span class="math inline">\(a &gt; k - 1\)</span>. But then <span class="math display">\[n = ak + b = (a - b)k + b(k+1) \in S(i),\]</span> since both <span class="math inline">\(k\)</span> and <span class="math inline">\(k + 1\)</span> are in <span class="math inline">\(S(i)\)</span>. Thus, any <span class="math inline">\(n \geq n(i) = k^2\)</span> is such that <span class="math inline">\(n \in S(i)\)</span>.</p>
<p>Next we show that <span class="math inline">\(d(j) = 1\)</span> for every state <span class="math inline">\(j\)</span>. Since it is irreducible, the Markov chain can go from <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span> is some <span class="math inline">\(a\)</span> steps and from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> in some <span class="math inline">\(b\)</span> steps. But the Markov chain can go from <span class="math inline">\(i\)</span> to <span class="math inline">\(i\)</span> in <span class="math inline">\(n(i)\)</span> or in <span class="math inline">\(n(i) + 1\)</span> steps. Consequently, the Markov chain can go from <span class="math inline">\(j\)</span> to <span class="math inline">\(j\)</span> in <span class="math inline">\(a + n(i) + b\)</span> steps and also in <span class="math inline">\(a + n(i) + 1 + b\)</span> steps by going from <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span> in <span class="math inline">\(a\)</span> steps, then from <span class="math inline">\(i\)</span> to <span class="math inline">\(i\)</span> in <span class="math inline">\(n(i)\)</span> or <span class="math inline">\(n(i) + 1\)</span> steps, then from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> in <span class="math inline">\(b\)</span> steps. Hence, <span class="math inline">\(\{n &gt; 0 \mid P^n(j, j) &gt; 0\}\)</span> contains two consecutive integers <span class="math inline">\(a + n(i) + b\)</span> and <span class="math inline">\(a + n(i) + 1 + b\)</span>, so that its g.c.d. must be equal to one. Thus, <span class="math inline">\(d(j) = 1\)</span> for every state <span class="math inline">\(j\)</span>.</p>
<p>Let us now fix a state <span class="math inline">\(i\)</span> arbitrarily. The claim is that there is some integer <span class="math inline">\(k\)</span> such that the Markov chain can go from any state <span class="math inline">\(j\)</span> to state <span class="math inline">\(i\)</span> in <span class="math inline">\(k\)</span> steps. To see this, using the irreducibility of the Markov chain, we know that for every <span class="math inline">\(j\)</span> there is some integer <span class="math inline">\(n(j, i)\)</span> so that the Markov chain can go from <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span> in <span class="math inline">\(n(j, i)\)</span> steps. But then, the Markov chain can go from <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span> in <span class="math inline">\(n + n(j, i)\)</span> steps for any <span class="math inline">\(n \geq n(j)\)</span>. Indeed, the Markov chain can go first from <span class="math inline">\(j\)</span> to <span class="math inline">\(j\)</span> in <span class="math inline">\(n\)</span> steps, then from <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span> in <span class="math inline">\(n(j, i)\)</span> steps. Thus, the Markov chain can go from <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span> in <span class="math inline">\(n\)</span> steps, for any <span class="math inline">\(n \geq n(j) + n(j, i)\)</span>. We then let <span class="math inline">\(k = \max_j \{n(j) + n(j, i)\}\)</span>.</p>
<p>Next, consider two independent copies <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Z_n\)</span> of the Markov chain with transition matrix <span class="math inline">\(P\)</span>. Markov chain <span class="math inline">\(X_n\)</span> starts with the invariant distribution <span class="math inline">\(\pi\)</span>. Markov chain <span class="math inline">\(Z_n\)</span> starts with an arbitrary initial distribution <span class="math inline">\(\pi_0\)</span>. Define state <span class="math inline">\(i\)</span> and the integer <span class="math inline">\(k\)</span> as in the previous paragraph. There is some positive probability <span class="math inline">\(p\)</span> that the two Markov chains both are in state <span class="math inline">\(i\)</span> after <span class="math inline">\(k\)</span> steps. If not, there is again a probability <span class="math inline">\(p\)</span> that they are both in state <span class="math inline">\(i\)</span> after <span class="math inline">\(k\)</span> more steps, and so on. Thus, if we designate by <span class="math inline">\(\tau\)</span> the first time that the two Markov chains meet, i.e., <span class="math inline">\(\tau = \min \{n \geq 0 \mid X_n = Z_n\}\)</span>, we see that <span class="math inline">\({\mathbb{P}}[\tau &gt; km] \leq (1 - p)^m\)</span> for <span class="math inline">\(m = 1, 2, \ldots\)</span>. Now, define the Markov chain <span class="math inline">\(Y_n\)</span> so that <span class="math inline">\(Y_n = Z_n\)</span> for <span class="math inline">\(n &lt; \tau\)</span> and <span class="math inline">\(Y_n = X_n\)</span> for <span class="math inline">\(n \geq \tau\)</span>. In words, the Markov chain starts like <span class="math inline">\(Z_n\)</span>, but it sticks to <span class="math inline">\(X_n\)</span> once <span class="math inline">\(X_n = Z_n\)</span>. This Markov chain <span class="math inline">\(Y_n\)</span> still has transition matrix <span class="math inline">\(P\)</span> and its initial distribution is <span class="math inline">\(\pi_0\)</span>. Note that <span class="math inline">\({\mathbb{P}}[X_n \neq Y_n] = {\mathbb{P}}[\tau &gt; n] \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>. Hence, <span class="math display">\[|{\mathbb{P}}[X_n = i] - {\mathbb{P}}[Y_n = i]| \leq {\mathbb{P}}[X_n \neq Y_n] \to 0, \text{ as } n \to \infty.\]</span> But <span class="math inline">\({\mathbb{P}}[X_n = i] = \pi(i)\)</span> for all <span class="math inline">\(n\)</span> since <span class="math inline">\(X_n\)</span> starts with the invariant distribution <span class="math inline">\(\pi\)</span>. We conclude that <span class="math inline">\({\mathbb{P}}[Y_n = i] \to \pi(i)\)</span> as <span class="math inline">\(n \to \infty\)</span>. <span class="math inline">\(\square\)</span></p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The observant reader will have noticed that the LHS of Equation <a href="#eq:8">8</a> is a random variable, and the RHS of Equation <a href="#eq:8">8</a> is a constant, so Equation <a href="#eq:8">8</a> does not make sense, strictly speaking! In fact, when discussing random variables, there are numerous ways to formalize the notion of convergence. One of them is <em>convergence in probability</em>, which we have already encountered when we introduced the weak law of large numbers: <a name="eq:conv-probability"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
    \lim_{n\to\infty} {\mathbb{P}}\!\left(\left| \frac{1}{n} \sum_{m=0}^{n-1} 1\{X_m = i\} - \pi(i) \right| &gt; \varepsilon\right) = 0, \qquad \forall \varepsilon &gt; 0.
\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(6)</span></span>  Another form of convergence is <em>convergence with probability <span class="math inline">\(1\)</span></em>: <a name="eq:conv-as"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
    {\mathbb{P}}\!\left(\lim_{n\to\infty} \frac{1}{n} \sum_{m=0}^{n-1} 1\{X_m = i\} = \pi(i)\right) = 1.
\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(7)</span></span>  It turns out that Equation <a href="#eq:conv-as">7</a> implies Equation <a href="#eq:conv-probability">6</a> and in this case, both types of convergence occur. Furthermore, since the LHS of Equation <a href="#eq:8">8</a> is bounded, we also have <em>convergence of expected value</em>: <span class="math display">\[
    \lim_{n\to\infty} \frac{1}{n} \sum_{m=0}^{n-1} {\mathbb{P}}(X_m = i) = \pi(i).
\]</span><a href="#fnref1">↩</a></p></li>
</ol>
</div>
</article>
</body>

<!-- Mirrored from www.eecs70.org/static/notes/n24.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:43:27 GMT -->
</html>
