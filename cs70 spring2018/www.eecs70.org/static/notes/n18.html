<!DOCTYPE html>
<!--==============================================================================
	           "GitHub HTML5 Pandoc Template" v1.2 — by Tristano Ajmone           
	==============================================================================
	(c) Tristano Ajmone, 2017, MIT License (MIT). Project's home repository:

	- https://github.com/tajmone/pandoc-goodies

	This template reuses source code taken from the following projects:

	- GitHub Markdown CSS: © Sindre Sorhus, MIT License (MIT):
	  https://github.com/sindresorhus/github-markdown-css

	- Primer CSS: © 2016 GitHub Inc., MIT License (MIT):
	  http://primercss.io/
	==============================================================================-->
<html>

<!-- Mirrored from www.eecs70.org/static/notes/n18.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:40:58 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Chebyshev's Inequality</title>
<style type="text/css">@font-face{font-family:octicons-link;src:url(data:font/woff;charset=utf-8;base64,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) format('woff')}.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;color:#24292e;font-family:-apple-system,system-ui,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:16px;line-height:1.5;word-wrap:break-word;box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto;padding:45px}.markdown-body .octicon{display:inline-block;fill:currentColor;vertical-align:text-bottom}.markdown-body a{background-color:transparent;-webkit-text-decoration-skip:objects;color:#0366d6;text-decoration:none}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body h1{margin:.67em 0}.markdown-body img{border-style:none}.markdown-body svg:not(:root){overflow:hidden}.markdown-body code,.markdown-body kbd,.markdown-body pre{font-family:monospace,monospace}.markdown-body input{font:inherit;margin:0;overflow:visible;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=checkbox]{box-sizing:border-box;padding:0}.markdown-body *{box-sizing:border-box}.markdown-body a:hover{text-decoration:underline}.markdown-body strong{font-weight:600}.markdown-body hr{box-sizing:content-box;overflow:hidden;background:0 0;border-bottom:1px solid #dfe2e5}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body table{border-spacing:0;border-collapse:collapse;display:block;width:100%;overflow:auto}.markdown-body td,.markdown-body th{padding:0}.markdown-body blockquote{margin:0}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body dd{margin-left:0}.markdown-body code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace}.markdown-body pre{font:12px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;word-wrap:normal}.markdown-body .pl-0{padding-left:0!important}.markdown-body .pl-1{padding-left:4px!important}.markdown-body .pl-2{padding-left:8px!important}.markdown-body .pl-3{padding-left:16px!important}.markdown-body .pl-4{padding-left:24px!important}.markdown-body .pl-5{padding-left:32px!important}.markdown-body .pl-6{padding-left:40px!important}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:0}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e1e4e8;border:0}.markdown-body blockquote{padding:0 1em;color:#6a737d;border-left:.25em solid #dfe2e5}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd{font-size:11px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#1b1f23;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1{padding-bottom:.3em;font-size:2em;border-bottom:1px solid #eaecef}.markdown-body h2{padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid #eaecef}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#6a737d}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #dfe2e5}.markdown-body table tr{background-color:#fff;border-top:1px solid #c6cbd1}.markdown-body table tr:nth-child(2n){background-color:#f6f8fa}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body code{padding:.2em 0;margin:0;font-size:85%;background-color:rgba(27,31,35,.05);border-radius:3px}.markdown-body code::after,.markdown-body code::before{letter-spacing:-.2em;content:"\00a0"}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f6f8fa;border-radius:3px}.markdown-body pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code::after,.markdown-body pre code::before{content:normal}.markdown-body .full-commit .btn-outline:not(:disabled):hover{color:#005cc5;border-color:#005cc5}.markdown-body kbd{display:inline-block;padding:3px 5px;font:11px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fcfcfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body :checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item+.task-list-item{margin-top:3px}.markdown-body .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown-body hr{border-bottom-color:#eee}.flash{position:relative;padding:16px;color:#246;background-color:#e2eef9;border:1px solid #bac6d3;border-radius:3px}.flash p:last-child{margin-bottom:0}.flash-messages{margin-bottom:24px}.flash-warn{color:#4c4a42;background-color:#fff9ea;border-color:#dfd8c2}.flash-error{color:#911;background-color:#fcdede;border-color:#d2b2b2}.flash-success{color:#22662c;background-color:#e2f9e5;border-color:#bad3be}.flash-plain{color:#4c4a42;background-color:#f5f5f5;border-color:#c1c1c1}.figure{text-align:center;}</style>
  <style type="text/css">code{white-space: pre;}</style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {equationNumbers: {autoNumber: "AMS"}}
    });
  </script>
  <script src="../../../cdn.mathjax.org/mathjax/latest/MathJax2ba6.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title">Chebyshev's Inequality</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#chebyshevs-inequality">Chebyshev’s Inequality</a><ul>
<li><a href="#problem-estimating-the-bias-of-a-coin">Problem: Estimating the Bias of a Coin</a></li>
<li><a href="#markovs-inequality">Markov’s Inequality</a></li>
<li><a href="#chebyshevs-inequality-1">Chebyshev’s Inequality</a></li>
<li><a href="#example-estimating-the-bias-of-a-coin">Example: Estimating the Bias of a Coin</a></li>
<li><a href="#estimating-a-general-expectation">Estimating a General Expectation</a></li>
</ul></li>
<li><a href="#the-law-of-large-numbers">The Law of Large Numbers</a></li>
</ul>
</nav>
<h1 id="chebyshevs-inequality" class="unnumbered">Chebyshev’s Inequality</h1>
<h3 id="problem-estimating-the-bias-of-a-coin" class="unnumbered">Problem: Estimating the Bias of a Coin</h3>
<p>Suppose we have a biased coin, but we don’t know what the bias is. To estimate the bias, we toss the coin <span class="math inline">\(n\)</span> times and count how many Heads we observe. Then our estimate of the bias is given by <span class="math inline">\(\hat p = S_n/n\)</span>, where <span class="math inline">\(S_n\)</span> is the number of Heads in the <span class="math inline">\(n\)</span> tosses. Is this a good estimate? Let <span class="math inline">\(p\)</span> denote the true bias of the coin, which is unknown to us. Since <span class="math inline">\({\mathbb{E}}[S_n] = np\)</span>, we see that the estimator <span class="math inline">\(\hat p\)</span> has the correct expected value: <span class="math inline">\({\mathbb{E}}[{\hat p}] = {\mathbb{E}}[S_n]/n = p\)</span>. This means when <span class="math inline">\(n\)</span> is sufficiently large, we can expect <span class="math inline">\(\hat p\)</span> to be very close to <span class="math inline">\(p\)</span>; this is a manifestation of the <span><em>Law of Large Numbers</em></span>, which we shall see at the end of this note.</p>
<p>How large should <span class="math inline">\(n\)</span> be to guarantee that our estimate <span class="math inline">\(\hat p\)</span> is within an error <span class="math inline">\(\epsilon\)</span> of the true bias <span class="math inline">\(p\)</span>, i.e., <span class="math inline">\(|\hat p - p| \le \epsilon\)</span>? The answer is that we can never guarantee with absolute certainty that <span class="math inline">\(|\hat p - p| \le \epsilon\)</span>. This is because <span class="math inline">\(S_n\)</span> is a random variable that can take any integer values between <span class="math inline">\(0\)</span> and <span class="math inline">\(n\)</span>, and thus <span class="math inline">\(\hat p = S_n/n\)</span> is also a random variable that can take any values between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. So regardless of the value of the true bias <span class="math inline">\(p \in (0,1)\)</span>, it is possible that in our experiment of <span class="math inline">\(n\)</span> coin tosses we observe <span class="math inline">\(n\)</span> Heads (this happens with probability <span class="math inline">\(p^n\)</span>), in which case <span class="math inline">\(S_n = n\)</span> and <span class="math inline">\(\hat p = 1\)</span>. Similarly, it is also possible that all <span class="math inline">\(n\)</span> coin tosses come up Tails (this happens with probability <span class="math inline">\((1-p)^n\)</span>), in which case <span class="math inline">\(S_n = 0\)</span> and <span class="math inline">\(\hat p = 0\)</span>.</p>
<p>So instead of requiring that <span class="math inline">\(|\hat p - p| \le \epsilon\)</span> with absolute certainty, we relax our requirement and only demand that <span class="math inline">\(|\hat p - p| \le \epsilon\)</span> with <span><em>confidence</em></span> <span class="math inline">\(\delta\)</span>, namely, <span class="math inline">\({\mathbb{P}}[|\hat p - p| \le \epsilon] \ge 1-\delta\)</span>. This means there is a small probability <span class="math inline">\(\delta\)</span> that we make an error of more than <span class="math inline">\(\epsilon\)</span>, but with high probability (at least <span class="math inline">\(1-\delta\)</span>) our estimate is very close to <span class="math inline">\(p\)</span>. Now we can state our result: to guarantee that <span class="math inline">\({\mathbb{P}}[|\hat p - p| \le \epsilon] \ge 1-\delta\)</span>, it suffices to toss the coin <span class="math inline">\(n\)</span> times where <span class="math display">\[n \ge \frac{1}{4\epsilon^2 \delta}.\]</span></p>
<p>To prove such a result we use the tool called <span><em>Chebyshev’s Inequality</em></span>, which provides a quantitative bound on how far away a random variable is from its expected value.</p>
<h2 id="markovs-inequality" class="unnumbered">Markov’s Inequality</h2>
<p>Before going to Chebyshev’s inequality, we first state the following simpler bound, which applies only to <span><em>non-negative</em></span> random variables (i.e., r.v.’s which take only values <span class="math inline">\(\ge 0\)</span>).</p>
<p><span id="theorem:1" class="pandoc-numbering-text theorem"><strong>Theorem 1</strong> <em>(Markov's Inequality)</em></span></p>
<p><em>For a <span><em>non-negative</em></span> random variable <span class="math inline">\(X\)</span> with expectation <span class="math inline">\({\mathbb{E}}[X]=\mu\)</span>, and any <span class="math inline">\(\alpha&gt;0\)</span>, <span class="math display">\[{\mathbb{P}}[X\ge\alpha]\le{{{\mathbb{E}}[X]}\over\alpha}.\]</span></em></p>
<p><em>Proof</em>. From the definition of expectation, we have <span class="math display">\[\begin{aligned} {\mathbb{E}}[X] &amp;=\sum_a a\times{\mathbb{P}}[X=a]\cr
               &amp;\ge \sum_{a\ge\alpha} a\times{\mathbb{P}}[X=a]\cr
               &amp;\ge \alpha\sum_{a\ge\alpha} {\mathbb{P}}[X=a]\cr
               &amp;=\alpha{\mathbb{P}}[X\ge\alpha]. \end{aligned}\]</span> Rearranging the inequality gives us the desired result. The crucial step here is the second line, where we have used the fact that <span class="math inline">\(X\)</span> takes on only non-negative values. (Why is this step not valid otherwise?) <span class="math inline">\(\square\)</span></p>
<p>There is an intuitive way of understanding Markov’s inequality through an analogy of a seesaw. Imagine that the distribution of a non-negative random variable <span class="math inline">\(X\)</span> is resting on a fulcrum, <span class="math inline">\(\mu = {\mathbb{E}}[X]\)</span>. We are trying to find an upper bound on the percentage of the distribution which lies beyond <span class="math inline">\(k \mu\)</span>, i.e., <span class="math inline">\({\mathbb{P}}[X \ge k \mu]\)</span>. In other words, we seek to add as much weight <span class="math inline">\(m_2\)</span> as possible on the seesaw at <span class="math inline">\(k \mu\)</span> while minimizing the effect it has on the seesaw’s balance. This weight will represent the upper bound we are searching for. To minimize the weight’s effect, we must imagine that the weight of the distribution which lies beyond <span class="math inline">\(k \mu\)</span> is concentrated at exactly <span class="math inline">\(k \mu\)</span>. However, to keep things stable and maximize the weight at <span class="math inline">\(k \mu\)</span>, we must add another weight <span class="math inline">\(m_1\)</span> as far left to the fulcrum as we can so that <span class="math inline">\(m_2\)</span> is as large as it can be. The farthest we can go to the left is <span class="math inline">\(0\)</span>, since <span class="math inline">\(X\)</span> is assumed to be non-negative. Moreover, the two weights <span class="math inline">\(m_1\)</span> and <span class="math inline">\(m_2\)</span> must add up to <span class="math inline">\(1\)</span>, since they represent the area under the distribution curve:</p>
<div class="figure">
<img src="n18-seesaw.png" alt="Figure 1: Markov’s inequality interpreted as balancing a seesaw." id="fig:seesaw" style="width:50.0%" />
<p class="caption">Figure 1: Markov’s inequality interpreted as balancing a seesaw.</p>
</div>
<p>Since the lever arms are in the ratio <span class="math inline">\(k-1\)</span> to <span class="math inline">\(1\)</span>, a unit weight at <span class="math inline">\(k \mu\)</span> balances <span class="math inline">\(k-1\)</span> units of weight at <span class="math inline">\(0\)</span>. So the weights should be <span class="math inline">\((k-1)/k\)</span> at <span class="math inline">\(0\)</span> and <span class="math inline">\(1/k\)</span> at <span class="math inline">\(k\mu\)</span>, which is exactly Markov’s bound with <span class="math inline">\(\alpha = k\mu\)</span>.</p>
<h2 id="chebyshevs-inequality-1" class="unnumbered">Chebyshev’s Inequality</h2>
<p>We have seen that, intuitively, the variance (or, more correctly the standard deviation) is a measure of “spread,” or deviation from the mean. We can now make this intuition quantitatively precise:</p>
<p><span id="theorem:2" class="pandoc-numbering-text theorem"><strong>Theorem 2</strong> <em>(Chebyshev's Inequality)</em></span></p>
<p><em>For a random variable <span class="math inline">\(X\)</span> with expectation <span class="math inline">\({\mathbb{E}}[X]=\mu\)</span>, and for any <span class="math inline">\(\alpha&gt;0\)</span>, <span class="math display">\[{\mathbb{P}}[|X-\mu|\ge\alpha] \le {{{\operatorname{var}}(X)}\over{\alpha^2}}.\]</span></em></p>
<p><em>Proof</em>. Define the random variable <span class="math inline">\(Y=(X-\mu)^2\)</span>. Note that <span class="math inline">\({\mathbb{E}}[Y]={\mathbb{E}}[(X-\mu)^2]={\operatorname{var}}(X)\)</span>. Also, notice that the event that we are interested in, <span class="math inline">\(|X-\mu| \ge \alpha\)</span>, is exactly the same as the event <span class="math inline">\(Y = (X-\mu)^2 \ge \alpha^2\)</span>. Therefore, <span class="math inline">\({\mathbb{P}}[|X-\mu| \ge \alpha] = {\mathbb{P}}[Y \ge \alpha^2]\)</span>. Moreover, <span class="math inline">\(Y\)</span> is obviously non-negative, so we can apply Markov’s inequality to it to get <span class="math display">\[{\mathbb{P}}[Y\ge\alpha^2] \le {{{\mathbb{E}}[Y]}\over{\alpha^2}} = {{{\operatorname{var}}(X)}\over{\alpha^2}}.\]</span> This completes the proof. <span class="math inline">\(\square\)</span></p>
<p>Let’s pause to consider what Chebyshev’s inequality says. It tells us that the probability of any given deviation, <span class="math inline">\(\alpha\)</span>, from the mean, either above it or below it (note the absolute value sign), is at most <span class="math inline">\({\operatorname{var}}(X)/\alpha^2\)</span>. As expected, this deviation probability will be small if the variance is small. An immediate corollary of Chebyshev’s inequality is the following:</p>
<p><span id="corollary:1" class="pandoc-numbering-text corollary"><strong>Corollary 1</strong></span></p>
<p><em>For a random variable <span class="math inline">\(X\)</span> with expectation <span class="math inline">\({\mathbb{E}}[X]=\mu\)</span>, and standard deviation <span class="math inline">\(\sigma=\sqrt{{\operatorname{var}}(X)}\)</span>, <span class="math display">\[{\mathbb{P}}[|X-\mu|\ge\beta\sigma] \le {1\over{\beta^2}}.\]</span></em></p>
<p><em>Proof</em>. Plug <span class="math inline">\(\alpha=\beta\sigma\)</span> into Chebyshev’s inequality. <span class="math inline">\(\square\)</span></p>
<p>So, for example, we see that the probability of deviating from the mean by more than (say) two standard deviations on either side is at most <span class="math inline">\(1/4\)</span>. In this sense, the standard deviation is a good working definition of the “width” or “spread” of a distribution.</p>
<p>In some special cases it is possible to get tighter bounds on the probability of deviations from the mean. However, for general random variables Chebyshev’s inequality is essentially the only tool. Its power derives from the fact that it can be applied to <span><em>any</em></span> random variable.</p>
<h2 id="example-estimating-the-bias-of-a-coin" class="unnumbered">Example: Estimating the Bias of a Coin</h2>
<p>Let us go back to our motivating example of estimating the bias of a coin. Recall that we have a coin of unknown bias <span class="math inline">\(p\)</span>, and our estimate of <span class="math inline">\(p\)</span> is <span class="math inline">\(\hat p = S_n/n\)</span> where <span class="math inline">\(S_n\)</span> is the number of Heads in <span class="math inline">\(n\)</span> coin tosses.</p>
<p>As usual, we will find it helpful to write <span class="math inline">\(S_n = X_1 + \cdots + X_n\)</span>, where <span class="math inline">\(X_i = 1\)</span> is the <span class="math inline">\(i\)</span>-th coin toss comes up Heads and <span class="math inline">\(X_i = 0\)</span> otherwise, and the random variables <span class="math inline">\(X_1,\dots,X_n\)</span> are mutually independent. Then <span class="math inline">\({\mathbb{E}}[X_i] = {\mathbb{P}}[X_i = 1] = p\)</span>, so by linearity of expectation, <span class="math display">\[{\mathbb{E}}[\hat p] = {\mathbb{E}}\!\left[\frac{1}{n} S_n\right] = \frac{1}{n} \sum_{i=1}^n {\mathbb{E}}[X_i] = p.\]</span> What about the variance of <span class="math inline">\(\hat p\)</span>? Note that since the <span class="math inline">\(X_i\)</span>’s are independent, the variance of <span class="math inline">\(S_n = \sum_{i=1}^n X_i\)</span> is equal to the sum of the variances: <span class="math display">\[{\operatorname{var}}(\hat p) = {\operatorname{var}}\!\left(\frac{1}{n} S_n\right) = \frac{1}{n^2} {\operatorname{var}}(S_n)
= \frac{1}{n^2} \sum_{i=1}^n {\operatorname{var}}(X_i) = \frac{\sigma^2}{n},\]</span> where we have written <span class="math inline">\(\sigma^2\)</span> for the variance of each of the <span class="math inline">\(X_i\)</span>.</p>
<p>So we see that <em>the variance of <span class="math inline">\(\hat{p}\)</span> decreases linearly with <span class="math inline">\(n\)</span></em>. This fact ensures that, as we take larger and larger sample sizes <span class="math inline">\(n\)</span>, the probability that we deviate much from the expectation <span class="math inline">\(p\)</span> gets smaller and smaller.</p>
<p>Let’s now use Chebyshev’s inequality to figure out how large <span class="math inline">\(n\)</span> has to be to ensure a specified accuracy in our estimate of the true bias <span class="math inline">\(p\)</span>. As we discussed in the beginning of this note, a natural way to measure this is for us to specify two parameters, <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\delta\)</span>, both in the range <span class="math inline">\((0,1)\)</span>. The parameter <span class="math inline">\(\epsilon\)</span> controls the <span><em>error</em></span> we are prepared to tolerate in our estimate, and <span class="math inline">\(\delta\)</span> controls the <span><em>confidence</em></span> we want to have in our estimate.</p>
<p>Applying Chebyshev’s inequality, we have <span class="math display">\[{\mathbb{P}}[|\hat p-p|\ge \epsilon] \le \frac{{\operatorname{var}}(\hat p)}{\epsilon^2}
                              = \frac{\sigma^2}{n\epsilon^2}.\]</span> To make this less than the desired value <span class="math inline">\(\delta\)</span>, we need to set <a name="eq:2"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   n\ge {{\sigma^2}\over{\epsilon^2\delta}}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(1)</span></span>  Now recall that <span class="math inline">\(\sigma^2={\operatorname{var}}(X_i)\)</span> is the variance of a single sample <span class="math inline">\(X_i\)</span>. So, since <span class="math inline">\(X_i\)</span> is an indicator random variable with <span class="math inline">\({\mathbb{P}}[X_i = 1] = p\)</span>, we have <span class="math inline">\(\sigma^2 = p(1-p)\)</span>, and Equation <a href="#eq:2">1</a> becomes <a name="eq:3"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   n\ge {{p(1-p)}\over {\epsilon^2\delta}}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(2)</span></span>  Since <span class="math inline">\(p(1-p)\)</span> is takes on its maximum value for <span class="math inline">\(p=1/2\)</span>, we can conclude that it is sufficient to choose <span class="math inline">\(n\)</span> such that: <span class="math display">\[\label{eq3}
   n\ge {{1}\over {4\epsilon^2\delta}},\]</span> as we claimed earlier.</p>
<p>For example, plugging in <span class="math inline">\(\epsilon=0.1\)</span> and <span class="math inline">\(\delta=0.05\)</span>, we see that a sample size of <span class="math inline">\(n=500\)</span> is sufficient to get an estimate <span class="math inline">\(\hat p\)</span> that is accurate to within an error of <span class="math inline">\(0.1\)</span> with probability at least <span class="math inline">\(95\%\)</span>.</p>
<p>As a concrete example, consider the problem of estimating the proportion <span class="math inline">\(p\)</span> of Democrats in the U.S. population, by taking a small random sample. We can model this as the problem of estimating the bias of a coin above, where each coin toss corresponds to a person that we select randomly from the entire population. Our calculation above shows that to get an estimate <span class="math inline">\(\hat p\)</span> that is accurate to within an error of <span class="math inline">\(0.1\)</span> with probability at least <span class="math inline">\(95\%\)</span>, it suffices to sample <span class="math inline">\(n = 500\)</span> people. In particular, notice that the size of the sample is independent of the total size of the population! This is how polls can accurately estimate quantities of interest for a population of several hundred million while sampling only a very small number of people.</p>
<h2 id="estimating-a-general-expectation" class="unnumbered">Estimating a General Expectation</h2>
<p>What if we wanted to estimate something a little more complex than the bias of a coin? For example, suppose we want to estimate the average wealth of people in the U.S. We can model this as the problem of estimating the expected value of an unknown probability distribution. Then we can use exactly the same scheme as before, except that now we sample the random variables <span class="math inline">\(X_1,X_2,\dots,X_n\)</span> independently from our unknown distribution. Clearly <span class="math inline">\({\mathbb{E}}[X_i]=\mu\)</span>, the expected value that we are trying to estimate. Our estimate of <span class="math inline">\(\mu\)</span> will be <span class="math inline">\(\hat \mu= n^{-1} \sum_{i=1}^n X_i\)</span>, for a suitably chosen sample size <span class="math inline">\(n\)</span>.</p>
<p>Following the same calculation as before, we have <span class="math inline">\({\mathbb{E}}[\hat \mu]=\mu\)</span> and <span class="math inline">\({\operatorname{var}}(\hat \mu)= \sigma^2/n\)</span>, where <span class="math inline">\(\sigma^2={\operatorname{var}}(X_i)\)</span> is the variance of the <span class="math inline">\(X_i\)</span>. (Recall that the only facts we used about the <span class="math inline">\(X_i\)</span> was that they were independent and had the same distribution — actually the same expectation and variance would be enough: why?) This time, however, since we do not have any a priori bound on the mean <span class="math inline">\(\mu\)</span>, it makes more sense to let <span class="math inline">\(\epsilon\)</span> be the relative error, i.e., we wish to find an estimate <span class="math inline">\(\hat \mu\)</span> that is within an additive error of <span class="math inline">\(\epsilon\mu\)</span>: <span class="math display">\[{\mathbb{P}}[|\hat \mu-\mu|\ge \epsilon\mu] \le \delta.\]</span> Using Equation <a href="#eq:2">1</a>, but substituting <span class="math inline">\(\epsilon\mu\)</span> in place of <span class="math inline">\(\epsilon\)</span>, it is enough for the sample size <span class="math inline">\(n\)</span> to satisfy <a name="eq:4"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   n\ge {{\sigma^2}\over{\mu^2}}\times{1\over{\epsilon^2\delta}}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(3)</span></span>  Here <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\delta\)</span> are the desired relative error and confidence respectively. Now of course we don’t know the other two quantities, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, appearing in Equation <a href="#eq:4">3</a>. In practice, we would use a lower bound on <span class="math inline">\(\mu\)</span> and an upper bound on <span class="math inline">\(\sigma^2\)</span> (just as we used an upper bound on <span class="math inline">\(p(1-p)\)</span> in the coin tossing problem). Plugging these bounds into equation will ensure that our sample size is large enough.</p>
<p>For example, in the average wealth problem we could probably safely take <span class="math inline">\(\mu\)</span> to be at least (say) $20k (probably more). However, the existence of very wealthy people such as Bill Gates means that we would need to take a very high value for the variance <span class="math inline">\(\sigma^2\)</span>. Indeed, if there is at least one individual with wealth $50 billion, then assuming a relatively small value of <span class="math inline">\(\mu\)</span> means that the variance must be at least about <span class="math inline">\((50\times 10^9)^2 / (250\times 10^6) = 10^{13}\)</span>. There is really no way around this problem with simple uniform sampling: the uneven distribution of wealth means that the variance is inherently very large, and we will need a huge number of samples before we are likely to find anybody who is immensely wealthy. But if we don’t include such people in our sample, then our estimate will be way too low.</p>
<h1 id="the-law-of-large-numbers" class="unnumbered">The Law of Large Numbers</h1>
<p>The estimation method we used in the previous sections is based on a principle that we accept as part of everyday life: namely, the Law of Large Numbers (LLN). This asserts that, if we observe some random variable many times, and take the average of the observations, then this average will converge to a <span><em>single value</em></span>, which is of course the expectation of the random variable. In other words, averaging tends to smooth out any large fluctuations, and the more averaging we do the better the smoothing.</p>
<p><span id="theorem:3" class="pandoc-numbering-text theorem"><strong>Theorem 3</strong> <em>(Law of Large Numbers)</em></span></p>
<p><em>Let <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> be i.i.d. random variables with common expectation <span class="math inline">\(\mu={\mathbb{E}}[X_i]\)</span>. Define <span class="math inline">\(A_n= n^{-1} \sum_{i=1}^n X_i\)</span>. Then for any <span class="math inline">\(\alpha&gt;0\)</span>, we have <span class="math display">\[{\mathbb{P}}\left[|A_n-\mu|\ge\alpha\right] \to 0 \qquad \text{as $n\to\infty$}.\]</span></em></p>
<p><em>Proof</em>. Let <span class="math inline">\({\operatorname{var}}(X_i)=\sigma^2\)</span> be the common variance of the r.v.’s; we assume that <span class="math inline">\(\sigma^2\)</span> is finite.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> With this (relatively mild) assumption, the LLN is an immediate consequence of Chebyshev’s inequality. For, as we have seen above, <span class="math inline">\({\mathbb{E}}[A_n]=\mu\)</span> and <span class="math inline">\({\operatorname{var}}(A_n)= \sigma^2/n\)</span>, so by Chebyshev we have <span class="math display">\[{\mathbb{P}}\left[|A_n-\mu|\ge\alpha\right] \le \frac{{\operatorname{var}}(A_n)}{\alpha^2} = \frac{\sigma^2}{n\alpha^2}  \to 0 \qquad\text{as $n\to\infty$}.\]</span> This completes the proof. <span class="math inline">\(\square\)</span></p>
<p>Notice that the LLN says that the probability of <span><em>any</em></span> deviation <span class="math inline">\(\alpha\)</span> from the mean, however small, tends to zero as the number of observations <span class="math inline">\(n\)</span> in our average tends to infinity. Thus by taking <span class="math inline">\(n\)</span> large enough, we can make the probability of any given deviation as small as we like.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>If <span class="math inline">\(\sigma^2\)</span> is not finite, the LLN still holds but the proof is much trickier.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
</article>
</body>

<!-- Mirrored from www.eecs70.org/static/notes/n18.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:41:27 GMT -->
</html>
