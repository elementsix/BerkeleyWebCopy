<!DOCTYPE html>
<!--==============================================================================
	           "GitHub HTML5 Pandoc Template" v1.2 — by Tristano Ajmone           
	==============================================================================
	(c) Tristano Ajmone, 2017, MIT License (MIT). Project's home repository:

	- https://github.com/tajmone/pandoc-goodies

	This template reuses source code taken from the following projects:

	- GitHub Markdown CSS: © Sindre Sorhus, MIT License (MIT):
	  https://github.com/sindresorhus/github-markdown-css

	- Primer CSS: © 2016 GitHub Inc., MIT License (MIT):
	  http://primercss.io/
	==============================================================================-->
<html>

<!-- Mirrored from www.eecs70.org/static/notes/n19.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:41:27 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Some Important Distributions</title>
<style type="text/css">@font-face{font-family:octicons-link;src:url(data:font/woff;charset=utf-8;base64,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) format('woff')}.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;color:#24292e;font-family:-apple-system,system-ui,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:16px;line-height:1.5;word-wrap:break-word;box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto;padding:45px}.markdown-body .octicon{display:inline-block;fill:currentColor;vertical-align:text-bottom}.markdown-body a{background-color:transparent;-webkit-text-decoration-skip:objects;color:#0366d6;text-decoration:none}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body h1{margin:.67em 0}.markdown-body img{border-style:none}.markdown-body svg:not(:root){overflow:hidden}.markdown-body code,.markdown-body kbd,.markdown-body pre{font-family:monospace,monospace}.markdown-body input{font:inherit;margin:0;overflow:visible;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=checkbox]{box-sizing:border-box;padding:0}.markdown-body *{box-sizing:border-box}.markdown-body a:hover{text-decoration:underline}.markdown-body strong{font-weight:600}.markdown-body hr{box-sizing:content-box;overflow:hidden;background:0 0;border-bottom:1px solid #dfe2e5}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body table{border-spacing:0;border-collapse:collapse;display:block;width:100%;overflow:auto}.markdown-body td,.markdown-body th{padding:0}.markdown-body blockquote{margin:0}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body dd{margin-left:0}.markdown-body code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace}.markdown-body pre{font:12px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;word-wrap:normal}.markdown-body .pl-0{padding-left:0!important}.markdown-body .pl-1{padding-left:4px!important}.markdown-body .pl-2{padding-left:8px!important}.markdown-body .pl-3{padding-left:16px!important}.markdown-body .pl-4{padding-left:24px!important}.markdown-body .pl-5{padding-left:32px!important}.markdown-body .pl-6{padding-left:40px!important}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:0}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e1e4e8;border:0}.markdown-body blockquote{padding:0 1em;color:#6a737d;border-left:.25em solid #dfe2e5}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd{font-size:11px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#1b1f23;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1{padding-bottom:.3em;font-size:2em;border-bottom:1px solid #eaecef}.markdown-body h2{padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid #eaecef}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#6a737d}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #dfe2e5}.markdown-body table tr{background-color:#fff;border-top:1px solid #c6cbd1}.markdown-body table tr:nth-child(2n){background-color:#f6f8fa}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body code{padding:.2em 0;margin:0;font-size:85%;background-color:rgba(27,31,35,.05);border-radius:3px}.markdown-body code::after,.markdown-body code::before{letter-spacing:-.2em;content:"\00a0"}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f6f8fa;border-radius:3px}.markdown-body pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code::after,.markdown-body pre code::before{content:normal}.markdown-body .full-commit .btn-outline:not(:disabled):hover{color:#005cc5;border-color:#005cc5}.markdown-body kbd{display:inline-block;padding:3px 5px;font:11px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fcfcfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body :checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item+.task-list-item{margin-top:3px}.markdown-body .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown-body hr{border-bottom-color:#eee}.flash{position:relative;padding:16px;color:#246;background-color:#e2eef9;border:1px solid #bac6d3;border-radius:3px}.flash p:last-child{margin-bottom:0}.flash-messages{margin-bottom:24px}.flash-warn{color:#4c4a42;background-color:#fff9ea;border-color:#dfd8c2}.flash-error{color:#911;background-color:#fcdede;border-color:#d2b2b2}.flash-success{color:#22662c;background-color:#e2f9e5;border-color:#bad3be}.flash-plain{color:#4c4a42;background-color:#f5f5f5;border-color:#c1c1c1}.figure{text-align:center;}</style>
  <style type="text/css">code{white-space: pre;}</style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {equationNumbers: {autoNumber: "AMS"}}
    });
  </script>
  <script src="../../../cdn.mathjax.org/mathjax/latest/MathJax2ba6.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title">Some Important Distributions</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#some-important-distributions">Some Important Distributions</a><ul>
<li><a href="#geometric-distribution">Geometric Distribution</a><ul>
<li><a href="#application-the-coupon-collectors-problem">Application: The Coupon Collector’s Problem</a></li>
</ul></li>
<li><a href="#poisson-distribution">Poisson Distribution</a><ul>
<li><a href="#poisson-and-coin-flips">Poisson and Coin Flips</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<h1 id="some-important-distributions" class="unnumbered">Some Important Distributions</h1>
<p>Recall our basic probabilistic experiment of tossing a biased coin <span class="math inline">\(n\)</span> times. This is a very simple model, yet surprisingly powerful. Many important probability distributions that are widely used to model real-world phenomena can be derived from looking at this basic coin tossing model.</p>
<p>The first example, which we have seen in Note 16, is the <span><em>binomial distribution</em></span> <span class="math inline">\({\operatorname{Bin}}(n,p)\)</span>. This is the distribution of the number of Heads, <span class="math inline">\(S_n\)</span>, in <span class="math inline">\(n\)</span> tosses of a biased coin with probability <span class="math inline">\(p\)</span> to be Heads. Recall that the distribution of <span class="math inline">\(S_n\)</span> is <span class="math inline">\({\mathbb{P}}[S_n = k] = {n \choose k} p^k (1-p)^{n-k}\)</span> for <span class="math inline">\(k \in \{0,1,\dots,n\}\)</span>. The expected value is <span class="math inline">\({\mathbb{E}}[S_n] = np\)</span> and the variance is <span class="math inline">\({\operatorname{var}}(S_n) = np(1-p)\)</span>. The binomial distribution frequently appears to model the number of successes in a repeated experiment.</p>
<h2 id="geometric-distribution" class="unnumbered">Geometric Distribution</h2>
<p>Consider tossing a biased coin with Heads probability <span class="math inline">\(p\)</span> repeatedly. Let <span class="math inline">\(X\)</span> denote the number of tosses until the first Head appears. Then <span class="math inline">\(X\)</span> is a random variable that takes values in the set of positive integers <span class="math inline">\(\{1,2,3,\dots\}\)</span>. The event that <span class="math inline">\(X = i\)</span> is equal to the event of observing Tails for the first <span class="math inline">\(i-1\)</span> tosses and getting Heads in the <span class="math inline">\(i\)</span>-th toss, which occurs with probability <span class="math inline">\((1-p)^{i-1}p\)</span>. Such a random variable is called a geometric random variable.</p>
<p>The geometric distribution frequently occurs in applications because we are often interested in how long we have to wait before a certain event happens: how many runs before the system fails, how many shots before one is on target, how many poll samples before we find a Democrat, how many retransmissions of a packet before successfully reaching the destination, etc.</p>
<p><span id="definition:1" class="pandoc-numbering-text definition"><strong>Definition 1</strong> <em>(Geometric Distribution)</em></span></p>
<p><em>A random variable <span class="math inline">\(X\)</span> for which <span class="math display">\[{\mathbb{P}}[X=i] = (1-p)^{i-1}p\qquad \text{for $i=1,2,3,\ldots$}\]</span> is said to have the geometric distribution with parameter <span class="math inline">\(p\)</span>. This is abbreviated as <span class="math inline">\(X \sim {\operatorname{Geom}}(p)\)</span>.</em></p>
<p>As a sanity check, we can verify that the total probability of <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(1\)</span>: <span class="math display">\[\sum_{i=1}^\infty {\mathbb{P}}[X = i] = \sum_{i=1}^\infty (1-p)^{i-1} p = p \sum_{i=1}^\infty (1-p)^{i-1} = p \times \frac{1}{1-(1-p)} = 1,\]</span> where in the second-to-last step we have used the formula for geometric series.</p>
<p>If we plot the distribution of <span class="math inline">\(X\)</span> (i.e., the values <span class="math inline">\({\mathbb{P}}[X=i]\)</span> against <span class="math inline">\(i\)</span>) we get a curve that decreases monotonically by a factor of <span class="math inline">\(1-p\)</span> at each step, as shown in Figure <a href="#fig:geometric">1</a>.</p>
<div class="figure">
<img src="n19-geometric.png" alt="Figure 1: The geometric distribution." id="fig:geometric" style="width:50.0%" />
<p class="caption">Figure 1: The geometric distribution.</p>
</div>
<p>Let us now compute the expectation <span class="math inline">\({\mathbb{E}}[X]\)</span>. Applying the definition of expected value directly gives us: <span class="math display">\[{\mathbb{E}}[X] = \sum_{i=1}^\infty i \times {\mathbb{P}}[X = i] = p\sum_{i=1}^\infty i(1-p)^{i-1}.\]</span> However, the final summation is difficult to evaluate and requires some calculus trick. Instead, we will use the following alternative formula for expectation.</p>
<p><span id="thm:exp" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></p>
<p><em>Let <span class="math inline">\(X\)</span> be a random variable that takes values in <span class="math inline">\(\{0,1,2,\dots\}\)</span>. Then <span class="math display">\[{\mathbb{E}}[X] = \sum_{i=1}^\infty {\mathbb{P}}[X\ge i].\]</span></em></p>
<p><em>Proof</em>. For notational convenience, let’s write <span class="math inline">\(p_i={\mathbb{P}}[X=i]\)</span>, for <span class="math inline">\(i=0,1,2,\ldots\)</span>. From the definition of expectation, we have <span class="math display">\[\begin{aligned}{\mathbb{E}}[X]&amp;=(0\times p_0) + (1\times p_1) + (2\times p_2) + (3\times p_3) + (4\times p_4) + \cdots\cr
               &amp;= p_1 +(p_2+p_2) + (p_3+p_3+p_3) + (p_4+p_4+p_4+p_4)+\cdots\cr
               &amp;= (p_1+p_2+p_3+p_4+\cdots) + (p_2+p_3+p_4+\cdots) + (p_3+p_4+\cdots) + (p_4+\cdots) +\cdots\cr
               &amp;= {\mathbb{P}}[X\ge 1] + {\mathbb{P}}[X\ge 2] + {\mathbb{P}}[X\ge 3] + {\mathbb{P}}[X\ge 4] +\cdots.\end{aligned}\]</span> In the third line, we have regrouped the terms into convenient infinite sums, and each infinite sum is exactly the probability that <span class="math inline">\(X \ge i\)</span> for each <span class="math inline">\(i\)</span>. You should check that you understand how the fourth line follows from the third.</p>
<p>Let us repeat the proof more formally, this time using more compact mathematical notation: <span class="math display">\[{\mathbb{E}}[X] = \sum_{j=1}^\infty j \times {\mathbb{P}}[X=j] = \sum_{j=1}^\infty \sum_{i = 1}^j {\mathbb{P}}[X=j] = \sum_{i=1}^\infty \sum_{j = i}^\infty {\mathbb{P}}[X=j] = \sum_{i=1}^\infty {\mathbb{P}}[X\ge i].\]</span> <span class="math inline">\(\square\)</span></p>
<p>We can now use <a href="#thm:exp">Theorem 1</a> to compute <span class="math inline">\({\mathbb{E}}[X]\)</span> more easily.</p>
<p><span id="thm:geometric-expectation" class="pandoc-numbering-text thm"><strong>Theorem 2</strong></span></p>
<p><em>For a geometric random variable <span class="math inline">\(X \sim {\operatorname{Geom}}(p)\)</span>, we have <span class="math inline">\({\mathbb{E}}[X] = 1/p\)</span>.</em></p>
<p><em>Proof</em>. The key observation is that for a geometric random variable <span class="math inline">\(X\)</span>, <a name="eq:1"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   {\mathbb{P}}[X\ge i] = (1-p)^{i-1} ~ \text{ for } i = 1,2,\dots.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(1)</span></span>  We can obtain this simply by summing <span class="math inline">\({\mathbb{P}}[X = j]\)</span> for <span class="math inline">\(j \ge i\)</span>. Another way of seeing this is to note that the event “<span class="math inline">\(X\ge i\)</span>” means at least <span class="math inline">\(i\)</span> tosses are required. This is equivalent to saying that the first <span class="math inline">\(i-1\)</span> tosses are all Tails, and the probability of this event is precisely <span class="math inline">\((1-p)^{i-1}\)</span>. Now, plugging Equation <a href="#eq:1">1</a> into <a href="#thm:exp">Theorem 1</a>, we get <span class="math display">\[{\mathbb{E}}[X] = \sum_{i=1}^\infty{\mathbb{P}}[X\ge i] = \sum_{i=1}^\infty (1-p)^{i-1}
                                         = {1\over{1-(1-p)}} ={1\over p},\]</span> where we have used the formula for geometric series. <span class="math inline">\(\square\)</span></p>
<p>So, <em>the expected number of tosses of a biased coin until the first Head appears is <span class="math inline">\(1/p\)</span></em>. Intuitively, if in each coin toss we expect to get <span class="math inline">\(p\)</span> Heads, then we need to toss the coin <span class="math inline">\(1/p\)</span> times to get <span class="math inline">\(1\)</span> Head. So for a fair coin, the expected number of tosses is <span class="math inline">\(2\)</span>, but remember that the actual number of coin tosses that we need can be any positive integers.</p>
<p><span><strong>Remark:</strong></span> Another way of deriving <span class="math inline">\({\mathbb{E}}[X] = 1/p\)</span> is to use the interpretation of a geometric random variable <span class="math inline">\(X\)</span> as the number of coin tosses until we get a Head. Consider what happens in the first coin toss: If the first toss comes up Heads, then <span class="math inline">\(X = 1\)</span>. Otherwise, we have used one toss, and we repeat the coin tossing process again; the number of coin tosses after the first toss is again a geometric random variable with parameter <span class="math inline">\(p\)</span>. Therefore, we can calculate: <span class="math display">\[{\mathbb{E}}[X] = \underbrace{p \cdot 1}_{\text{first toss is H}} + \underbrace{(1-p) \cdot (1 + {\mathbb{E}}[X])}_{\text{first toss is T, then toss again}}.\]</span> Solving for <span class="math inline">\({\mathbb{E}}[X]\)</span> yields <span class="math inline">\({\mathbb{E}}[X] = 1/p\)</span>, as claimed.</p>
<p>Let us now compute the variance of <span class="math inline">\(X\)</span>.</p>
<p><span id="thm:geometric-variance" class="pandoc-numbering-text thm"><strong>Theorem 3</strong></span></p>
<p><em>For a geometric random variable <span class="math inline">\(X \sim {\operatorname{Geom}}(p)\)</span>, we have <span class="math inline">\({\operatorname{var}}(X) = (1-p)/p^2\)</span>.</em></p>
<p><em>Proof</em>. We will show that <span class="math inline">\({\mathbb{E}}[X(X-1)] = 2(1-p)/p^2\)</span>. Since we already know <span class="math inline">\({\mathbb{E}}[X] = 1/p\)</span>, this will imply the desired result: <span class="math display">\[\begin{aligned}
{\operatorname{var}}(X) ~= {\mathbb{E}}[X^2] - {\mathbb{E}}[X]^2  
            ~&amp;=~ {\mathbb{E}}[X(X-1)] + {\mathbb{E}}[X] - {\mathbb{E}}[X]^2 \\
            &amp;= \frac{2(1-p)}{p^2} + \frac{1}{p} - \frac{1}{p^2}
            ~=~ \frac{2(1-p) + p - 1}{p^2}
            ~=~ \frac{1-p}{p^2}.\end{aligned}\]</span></p>
<p>Now to show <span class="math inline">\({\mathbb{E}}[X(X-1)] = 2(1-p)/p^2\)</span>, we begin with the following identity of geometric series: <span class="math display">\[\sum_{i=0}^\infty (1-p)^i = \frac{1}{p}.\]</span> Differentiating the identity above with respect to <span class="math inline">\(p\)</span> yields (the <span class="math inline">\(i = 0\)</span> term is equal to <span class="math inline">\(0\)</span> so we omit it): <span class="math display">\[-\sum_{i=1}^\infty i (1-p)^{i-1} = -\frac{1}{p^2}.\]</span> Differentiating both sides with respect to <span class="math inline">\(p\)</span> again gives us (the <span class="math inline">\(i = 1\)</span> term is equal to <span class="math inline">\(0\)</span> so we omit it): <a name="eq:geom-help"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
\sum_{i=2}^\infty i(i-1) (1-p)^{i-2} = \frac{2}{p^3}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(2)</span></span>  Now using the geometric distribution of <span class="math inline">\(X\)</span> and Equation <a href="#eq:geom-help">2</a>, we can calculate: <span class="math display">\[\begin{aligned}
{\mathbb{E}}[X(X-1)] ~&amp;=~ \sum_{i=1}^\infty i(i-1) \times {\mathbb{P}}[X = i]  \\
   ~&amp;=~ \sum_{i=2}^\infty i(i-1) (1-p)^{i-1} p ~~~~~~~~~~~~~~~ \text{(the $i = 1$ term is equal to $0$ so we omit it)} \\
   ~&amp;=~ p(1-p) \sum_{i=2}^\infty i(i-1) (1-p)^{i-2} \\
   ~&amp;=~ p(1-p) \times \frac{2}{p^3} ~~~~~~~~~~~~~~~~~~~~~~~~~~~ \text{(using the identity)} \\
   ~&amp;=~ \frac{2(1-p)}{p^2},\end{aligned}\]</span> as desired. <span class="math inline">\(\square\)</span></p>
<h3 id="application-the-coupon-collectors-problem" class="unnumbered">Application: The Coupon Collector’s Problem</h3>
<p>Suppose we are trying to collect a set of <span class="math inline">\(n\)</span> different baseball cards. We get the cards by buying boxes of cereal: each box contains exactly one card, and it is equally likely to be any of the <span class="math inline">\(n\)</span> cards. How many boxes do we need to buy until we have collected at least one copy of every card?</p>
<p>Let <span class="math inline">\(X\)</span> denote the number of boxes we need to buy in order to collect all <span class="math inline">\(n\)</span> cards. The distribution of <span class="math inline">\(X\)</span> is difficult to compute directly (try it for <span class="math inline">\(n = 3\)</span>). But if we are only interested in its expected value <span class="math inline">\({\mathbb{E}}[X]\)</span>, then we can evaluate it easily using linearity of expectation and what we have just learned about the geometric distribution.</p>
<p>As usual, we start by writing <a name="eq:2"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   X=X_1+X_2+\ldots+X_n\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(3)</span></span>  for suitable simple random variables <span class="math inline">\(X_i\)</span>. What should the <span class="math inline">\(X_i\)</span> be? Naturally, <span class="math inline">\(X_i\)</span> is the number of boxes we buy while trying to get the <span class="math inline">\(i\)</span>-th new card (starting immediately after we’ve got the <span class="math inline">\((i-1)\)</span>-st new card). With this definition, make sure you believe Equation <a href="#eq:2">3</a> before proceeding.</p>
<p>What does the distribution of <span class="math inline">\(X_i\)</span> look like? Well, <span class="math inline">\(X_1\)</span> is trivial: no matter what happens, we always get a new card in the first box (since we have none to start with). So <span class="math inline">\({\mathbb{P}}[X_1=1]=1\)</span>, and thus <span class="math inline">\({\mathbb{E}}[X_1]=1\)</span>.</p>
<p>How about <span class="math inline">\(X_2\)</span>? Each time we buy a box, we’ll get the same old card with probability <span class="math inline">\(1/n\)</span>, and a new card with probability <span class="math inline">\((n-1)/n\)</span>. So we can think of buying boxes as flipping a biased coin with Heads probability <span class="math inline">\(p= (n-1)/n\)</span>; then <span class="math inline">\(X_2\)</span> is just the number of tosses until the first Head appears. So <span class="math inline">\(X_2\)</span> has the geometric distribution with parameter <span class="math inline">\(p= (n-1)/n\)</span>, and <span class="math display">\[{\mathbb{E}}[X_2]={n\over{n-1}}.\]</span></p>
<p>How about <span class="math inline">\(X_3\)</span>? This is very similar to <span class="math inline">\(X_2\)</span> except that now we only get a new card with probability <span class="math inline">\((n-2)/n\)</span> (since there are now two old ones). So <span class="math inline">\(X_3\)</span> has the geometric distribution with parameter <span class="math inline">\(p= (n-2)/n\)</span>, and <span class="math display">\[{\mathbb{E}}[X_3] = {n\over{n-2}}.\]</span> Arguing in the same way, we see that, for <span class="math inline">\(i=1,2,\ldots,n\)</span>, <span class="math inline">\(X_i\)</span> has the geometric distribution with parameter <span class="math inline">\(p= (n-i+1)/n\)</span>, and hence that <span class="math display">\[{\mathbb{E}}[X_i] = {n\over{n-i+1}}.\]</span> Finally, applying linearity of expectation to Equation <a href="#eq:2">3</a>, we get <a name="eq:3"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   {\mathbb{E}}[X]=\sum_{i=1}^n{\mathbb{E}}[X_i]={n\over n} + {n\over{n-1}} +\cdots+{n\over 2}+{n\over 1} = n\sum_{i=1}^n{1\over i}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(4)</span></span>  This is an exact expression for <span class="math inline">\({\mathbb{E}}[X]\)</span>. We can obtain a tidier form by noting that the sum in it actually has a very good approximation,<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> namely:<span class="math display">\[\sum_{i=1}^n{1\over i}\approx\ln n+\gamma,\]</span> where <span class="math inline">\(\gamma=0.5772\ldots\)</span> is known as <span><em>Euler’s constant</em></span>.</p>
<p>Thus, <em>the expected number of cereal boxes needed to collect <span class="math inline">\(n\)</span> cards is about <span class="math inline">\(n(\ln n + \gamma)\)</span></em>. This is an excellent approximation to the exact formula Equation <a href="#eq:3">4</a> even for quite small values of <span class="math inline">\(n\)</span>. So for example, for <span class="math inline">\(n=100\)</span>, we expect to buy about <span class="math inline">\(518\)</span> boxes.</p>
<h2 id="poisson-distribution" class="unnumbered">Poisson Distribution</h2>
<p>Consider the number of clicks of a Geiger counter, which measures radioactive emissions. The average number of such clicks per unit time, <span class="math inline">\(\lambda\)</span>, is a measure of radioactivity, but the actual number of clicks fluctuates according to a certain distribution called the Poisson distribution. What is remarkable is that the average value, <span class="math inline">\(\lambda\)</span>, completely determines the probability distribution on the number of clicks <span class="math inline">\(X\)</span>.</p>
<p><span id="definition:2" class="pandoc-numbering-text definition"><strong>Definition 2</strong> <em>(Poisson Distribution)</em></span></p>
<p><em>A random variable <span class="math inline">\(X\)</span> for which</em> <a name="eq:5"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[{\mathbb{P}}[X=i] = {{\lambda^i}\over{i!}}{e}^{-\lambda}\qquad\text{for $i=0,1,2,\ldots$}\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(5)</span></span>  <em>is said to have the Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. This is abbreviated as <span class="math inline">\(X \sim {\operatorname{Poiss}}(\lambda)\)</span>.</em></p>
<p>To make sure this is a valid definition, let us check that Equation <a href="#eq:5">5</a> is in fact a distribution, i.e., that the probabilities sum to <span class="math inline">\(1\)</span>. We have <span class="math display">\[\sum_{i=0}^\infty {{\lambda^i}\over{i!}}{e}^{-\lambda} =
   e^{-\lambda}\sum_{i=0}^\infty {{\lambda^i}\over{i!}} =
   e^{-\lambda}\times{e}^{\lambda} = 1.\]</span> In the second-to-last step, we used the Taylor series expansion <span class="math inline">\(e^x = 1+x+x^2/2{!}+x^3/3{!}+\cdots\)</span>.</p>
<p>The Poisson distribution is also a very widely accepted model for so-called “rare events,&quot; such as misconnected phone calls, radioactive emissions, crossovers in chromosomes, the number of cases of disease, the number of births per hour, etc. This model is appropriate whenever the occurrences can be assumed to happen randomly with some constant density in a continuous region (of time or space), such that occurrences in disjoint subregions are independent. One can then show that the number of occurrences in a region of unit size should obey the Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>.</p>
<p><span><strong>Example:</strong></span> Suppose when we write an article, we make an average of <span class="math inline">\(1\)</span> typo per page. We can model this with a Poisson random variable <span class="math inline">\(X\)</span> with <span class="math inline">\(\lambda = 1\)</span>. So the probability that a page has <span class="math inline">\(5\)</span> typos is <span class="math display">\[{\mathbb{P}}[X = 5] ~=~ \frac{1^5}{5!} e^{-1} ~=~ \frac{1}{120 \, e} ~\approx~ \frac{1}{326}.\]</span> Now suppose the article has <span class="math inline">\(200\)</span> pages. If we assume the number of typos in each page is independent, then the probability that there is at least one page with exactly <span class="math inline">\(5\)</span> typos is <span class="math display">\[\begin{aligned}
{\mathbb{P}}[\exists \: \text{a page with exactly $5$ typos}]
~&amp;=~ 1 - {\mathbb{P}}[\text{every page has $\neq 5$ typos}] \\
~&amp;=~ 1 - \prod_{k=1}^{200} {\mathbb{P}}[\text{page $k$ has $\neq 5$ typos}] \\
~&amp;=~ 1 - \prod_{k=1}^{200} (1 - {\mathbb{P}}[\text{page $k$ has exactly $5$ typos}]) \\
~&amp;=~ 1 - \left(1 - \frac{1}{120 \, e}\right)^{200},\end{aligned}\]</span> where in the last step we have used our earlier calculation for <span class="math inline">\({\mathbb{P}}[X = 5]\)</span>.</p>
<p>Let us now calculate the expectation and variance of a Poisson random variable. As we noted before, the expected value is simply <span class="math inline">\(\lambda\)</span>. Here we see that the variance is also equal to <span class="math inline">\(\lambda\)</span>.</p>
<p><span id="thm:poisson" class="pandoc-numbering-text thm"><strong>Theorem 4</strong></span></p>
<p><em>For a Poisson random variable <span class="math inline">\(X \sim {\operatorname{Poiss}}(\lambda)\)</span>, we have <span class="math inline">\({\mathbb{E}}[X] = \lambda\)</span> and <span class="math inline">\({\operatorname{var}}(X) = \lambda\)</span>.</em></p>
<p><em>Proof</em>. We can calculate <span class="math inline">\({\mathbb{E}}[X]\)</span> directly from the definition of expectation: <span class="math display">\[\begin{aligned}
{\mathbb{E}}[X]&amp;=\sum_{i=0}^\infty i\times{\mathbb{P}}[X=i]\\
               &amp;=\sum_{i=1}^\infty i \: {{\lambda^i}\over{i!}}{e}^{-\lambda}  ~~~~~~~~~~~~~~~~~~~~~~~~~ \text{(the $i=0$ term is equal to $0$ so we omit it)}\\
               &amp;=\lambda{e}^{-\lambda}\sum_{i=1}^\infty{{\lambda^{i-1}}\over{(i-1)!}}\\
               &amp;=\lambda{e}^{-\lambda}{e}^{\lambda} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \text{(since ${e}^\lambda = \textstyle{\sum_{j=0}^\infty \lambda^j/j!}$ with $j = i-1$)}\\
               &amp;=\lambda.\end{aligned}\]</span></p>
<p>Similarly, we can calculate <span class="math inline">\({\mathbb{E}}[X(X-1)]\)</span> as follows: <span class="math display">\[\begin{aligned}
{\mathbb{E}}[X(X-1)] &amp;=\sum_{i=0}^\infty i(i-1) \times{\mathbb{P}}[X=i]\\
               &amp;=\sum_{i=2}^\infty i(i-1) \: {{\lambda^i}\over{i!}}{e}^{-\lambda}  ~~~~~~~~~~~~~~~~ \text{(the $i=0$ and $i=1$ terms are equal to $0$ so we omit them)}\\
               &amp;=\lambda^2 {e}^{-\lambda}\sum_{i=2}^\infty{{\lambda^{i-2}}\over{(i-2)!}}\\
               &amp;=\lambda^2 {e}^{-\lambda} {e}^{\lambda} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \text{(since ${e}^\lambda = \textstyle{\sum_{j=0}^\infty \lambda^j/j!}$ with $j = i-2$)}\\
               &amp;=\lambda^2.\end{aligned}\]</span> Therefore, <span class="math display">\[{\operatorname{var}}(X) ~=~ {\mathbb{E}}[X^2] - {\mathbb{E}}[X]^2 ~=~ {\mathbb{E}}[X(X-1)] + {\mathbb{E}}[X] - {\mathbb{E}}[X]^2 ~=~ \lambda^2 + \lambda - \lambda^2 ~=~ \lambda,\]</span> as desired. <span class="math inline">\(\square\)</span></p>
<p>A plot of the Poisson distribution reveals a curve that rises monotonically to a single peak and then decreases monotonically. The peak is as close as possible to the expected value, i.e., at <span class="math inline">\(i=\lfloor\lambda\rfloor\)</span>. Figure <a href="#fig:poisson">2</a> shows an example for <span class="math inline">\(\lambda = 5\)</span>.</p>
<div class="figure">
<img src="n19-poisson.png" alt="Figure 2: The Poisson distribution with \lambda = 5." id="fig:poisson" style="width:50.0%" />
<p class="caption">Figure 2: The Poisson distribution with <span class="math inline">\(\lambda = 5\)</span>.</p>
</div>
<h3 id="poisson-and-coin-flips" class="unnumbered">Poisson and Coin Flips</h3>
<p>To see a concrete example of how Poisson distribution arises, suppose we want to model the number of cell phone users initiating calls in a network during a time period, of duration (say) <span class="math inline">\(1\)</span> minute. There are many customers in the network, and all of them can potentially make a call during this time period. However, only a very small fraction of them actually will. Under this scenario, it seems reasonable to make two assumptions:</p>
<ul>
<li><p>The probability of having more than <span class="math inline">\(1\)</span> customer initiating a call in any small time interval is negligible.</p></li>
<li><p>The initiation of calls in disjoint time intervals are independent events.</p></li>
</ul>
<p>Then if we divide the one-minute time period into <span class="math inline">\(n\)</span> disjoint intervals, then the number of calls <span class="math inline">\(X\)</span> in that time period can be modeled as a binomial random variable with parameter <span class="math inline">\(n\)</span> and probability of success <span class="math inline">\(p\)</span>, i.e., <span class="math inline">\(p\)</span> is the probability of having a call initiated in a time interval of length <span class="math inline">\(1/n\)</span>. But what should <span class="math inline">\(p\)</span> be in terms of the relevant parameters of the problem? If calls are initiated at an average rate of <span class="math inline">\(\lambda\)</span> calls per minute, then <span class="math inline">\({\mathbb{E}}[X] = \lambda\)</span> and so <span class="math inline">\(np = \lambda\)</span>, i.e., <span class="math inline">\(p = \lambda/n\)</span> . So <span class="math inline">\(X \sim {\operatorname{Bin}}(n, \lambda/n)\)</span>. As we shall see below, as we let <span class="math inline">\(n\)</span> tend to infinity, this distribution tends to the Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. We can also see why the Poisson distribution is a model for “rare events.” We are thinking of it as a sequence of a large number, <span class="math inline">\(n\)</span>, of coin flips, where we expect only a finite number <span class="math inline">\(\lambda\)</span> of Heads.</p>
<p>Now we will prove that the Poisson distribution <span class="math inline">\({\operatorname{Poiss}}(\lambda)\)</span> is the limit of the binomial distribution <span class="math inline">\({\operatorname{Bin}}(n, \lambda/n)\)</span>, as <span class="math inline">\(n\)</span> tends to infinity.</p>
<p><span id="thm:convergence-poisson" class="pandoc-numbering-text thm"><strong>Theorem 5</strong></span></p>
<p><em>Let <span class="math inline">\(X \sim {\operatorname{Bin}}(n, \lambda/n)\)</span> where <span class="math inline">\(\lambda &gt; 0\)</span> is a fixed constant. Then for every <span class="math inline">\(i = 0,1,2,\dots\)</span>, <span class="math display">\[{\mathbb{P}}[X = i] \longrightarrow \frac{\lambda^i}{i!} e^{-\lambda} ~~~ \text{ as } ~ n \to \infty.\]</span> That is, the probability distribution of <span class="math inline">\(X\)</span> converges to the Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>.</em></p>
<p><em>Proof</em>. Fix <span class="math inline">\(i \in \{0,1,2,\dots\}\)</span>, and assume <span class="math inline">\(n \ge i\)</span> (because we will let <span class="math inline">\(n \to \infty\)</span>). Then, because <span class="math inline">\(X\)</span> has binomial distribution with parameter <span class="math inline">\(n\)</span> and <span class="math inline">\(p = \lambda/n\)</span>, <span class="math display">\[{\mathbb{P}}[X = i] = {n \choose i} p^i (1-p)^{n-i}
= \frac{n!}{i! (n-i)!} \left(\frac{\lambda}{n}\right)^i \left(1-\frac{\lambda}{n}\right)^{n-i}.\]</span> Let us collect the factors into <a name="eq:binpoiss"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
{\mathbb{P}}[X = i]
= \frac{\lambda^i}{i!} \left( \frac{n!}{(n-i)!} \cdot \frac{1}{n^i} \right) \cdot \left(1-\frac{\lambda}{n}\right)^n \cdot \left(1-\frac{\lambda}{n}\right)^{-i}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(6)</span></span>  The first parenthesis above becomes, as <span class="math inline">\(n \to \infty\)</span>, <span class="math display">\[\frac{n!}{(n-i)!} \cdot \frac{1}{n^i} = \frac{n \cdot (n-1) \cdots (n-i+1) \cdot (n-i)!}{(n-i)!} \cdot \frac{1}{n^i}
= \frac{n}{n} \cdot \frac{(n-1)}{n} \cdots \frac{(n-i+1)}{n}
\to 1.\]</span> From calculus, the second parenthesis in Equation <a href="#eq:binpoiss">6</a> becomes, as <span class="math inline">\(n \to \infty\)</span>, <span class="math display">\[\left(1-\frac{\lambda}{n}\right)^n \to e^{-\lambda}.\]</span> And since <span class="math inline">\(i\)</span> is fixed, the third parenthesis in Equation <a href="#eq:binpoiss">6</a> becomes, as <span class="math inline">\(n \to \infty\)</span>, <span class="math display">\[\left(1-\frac{\lambda}{n}\right)^{-i} \to (1-0)^{-i} = 1.\]</span> Substituting these results back to  gives us <span class="math display">\[{\mathbb{P}}[X = i]
\to \frac{\lambda^i}{i!} \cdot 1 \cdot e^{-\lambda} \cdot 1 = \frac{\lambda^i}{i!} e^{-\lambda},\]</span> as desired. <span class="math inline">\(\square\)</span></p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is another of the little tricks you might like to carry around in your toolbox.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
</article>
</body>

<!-- Mirrored from www.eecs70.org/static/notes/n19.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:41:44 GMT -->
</html>
