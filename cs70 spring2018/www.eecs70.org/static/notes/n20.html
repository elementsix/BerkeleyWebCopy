<!DOCTYPE html>
<!--==============================================================================
	           "GitHub HTML5 Pandoc Template" v1.2 — by Tristano Ajmone           
	==============================================================================
	(c) Tristano Ajmone, 2017, MIT License (MIT). Project's home repository:

	- https://github.com/tajmone/pandoc-goodies

	This template reuses source code taken from the following projects:

	- GitHub Markdown CSS: © Sindre Sorhus, MIT License (MIT):
	  https://github.com/sindresorhus/github-markdown-css

	- Primer CSS: © 2016 GitHub Inc., MIT License (MIT):
	  http://primercss.io/
	==============================================================================-->
<html>

<!-- Mirrored from www.eecs70.org/static/notes/n20.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:41:44 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Continuous Probability</title>
<style type="text/css">@font-face{font-family:octicons-link;src:url(data:font/woff;charset=utf-8;base64,d09GRgABAAAAAAZwABAAAAAACFQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABEU0lHAAAGaAAAAAgAAAAIAAAAAUdTVUIAAAZcAAAACgAAAAoAAQAAT1MvMgAAAyQAAABJAAAAYFYEU3RjbWFwAAADcAAAAEUAAACAAJThvmN2dCAAAATkAAAABAAAAAQAAAAAZnBnbQAAA7gAAACyAAABCUM+8IhnYXNwAAAGTAAAABAAAAAQABoAI2dseWYAAAFsAAABPAAAAZwcEq9taGVhZAAAAsgAAAA0AAAANgh4a91oaGVhAAADCAAAABoAAAAkCA8DRGhtdHgAAAL8AAAADAAAAAwGAACfbG9jYQAAAsAAAAAIAAAACABiATBtYXhwAAACqAAAABgAAAAgAA8ASm5hbWUAAAToAAABQgAAAlXu73sOcG9zdAAABiwAAAAeAAAAME3QpOBwcmVwAAAEbAAAAHYAAAB/aFGpk3jaTY6xa8JAGMW/O62BDi0tJLYQincXEypYIiGJjSgHniQ6umTsUEyLm5BV6NDBP8Tpts6F0v+k/0an2i+itHDw3v2+9+DBKTzsJNnWJNTgHEy4BgG3EMI9DCEDOGEXzDADU5hBKMIgNPZqoD3SilVaXZCER3/I7AtxEJLtzzuZfI+VVkprxTlXShWKb3TBecG11rwoNlmmn1P2WYcJczl32etSpKnziC7lQyWe1smVPy/Lt7Kc+0vWY/gAgIIEqAN9we0pwKXreiMasxvabDQMM4riO+qxM2ogwDGOZTXxwxDiycQIcoYFBLj5K3EIaSctAq2kTYiw+ymhce7vwM9jSqO8JyVd5RH9gyTt2+J/yUmYlIR0s04n6+7Vm1ozezUeLEaUjhaDSuXHwVRgvLJn1tQ7xiuVv/ocTRF42mNgZGBgYGbwZOBiAAFGJBIMAAizAFoAAABiAGIAznjaY2BkYGAA4in8zwXi+W2+MjCzMIDApSwvXzC97Z4Ig8N/BxYGZgcgl52BCSQKAA3jCV8CAABfAAAAAAQAAEB42mNgZGBg4f3vACQZQABIMjKgAmYAKEgBXgAAeNpjYGY6wTiBgZWBg2kmUxoDA4MPhGZMYzBi1AHygVLYQUCaawqDA4PChxhmh/8ODDEsvAwHgMKMIDnGL0x7gJQCAwMAJd4MFwAAAHjaY2BgYGaA4DAGRgYQkAHyGMF8NgYrIM3JIAGVYYDT+AEjAwuDFpBmA9KMDEwMCh9i/v8H8sH0/4dQc1iAmAkALaUKLgAAAHjaTY9LDsIgEIbtgqHUPpDi3gPoBVyRTmTddOmqTXThEXqrob2gQ1FjwpDvfwCBdmdXC5AVKFu3e5MfNFJ29KTQT48Ob9/lqYwOGZxeUelN2U2R6+cArgtCJpauW7UQBqnFkUsjAY/kOU1cP+DAgvxwn1chZDwUbd6CFimGXwzwF6tPbFIcjEl+vvmM/byA48e6tWrKArm4ZJlCbdsrxksL1AwWn/yBSJKpYbq8AXaaTb8AAHja28jAwOC00ZrBeQNDQOWO//sdBBgYGRiYWYAEELEwMTE4uzo5Zzo5b2BxdnFOcALxNjA6b2ByTswC8jYwg0VlNuoCTWAMqNzMzsoK1rEhNqByEyerg5PMJlYuVueETKcd/89uBpnpvIEVomeHLoMsAAe1Id4AAAAAAAB42oWQT07CQBTGv0JBhagk7HQzKxca2sJCE1hDt4QF+9JOS0nbaaYDCQfwCJ7Au3AHj+LO13FMmm6cl7785vven0kBjHCBhfpYuNa5Ph1c0e2Xu3jEvWG7UdPDLZ4N92nOm+EBXuAbHmIMSRMs+4aUEd4Nd3CHD8NdvOLTsA2GL8M9PODbcL+hD7C1xoaHeLJSEao0FEW14ckxC+TU8TxvsY6X0eLPmRhry2WVioLpkrbp84LLQPGI7c6sOiUzpWIWS5GzlSgUzzLBSikOPFTOXqly7rqx0Z1Q5BAIoZBSFihQYQOOBEdkCOgXTOHA07HAGjGWiIjaPZNW13/+lm6S9FT7rLHFJ6fQbkATOG1j2OFMucKJJsxIVfQORl+9Jyda6Sl1dUYhSCm1dyClfoeDve4qMYdLEbfqHf3O/AdDumsjAAB42mNgYoAAZQYjBmyAGYQZmdhL8zLdDEydARfoAqIAAAABAAMABwAKABMAB///AA8AAQAAAAAAAAAAAAAAAAABAAAAAA==) format('woff')}.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;color:#24292e;font-family:-apple-system,system-ui,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:16px;line-height:1.5;word-wrap:break-word;box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto;padding:45px}.markdown-body .octicon{display:inline-block;fill:currentColor;vertical-align:text-bottom}.markdown-body a{background-color:transparent;-webkit-text-decoration-skip:objects;color:#0366d6;text-decoration:none}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body h1{margin:.67em 0}.markdown-body img{border-style:none}.markdown-body svg:not(:root){overflow:hidden}.markdown-body code,.markdown-body kbd,.markdown-body pre{font-family:monospace,monospace}.markdown-body input{font:inherit;margin:0;overflow:visible;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=checkbox]{box-sizing:border-box;padding:0}.markdown-body *{box-sizing:border-box}.markdown-body a:hover{text-decoration:underline}.markdown-body strong{font-weight:600}.markdown-body hr{box-sizing:content-box;overflow:hidden;background:0 0;border-bottom:1px solid #dfe2e5}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body table{border-spacing:0;border-collapse:collapse;display:block;width:100%;overflow:auto}.markdown-body td,.markdown-body th{padding:0}.markdown-body blockquote{margin:0}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body dd{margin-left:0}.markdown-body code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace}.markdown-body pre{font:12px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;word-wrap:normal}.markdown-body .pl-0{padding-left:0!important}.markdown-body .pl-1{padding-left:4px!important}.markdown-body .pl-2{padding-left:8px!important}.markdown-body .pl-3{padding-left:16px!important}.markdown-body .pl-4{padding-left:24px!important}.markdown-body .pl-5{padding-left:32px!important}.markdown-body .pl-6{padding-left:40px!important}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:0}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e1e4e8;border:0}.markdown-body blockquote{padding:0 1em;color:#6a737d;border-left:.25em solid #dfe2e5}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd{font-size:11px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#1b1f23;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1{padding-bottom:.3em;font-size:2em;border-bottom:1px solid #eaecef}.markdown-body h2{padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid #eaecef}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#6a737d}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #dfe2e5}.markdown-body table tr{background-color:#fff;border-top:1px solid #c6cbd1}.markdown-body table tr:nth-child(2n){background-color:#f6f8fa}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body code{padding:.2em 0;margin:0;font-size:85%;background-color:rgba(27,31,35,.05);border-radius:3px}.markdown-body code::after,.markdown-body code::before{letter-spacing:-.2em;content:"\00a0"}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f6f8fa;border-radius:3px}.markdown-body pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code::after,.markdown-body pre code::before{content:normal}.markdown-body .full-commit .btn-outline:not(:disabled):hover{color:#005cc5;border-color:#005cc5}.markdown-body kbd{display:inline-block;padding:3px 5px;font:11px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fcfcfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body :checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item+.task-list-item{margin-top:3px}.markdown-body .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown-body hr{border-bottom-color:#eee}.flash{position:relative;padding:16px;color:#246;background-color:#e2eef9;border:1px solid #bac6d3;border-radius:3px}.flash p:last-child{margin-bottom:0}.flash-messages{margin-bottom:24px}.flash-warn{color:#4c4a42;background-color:#fff9ea;border-color:#dfd8c2}.flash-error{color:#911;background-color:#fcdede;border-color:#d2b2b2}.flash-success{color:#22662c;background-color:#e2f9e5;border-color:#bad3be}.flash-plain{color:#4c4a42;background-color:#f5f5f5;border-color:#c1c1c1}.figure{text-align:center;}</style>
  <style type="text/css">code{white-space: pre;}</style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {equationNumbers: {autoNumber: "AMS"}}
    });
  </script>
  <script src="../../../cdn.mathjax.org/mathjax/latest/MathJax2ba6.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title">Continuous Probability</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#a-brief-introduction-to-continuous-probability">A Brief Introduction to Continuous Probability</a><ul>
<li><a href="#continuous-uniform-probability-space">Continuous Uniform Probability Space</a></li>
</ul></li>
<li><a href="#continuous-random-variables">Continuous Random Variables</a><ul>
<li><a href="#expectation-and-variance">Expectation and Variance</a></li>
<li><a href="#joint-distribution">Joint Distribution</a></li>
<li><a href="#independence">Independence</a></li>
</ul></li>
<li><a href="#two-important-continuous-distributions">Two Important Continuous Distributions</a><ul>
<li><a href="#exponential-distribution">Exponential Distribution</a><ul>
<li><a href="#exponential-distribution-is-the-continuous-time-analog-of-geometric-distribution">Exponential Distribution is the Continuous-Time Analog of Geometric Distribution</a></li>
</ul></li>
<li><a href="#normal-distribution">Normal Distribution</a><ul>
<li><a href="#sum-of-independent-normal-random-variables">Sum of Independent Normal Random Variables</a></li>
</ul></li>
</ul></li>
<li><a href="#the-central-limit-theorem">The Central Limit Theorem</a></li>
<li><a href="#optional-buffons-needle">Optional: Buffon’s Needle</a><ul>
<li><a href="#buffons-needle-a-slick-approach">Buffon’s Needle: A Slick Approach</a></li>
</ul></li>
</ul>
</nav>
<h1 id="a-brief-introduction-to-continuous-probability" class="unnumbered">A Brief Introduction to Continuous Probability</h1>
<p>Up to now we have focused exclusively on <span><em>discrete</em></span> probability spaces <span class="math inline">\(\Omega\)</span>, where the number of sample points <span class="math inline">\(\omega\in\Omega\)</span> is either finite or countably infinite (such as the integers). As a consequence, we have only been able to talk about <span><em>discrete</em></span> random variables, which take on only a finite or countably infinite number of values.</p>
<p>But in real life many quantities that we wish to model probabilistically are <span><em>real-valued</em></span>; examples include the position of a particle in a box, the time at which an certain incident happens, or the direction of travel of a meteorite. In this lecture, we discuss how to extend the concepts we’ve seen in the discrete setting to this <span><em>continuous</em></span> setting. As we shall see, everything translates in a natural way once we have set up the right framework. The framework involves some elementary calculus but (at this level of discussion) nothing too scary.</p>
<h2 id="continuous-uniform-probability-space" class="unnumbered">Continuous Uniform Probability Space</h2>
<p>Suppose we spin a “wheel of fortune&quot; and record the position of the pointer on the outer circumference of the wheel. Assuming that the circumference is of length <span class="math inline">\(\ell\)</span> and that the wheel is unbiased, the position is presumably equally likely to take on any value in the real interval <span class="math inline">\([0,\ell]\)</span>. How do we model this experiment using a probability space?</p>
<p>Consider for a moment the analogous discrete setting, where the pointer can stop only at a finite number <span class="math inline">\(m\)</span> of positions distributed evenly around the wheel. (If <span class="math inline">\(m\)</span> is very large, then this is in some sense similar to the continuous setting, which we can think of as the limit <span class="math inline">\(m \to \infty\)</span>.) Then we would model this situation using the discrete sample space <span class="math inline">\(\Omega = \{0,\ell/m,2\ell/m,\dotsc,(m-1)\ell/m\}\)</span>, with uniform probabilities <span class="math inline">\({\mathbb{P}}[\omega] = 1/m\)</span> for each <span class="math inline">\(\omega\in\Omega\)</span>.</p>
<p>In the continuous setting, however, we get into trouble if we try the same approach. If we let <span class="math inline">\(\omega\)</span> range over all real numbers in <span class="math inline">\(\Omega = [0,\ell]\)</span>, what value should we assign to each <span class="math inline">\({\mathbb{P}}[\omega]\)</span>? By uniformity, this probability should be the same for all <span class="math inline">\(\omega\)</span>. But if we assign <span class="math inline">\({\mathbb{P}}[\omega]\)</span> to be any positive value, then because there are infinitely many <span class="math inline">\(\omega\)</span> in <span class="math inline">\(\Omega\)</span>, the sum of all probabilities <span class="math inline">\({\mathbb{P}}[\omega]\)</span> will be <span class="math inline">\(\infty\)</span>! Thus, <span class="math inline">\({\mathbb{P}}[\omega]\)</span> must be zero for all <span class="math inline">\(\omega\in\Omega\)</span>. But if all of our sample points have probability zero, then we are unable to assign meaningful probabilities to any events!</p>
<p>To resolve this problem, consider instead any non-empty <span><em>interval</em></span> <span class="math inline">\([a,b]\subseteq[0,\ell]\)</span>. Can we assign a non-zero probability value to this interval? Since the total probability assigned to <span class="math inline">\([0,\ell]\)</span> must be <span class="math inline">\(1\)</span>, and since we want our probability to be uniform, the logical value for the probability of interval <span class="math inline">\([a,b]\)</span> is <span class="math display">\[\frac{\text{length of $[a,b]$}}{\text{length of $[0,\ell]$}} = \frac{b-a}{\ell}.\]</span> In other words, the probability of an interval is proportional to its length.</p>
<p>Note that intervals are subsets of the sample space <span class="math inline">\(\Omega\)</span> and are therefore <span><em>events</em></span>. So in contrast to discrete probability, where we assigned probabilities to <span><em>points</em></span> in the sample space, in continuous probability we are assigning probabilities to certain basic events (in this case intervals). What about probabilities of other events? By specifying the probability of intervals, we have also specified the probability of any event <span class="math inline">\(E\)</span> which can be written as the disjoint union of (a finite or countably infinite number of) intervals, <span class="math inline">\(E=\bigcup_i E_i\)</span>. For then we can write <span class="math inline">\({\mathbb{P}}[E] = \sum_i {\mathbb{P}}[E_i]\)</span>, in analogous fashion to the discrete case. Thus for example the probability that the pointer ends up in the first or third quadrants of the wheel is <span class="math display">\[\frac{\ell/4}{\ell}+\frac{\ell/4}{\ell} = \frac{1}{2}.\]</span> For all practical purposes, such events are all we really need.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<h1 id="continuous-random-variables" class="unnumbered">Continuous Random Variables</h1>
<p>Recall that in the discrete setting we typically work with <span><em>random variables</em></span> and their distributions, rather than directly with probability spaces and events. The simplest example of a continuous random variable is the position <span class="math inline">\(X\)</span> of the pointer in the wheel of fortune, as discussed above. This random variable has the <span><em>uniform</em></span> distribution on <span class="math inline">\([0,\ell]\)</span>. How, precisely, should we define the distribution of a continuous random variable? In the discrete case the distribution of a r.v. <span class="math inline">\(X\)</span> is described by specifying, for each possible value <span class="math inline">\(a\)</span>, the probability <span class="math inline">\({\mathbb{P}}[X=a]\)</span>. But for the r.v. <span class="math inline">\(X\)</span> corresponding to the position of the pointer, we have <span class="math inline">\({\mathbb{P}}[X=a]=0\)</span> for every <span class="math inline">\(a\)</span>, so we run into the same problem as we encountered above in defining the probability space.</p>
<p>The resolution is the same: instead of specifying <span class="math inline">\({\mathbb{P}}[X=a]\)</span>, we specify <span class="math inline">\({\mathbb{P}}[a\le X\le b]\)</span> for all intervals <span class="math inline">\([a,b]\)</span>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> To do this formally, we need to introduce the concept of a <span><em>probability density function</em></span> (sometimes referred to just as a “density&quot;, or a “pdf&quot;).</p>
<p><span id="definition:1" class="pandoc-numbering-text definition"><strong>Definition 1</strong> <em>(Density)</em></span></p>
<p><em>A probability density function for a real-valued random variable <span class="math inline">\(X\)</span> is a function <span class="math inline">\(f:{\mathbb{R}}\to{\mathbb{R}}\)</span> satisfying:</em></p>
<ol style="list-style-type: decimal">
<li><p><em><span class="math inline">\(f\)</span> is non-negative: <span class="math inline">\(f(x) \ge 0\)</span> for all <span class="math inline">\(x \in {\mathbb{R}}\)</span>.</em></p></li>
<li><p><em>The total integral of <span class="math inline">\(f\)</span> is equal to <span class="math inline">\(1\)</span>: <span class="math inline">\(\int_{-\infty}^\infty f(x) \, {\mathrm{d}}x = 1\)</span>.</em></p></li>
</ol>
<p><em>Then the distribution of <span class="math inline">\(X\)</span> is given by:</em> <span class="math display">\[{\mathbb{P}}[a\le X\le b] = \int_{a}^b f(x) \, {\mathrm{d}}x\qquad\text{for all $a\le b$}.\]</span></p>
<p>Let’s examine this definition. Note that the definite integral is just the area under the curve <span class="math inline">\(f\)</span> between the values <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Thus <span class="math inline">\(f\)</span> plays a similar role to the “histogram&quot; we sometimes draw to picture the distribution of a discrete random variable. The first condition that <span class="math inline">\(f\)</span> be non-negative ensures that the probability of every event is non-negative. The second condition that the total integral of <span class="math inline">\(f\)</span> equal to <span class="math inline">\(1\)</span> ensures that it defines a valid probability distribution, because the r.v. <span class="math inline">\(X\)</span> must take on real values: <a name="eq:total"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
 {\mathbb{P}}[X \in {\mathbb{R}}] = {\mathbb{P}}[-\infty &lt; X &lt; \infty] = \int_{-\infty}^\infty f(x) \, {\mathrm{d}}x =  1.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(1)</span></span> </p>
<p>For example, consider the wheel-of-fortune r.v. <span class="math inline">\(X\)</span>, which has uniform distribution on the interval <span class="math inline">\([0,\ell]\)</span>. This means the density <span class="math inline">\(f\)</span> of <span class="math inline">\(X\)</span> vanishes outside this interval: <span class="math inline">\(f(x)=0\)</span> for <span class="math inline">\(x&lt;0\)</span> and for <span class="math inline">\(x&gt;\ell\)</span>. Within the interval <span class="math inline">\([0,\ell]\)</span> we want the distribution of <span class="math inline">\(X\)</span> to be uniform, which means we should take <span class="math inline">\(f\)</span> to be a constant <span class="math inline">\(f(x)=c\)</span> for <span class="math inline">\(0\le x\le\ell\)</span>. The value of <span class="math inline">\(c\)</span> is determined by the requirement Equation <a href="#eq:total">1</a> that the total area under <span class="math inline">\(f\)</span> is <span class="math inline">\(1\)</span>: <span class="math display">\[1 = \int_{-\infty}^\infty f(x) \, {\mathrm{d}}x = \int_{0}^\ell c \, {\mathrm{d}}x = c\ell,\]</span> which gives us <span class="math inline">\(c=1/\ell\)</span>. Therefore, the density of the uniform distribution on <span class="math inline">\([0,\ell]\)</span> is given by <span class="math display">\[f(x) = \begin{cases}
       0 &amp; \text{for $x&lt;0$;}\cr
       1/\ell &amp; \text{for $0\le x\le\ell$;}\cr
       0 &amp; \text{for $x&gt;\ell$.}\cr
   \end{cases}\]</span></p>
<h4 id="remark">Remark:</h4>
<p>Following the “histogram&quot; analogy above, it is tempting to think of <span class="math inline">\(f(x)\)</span> as a “probability&quot;. However, <span class="math inline">\(f(x)\)</span> doesn’t itself correspond to the probability of anything! In particular, *there is no requirement that <span class="math inline">\(f(x)\)</span> be bounded above by <span class="math inline">\(1\)</span>. For example, the density of the uniform distribution on the interval <span class="math inline">\([0,\ell]\)</span> with <span class="math inline">\(\ell = 1/2\)</span> is equal to <span class="math inline">\(f(x) = 1/(1/2) = 2\)</span> for <span class="math inline">\(0 \le x \le 1/2\)</span>, which is greater than <span class="math inline">\(1\)</span>. To connect density <span class="math inline">\(f(x)\)</span> with probabilities, we need to look at a very small interval <span class="math inline">\([x,x+\delta]\)</span> close to <span class="math inline">\(x\)</span>; then we have <a name="eq:small"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   {\mathbb{P}}[x\le X\le x+\delta] = \int_{x}^{x+\delta} f(z) \, {\mathrm{d}}z \approx \delta f(x).\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(2)</span></span>  Thus, we can interpret <span class="math inline">\(f(x)\)</span> as the “probability per unit length&quot; in the vicinity of <span class="math inline">\(x\)</span>.</p>
<h2 id="expectation-and-variance" class="unnumbered">Expectation and Variance</h2>
<p>As in the discrete case, we define the expectation of a continuous r.v. as follows:</p>
<p><span id="definition:2" class="pandoc-numbering-text definition"><strong>Definition 2</strong> <em>(Expectation)</em></span></p>
<p><em>The expectation of a continuous r.v. <span class="math inline">\(X\)</span> with probability density function <span class="math inline">\(f\)</span> is</em> <span class="math display">\[{\mathbb{E}}[X] = \int_{-\infty}^\infty xf(x) \, {\mathrm{d}}x.\]</span></p>
<p>Note that the integral plays the role of the summation in the discrete formula <span class="math inline">\({\mathbb{E}}[X] = \sum_a a{\mathbb{P}}[X=a]\)</span>. Similarly, we can define the variance as follows:</p>
<p><span id="definition:3" class="pandoc-numbering-text definition"><strong>Definition 3</strong> <em>(Variance)</em></span></p>
<p><em>The variance of a continuous r.v. <span class="math inline">\(X\)</span> with probability density function <span class="math inline">\(f\)</span> is</em> <span class="math display">\[{\operatorname{var}}(X) = {\mathbb{E}}[(X-{\mathbb{E}}[X])^2] = {\mathbb{E}}[X^2] - {\mathbb{E}}[X]^2 = \int_{-\infty}^\infty x^2 f(x) \, {\mathrm{d}}x \;- \left(\int_{-\infty}^\infty xf(x) \, {\mathrm{d}}x\right)^2.\]</span></p>
<p><span><strong>Example:</strong></span> Let <span class="math inline">\(X\)</span> be a uniform r.v. on the interval <span class="math inline">\([0,\ell]\)</span>. Then intuitively, its expected value should be in the middle, <span class="math inline">\(\ell/2\)</span>. Indeed, we can use our definition above to compute <span class="math display">\[{\mathbb{E}}[X] = \int_{0}^\ell x\frac{1}{\ell} \, {\mathrm{d}}x = \left[\frac{x^2}{2\ell}\right]_0^\ell = \frac{\ell}{2},\]</span> as claimed. We can also calculate its variance using the above definition and plugging in the value <span class="math inline">\({\mathbb{E}}[X] = \ell/2\)</span> to get: <span class="math display">\[{\operatorname{var}}(X) = \int_{0}^\ell x^2\frac{1}{\ell} \, {\mathrm{d}}x - {\mathbb{E}}[X]^2 = \left[\frac{x^3}{3\ell}\right]_0^\ell - \left(\frac{\ell}{2}\right)^2 = \frac{\ell^2}{3} - \frac{\ell^2}{4} = \frac{\ell^2}{12}.\]</span> The factor of <span class="math inline">\(1/12\)</span> here is not particularly intuitive, but the fact that the variance is proportional to <span class="math inline">\(\ell^2\)</span> should come as no surprise. Like its discrete counterpart, this distribution has large variance.</p>
<h2 id="joint-distribution" class="unnumbered">Joint Distribution</h2>
<p>Recall that for discrete random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, their joint distribution is specified by the probabilities <span class="math inline">\({\mathbb{P}}[X = a, Y = c]\)</span> for all possible values <span class="math inline">\(a,c\)</span>. Similarly, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are continuous random variables, then their joint distributions are specified by the probabilities <span class="math inline">\({\mathbb{P}}[a \le X \le b, c \le Y \le d]\)</span> for all <span class="math inline">\(a \le b\)</span>, <span class="math inline">\(c \le d\)</span>. Moreover, just as the distribution of <span class="math inline">\(X\)</span> can be characterized by its density function, the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can be characterized by their joint density.</p>
<p><span id="definition:4" class="pandoc-numbering-text definition"><strong>Definition 4</strong> <em>(Joint Density)</em></span></p>
<p><em>A joint density function for two random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a function <span class="math inline">\(f:{\mathbb{R}}^2\to{\mathbb{R}}\)</span> satisfying:</em></p>
<ol style="list-style-type: decimal">
<li><p><em><span class="math inline">\(f\)</span> is non-negative: <span class="math inline">\(f(x,y) \ge 0\)</span> for all <span class="math inline">\(x,y \in {\mathbb{R}}\)</span>.</em></p></li>
<li><p><em>The total integral of <span class="math inline">\(f\)</span> is equal to <span class="math inline">\(1\)</span>: <span class="math inline">\(\int_{-\infty}^\infty \int_{-\infty}^\infty f(x,y) \, {\mathrm{d}}x \, {\mathrm{d}}y = 1\)</span>.</em></p></li>
</ol>
<p><em>Then the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by:</em> <span class="math display">\[{\mathbb{P}}[a\le X\le b, ~ c \le Y \le d] = \int_{c}^d \int_a^b f(x,y) \, {\mathrm{d}}x \, {\mathrm{d}}y\qquad\text{for all $a\le b$ and $c \le d$}.\]</span></p>
<p>In analogy with Equation <a href="#eq:small">2</a>, we can connect the joint density <span class="math inline">\(f(x,y)\)</span> with probabilities by looking at a very small square <span class="math inline">\([x,x+\delta] \times [y,y+\delta]\)</span> close to <span class="math inline">\((x,y)\)</span>; then we have <a name="eq:small_joint"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   {\mathbb{P}}[x\le X\le x+\delta, ~ y \le Y \le y +\delta]
= \int_y^{y+\delta}\int_{x}^{x+\delta} f(u,v) \, {\mathrm{d}}u \, {\mathrm{d}}v \approx \delta^2
f(x,y).\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(3)</span></span>  Thus we can interpret <span class="math inline">\(f(x,y)\)</span> as the “probability per unit area&quot; in the vicinity of <span class="math inline">\((x,y)\)</span>.</p>
<h2 id="independence" class="unnumbered">Independence</h2>
<p>Recall that two discrete random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be independent if the events <span class="math inline">\(X=a\)</span> and <span class="math inline">\(Y=c\)</span> are independent for every possible values <span class="math inline">\(a,c\)</span>. We have a similar definition for continuous random variables:</p>
<p><span id="definition:5" class="pandoc-numbering-text definition"><strong>Definition 5</strong> <em>(Independence for Continuous R.V.s)</em></span></p>
<p><em>Two continuous r.v.’s <span class="math inline">\(X,Y\)</span> are independent if the events <span class="math inline">\(a\le X\le b\)</span> and <span class="math inline">\(c\le Y\le d\)</span> are independent for all <span class="math inline">\(a \le b\)</span> and <span class="math inline">\(c \le d\)</span>:</em> <span class="math display">\[{\mathbb{P}}[a \le X \le b, ~ c \le Y \le d] = {\mathbb{P}}[a \le X \le b] \cdot {\mathbb{P}}[c \le Y \le d].\]</span></p>
<p>What does this definition say about the joint density of independent r.v.’s <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>? Applying Equation <a href="#eq:small_joint">3</a> to connect the joint density with probabilities, we get, for small <span class="math inline">\(\delta\)</span>: <span class="math display">\[\begin{aligned}
\delta^2 f(x,y) &amp; \approx  {\mathbb{P}}[x\le X\le x+\delta, ~ y \le Y \le y
+\delta]\\
&amp; =  {\mathbb{P}}[x\le X\le x+\delta]  \cdot {\mathbb{P}}[y \le Y \le y
+\delta]\qquad\text{(by independence)}\\
&amp; \approx  \delta f_X(x) \times \delta f_Y(y) \\
&amp;= \delta^2 f_X(x) f_Y(y),\end{aligned}\]</span> where <span class="math inline">\(f_X\)</span> and <span class="math inline">\(f_Y\)</span> are the (marginal) densities of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> respectively. So we get the following result:</p>
<p><span id="thm:jointdensity" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></p>
<p><em>The joint density of independent r.v.’s <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the product of the marginal densities:</em> <span class="math display">\[f(x,y) = f_X(x) \, f_Y(y) ~~ \text{ for all } ~ x,y \in {\mathbb{R}}.\]</span></p>
<h1 id="two-important-continuous-distributions" class="unnumbered">Two Important Continuous Distributions</h1>
<p>We have already seen one important continuous distribution, namely the uniform distribution. In this section we will see two more: the <span><em>exponential</em></span> distribution and the <span><em>normal</em></span> (or <span><em>Gaussian</em></span>) distribution. These three distributions cover the vast majority of continuous random variables arising in applications.</p>
<h2 id="exponential-distribution" class="unnumbered">Exponential Distribution</h2>
<p>The exponential distribution is a continuous version of the geometric distribution, which we have already seen in the previous note. Recall that the geometric distribution describes the number of tosses of a coin until the first Head appears; the distribution has a single parameter <span class="math inline">\(p\)</span>, which is the bias (Heads probability) of the coin. Of course, in real life applications we are usually not waiting for a coin to come up Heads but rather waiting for a system to fail, a clock to ring, an experiment to succeed, etc.</p>
<p>In such applications we are frequently not dealing with discrete events or discrete time, but rather with <span><em>continuous</em></span> time: for example, if we are waiting for an apple to fall off a tree, it can do so at any time at all, not necessarily on the tick of a discrete clock. This situation is naturally modeled by the exponential distribution, defined as follows:</p>
<p><span id="definition:6" class="pandoc-numbering-text definition"><strong>Definition 6</strong> <em>(Exponential Distribution)</em></span></p>
<p><em>For <span class="math inline">\(\lambda&gt;0\)</span>, a continuous random variable <span class="math inline">\(X\)</span> with density function</em> <span class="math display">\[f(x) = \begin{cases}
        \lambda e^{-\lambda x} &amp; \text{if $x\ge 0$};\\
        0 &amp; \text{otherwise}.
   \end{cases}\]</span> <em>is called an exponential random variable with parameter <span class="math inline">\(\lambda\)</span>.</em></p>
<p>Note that by definition <span class="math inline">\(f(x)\)</span> is non-negative. Moreover, we can check that it satisfies Equation <a href="#eq:total">1</a>: <span class="math display">\[\int_{-\infty}^\infty f(x) \, {\mathrm{d}}x =
           \int_0^\infty \lambda e^{-\lambda x} \, {\mathrm{d}}x = \bigl[-e^{-\lambda x}\bigr]_0^\infty = 1,\]</span> so <span class="math inline">\(f(x)\)</span> is indeed a valid probability density function. Figure <a href="#fig:exponential">1</a> shows the plot of the density function for the exponential distribution with <span class="math inline">\(\lambda = 2\)</span>.</p>
<div class="figure">
<img src="n20-exponential.png" alt="Figure 1: The density function for the exponential distribution with \lambda = 2." id="fig:exponential" style="width:50.0%" />
<p class="caption">Figure 1: The density function for the exponential distribution with <span class="math inline">\(\lambda = 2\)</span>.</p>
</div>
<p>Let us now compute its expectation and variance.</p>
<p><span id="thm:exponential-mean-variance" class="pandoc-numbering-text thm"><strong>Theorem 2</strong></span></p>
<p><em>Let <span class="math inline">\(X\)</span> be an exponential random variable with parameter <span class="math inline">\(\lambda &gt; 0\)</span>. Then</em> <span class="math display">\[{\mathbb{E}}[X] = \frac{1}{\lambda} \qquad \text{ and } \qquad {\operatorname{var}}(X) = \frac{1}{\lambda^2}.\]</span></p>
<p><em>Proof</em>. We can calculate the expected value using integration by parts: <span class="math display">\[{\mathbb{E}}[X] = \int_{-\infty}^\infty xf(x) \, {\mathrm{d}}x = \int_0^\infty \lambda xe^{-\lambda x} \, {\mathrm{d}}x
             = \bigl[-xe^{-\lambda x}\bigr]_0^\infty + \int_0^\infty e^{-\lambda x}  \, {\mathrm{d}}x
             = 0 + \left[-\frac{e^{-\lambda x}}{\lambda}\right]_0^\infty = \frac{1}{\lambda}.\]</span> To compute the variance, we first evaluate <span class="math inline">\({\mathbb{E}}[X^2]\)</span>, again using integration by parts: <span class="math display">\[{\mathbb{E}}[X^2] = \int_{-\infty}^\infty x^2f(x) \, {\mathrm{d}}x = \int_0^\infty \lambda x^2e^{-\lambda x} \, {\mathrm{d}}x
             = \bigl[-x^2e^{-\lambda x}\bigr]_0^\infty + \int_0^\infty 2xe^{-\lambda x} \, {\mathrm{d}}x
             = 0 + \frac{2}{\lambda}{\mathbb{E}}[X] = \frac{2}{\lambda^2}.\]</span> The variance is therefore <span class="math display">\[{\operatorname{var}}(X) = {\mathbb{E}}[X^2] - {\mathbb{E}}[X]^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2},\]</span> as claimed. <span class="math inline">\(\square\)</span></p>
<h3 id="exponential-distribution-is-the-continuous-time-analog-of-geometric-distribution" class="unnumbered">Exponential Distribution is the Continuous-Time Analog of Geometric Distribution</h3>
<p>Like the geometric distribution, the exponential distribution has a single parameter <span class="math inline">\(\lambda\)</span>, which characterizes the <span><em>rate</em></span> at which events happen. Note that the exponential distribution satisfies, for any <span class="math inline">\(t\ge 0\)</span>, <a name="eq:exptail"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   {\mathbb{P}}[X&gt; t] = \int_t^\infty \lambda e^{-\lambda x} \, {\mathrm{d}}x = \bigl[-e^{-\lambda x}\bigr]_t^\infty
                  = e^{-\lambda t}.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(4)</span></span>  In other words, the probability that we have to wait more than time <span class="math inline">\(t\)</span> for our event to happen is <span class="math inline">\(e^{-\lambda t}\)</span>, which is an exponential decay with rate <span class="math inline">\(\lambda\)</span>.</p>
<p>Now consider a discrete-time setting in which we perform one trial every <span class="math inline">\(\delta\)</span> seconds (where <span class="math inline">\(\delta\)</span> is very small — in fact, we will take <span class="math inline">\(\delta\to 0\)</span> to make time “continuous&quot;), and where our success probability is <span class="math inline">\(p=\lambda\delta\)</span>. Making the success probability proportional to <span class="math inline">\(\delta\)</span> makes sense, as it corresponds to the natural assumption that there is a fixed <span><em>rate</em></span> of success <span><em>per unit time</em></span>, which we denote by <span class="math inline">\(\lambda = p/\delta\)</span>. In this discrete setting, the number of trials until we get a success has the geometric distribution with parameter <span class="math inline">\(p\)</span>, so if we let the r.v. <span class="math inline">\(Y\)</span> denote the time (in seconds) until we get a success we have <span class="math display">\[{\mathbb{P}}[Y&gt; k\delta] = (1-p)^k = (1-\lambda\delta)^k \qquad\text{for any $k\ge 0$}.\]</span> Hence, for any <span class="math inline">\(t&gt;0\)</span>, we have <a name="eq:geomtail"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   {\mathbb{P}}[Y&gt; t] = {\mathbb{P}}\!\left[Y&gt;\left(\frac{t}{\delta}\right)\delta\right] = (1-\lambda\delta)^{t/\delta}
           \approx e^{-\lambda t},\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(5)</span></span>  where this final approximation holds in the limit as <span class="math inline">\(\delta\to 0\)</span> with <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(t\)</span> fixed. (We are ignoring the detail of rounding <span class="math inline">\(t/\delta\)</span> to an integer since we are taking an approximation anyway.)</p>
<p>Comparing the expression with Equation <a href="#eq:exptail">4</a>, we see that this distribution has the same form as the exponential distribution with parameter <span class="math inline">\(\lambda\)</span>, where <span class="math inline">\(\lambda\)</span> (the success rate per unit time) plays an analogous role to <span class="math inline">\(p\)</span> (the probability of success on each trial) — though note that <span class="math inline">\(\lambda\)</span> is not constrained to be <span class="math inline">\(\le 1\)</span>. Thus we may view the exponential distribution as a continuous time analog of the geometric distribution.</p>
<h2 id="normal-distribution" class="unnumbered">Normal Distribution</h2>
<p>The last continuous distribution we will look at, and by far the most prevalent in applications, is called the <span><em>normal</em></span> or <span><em>Gaussian</em></span> distribution. It has two parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, which are the mean and standard deviation of the distribution, respectively.</p>
<p><span id="definition:7" class="pandoc-numbering-text definition"><strong>Definition 7</strong> <em>(Normal Distribution)</em></span></p>
<p><em>For any <span class="math inline">\(\mu \in {\mathbb{R}}\)</span> and <span class="math inline">\(\sigma&gt; 0\)</span>, a continuous random variable <span class="math inline">\(X\)</span> with pdf</em> <span class="math display">\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \,e^{-(x-\mu)^2/(2\sigma^2)}\]</span> <em>is called a normal random variable with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. In the special case <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>, <span class="math inline">\(X\)</span> is said to have the standard normal distribution.</em></p>
<p>Let’s first check that this is a valid definition of a probability density function. Clearly <span class="math inline">\(f(x) \ge 0\)</span> from the definition. For Equation <a href="#eq:total">1</a>: <a name="eq:totalnormal"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   \int_{-\infty}^\infty f(x) \, {\mathrm{d}}x = \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^\infty e^{-(x-\mu)^2/(2\sigma^2)} =1.\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(6)</span></span>  The fact that this integral evaluates to <span class="math inline">\(1\)</span> is a routine exercise in integral calculus, and is left as an exercise (or feel free to look it up in any standard book on probability or on the internet).</p>
<p>A plot of the pdf <span class="math inline">\(f\)</span> reveals a classical “bell-shaped&quot; curve, centered at (and symmetric around) <span class="math inline">\(x=\mu\)</span>, and with “width&quot; determined by <span class="math inline">\(\sigma\)</span>. Figure <a href="#fig:normal">2</a> shows the normal density for several different choices of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<div class="figure">
<img src="n20-normal.png" alt="Figure 2: The density function for the normal distribution with several different choices for \mu and \sigma." id="fig:normal" style="width:50.0%" />
<p class="caption">Figure 2: The density function for the normal distribution with several different choices for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
</div>
<p>The figure above shows that the normal density with different values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are very similar to each other. Indeed, the normal distribution has the following nice property with respect to shifting and rescaling.</p>
<p><span id="lem:normalshift" class="pandoc-numbering-text lem"><strong>Lemma 1</strong></span></p>
<p><em>If <span class="math inline">\(X\)</span> is a normal random variable with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, then <span class="math inline">\(Y = (X - \mu)/\sigma\)</span> is a standard normal random variable. Equivalently, if <span class="math inline">\(Y\)</span> is a standard normal random variable, then <span class="math inline">\(X = \sigma Y + \mu\)</span> has normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</em></p>
<p><em>Proof</em>. Given that <span class="math inline">\(X\)</span> has normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, we can calculate the distribution of <span class="math inline">\(Y=(X - \mu)/\sigma\)</span> as: <span class="math display">\[{\mathbb{P}}[a\le Y\le b] = {\mathbb{P}}[\sigma a+\mu \le X\le \sigma b+\mu] =
          \frac{1}{\sqrt{2\pi\sigma^2}}\int_{\sigma a+\mu}^{\sigma b+\mu} e^{-(x-\mu)^2/(2\sigma^2)} \, {\mathrm{d}}x =
          \frac{1}{\sqrt{2\pi}}\int_{a}^{b} e^{-y^2/2} \, {\mathrm{d}}y,\]</span> by a simple change of variable <span class="math inline">\(x = \sigma y + \mu\)</span> in the integral. Hence <span class="math inline">\(Y\)</span> is indeed standard normal. Note that <span class="math inline">\(Y\)</span> is obtained from <span class="math inline">\(X\)</span> just by shifting the origin to <span class="math inline">\(\mu\)</span> and scaling by <span class="math inline">\(\sigma\)</span>. <span class="math inline">\(\square\)</span></p>
<p>Let us now calculate the expectation and variance of a normal random variable.</p>
<p><span id="thm:normal-expectation-variance" class="pandoc-numbering-text thm"><strong>Theorem 3</strong></span></p>
<p><em>For a normal random variable <span class="math inline">\(X\)</span> with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>,</em> <span class="math display">\[{\mathbb{E}}[X] = \mu \qquad \text { and } \qquad {\operatorname{var}}(X) = \sigma^2.\]</span></p>
<p><em>Proof</em>. Let’s first consider the case when <span class="math inline">\(X\)</span> is standard normal, i.e., when <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma = 1\)</span>. By definition, its expectation is <span class="math display">\[{\mathbb{E}}[X] = \int_{-\infty}^\infty xf(x) \, {\mathrm{d}}x = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty x e^{-x^2/2} \, {\mathrm{d}}x =
           \frac{1}{\sqrt{2\pi}}\left(\int_{-\infty}^0 x e^{-x^2/2} \, {\mathrm{d}}x +\int_{0}^\infty x e^{-x^2/2} \, {\mathrm{d}}x \right) = 0.\]</span> The last step follows from the fact that the function <span class="math inline">\(e^{-x^2/2}\)</span> is symmetrical about <span class="math inline">\(x=0\)</span>, so the two integrals are the same except for the sign. For the variance, we have <span class="math display">\[\begin{aligned}
   {\operatorname{var}}(X) = {\mathbb{E}}[X^2] - {\mathbb{E}}[X]^2 &amp;= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty x^2 e^{-x^2/2} \, {\mathrm{d}}x \\
                &amp;= \frac{1}{\sqrt{2\pi}}\bigl[-xe^{-x^2/2}\bigr]_{-\infty}^\infty
                        + \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-x^2/2} \, {\mathrm{d}}x \\
                &amp;=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{-x^2/2} \, {\mathrm{d}}x = 1.\end{aligned}\]</span> In the first line here we used the fact that <span class="math inline">\({\mathbb{E}}[X]=0\)</span>; in the second line we used integration by parts; and in the last line we used Equation <a href="#eq:totalnormal">6</a> in the special case <span class="math inline">\(\mu=0\)</span>, <span class="math inline">\(\sigma=1\)</span>. So the standard normal distribution has expectation <span class="math inline">\({\mathbb{E}}[X]=0=\mu\)</span> and variance <span class="math inline">\({\operatorname{var}}(X)=1=\sigma^2\)</span>.</p>
<p>Now consider the general case when <span class="math inline">\(X\)</span> has normal distribution with general parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. By <a href="#lem:normalshift">Lemma 1</a>, we know that <span class="math inline">\(Y = (X - \mu)/\sigma\)</span> is a standard normal random variable, so <span class="math inline">\({\mathbb{E}}[Y] = 0\)</span> and <span class="math inline">\({\operatorname{var}}(Y) = 1\)</span>, as we have just established above. Therefore, we can read off the expectation and variance of <span class="math inline">\(X\)</span> from those of <span class="math inline">\(Y\)</span>. For the expectation, using linearity, we have <span class="math display">\[0 = {\mathbb{E}}[Y] = {\mathbb{E}}\!\left(\frac{X-\mu}{\sigma}\right) = \frac{{\mathbb{E}}[X]-\mu}{\sigma},\]</span> and hence <span class="math inline">\({\mathbb{E}}[X] = \mu\)</span>. For the variance we have <span class="math display">\[1 = {\operatorname{var}}(Y) = {\operatorname{var}}\!\left(\frac{X-\mu}{\sigma}\right) = \frac{{\operatorname{var}}(X)}{\sigma^2},\]</span> and hence <span class="math inline">\({\operatorname{var}}(X)=\sigma^2\)</span>. <span class="math inline">\(\square\)</span></p>
<p>The bottom line, then, is that the normal distribution has expectation <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. This explains the notation for the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<p>The fact that the variance is <span class="math inline">\(\sigma^2\)</span> (so that the standard deviation is <span class="math inline">\(\sigma\)</span>) explains our earlier comment that <span class="math inline">\(\sigma\)</span> determines the “width” of the normal distribution. Namely, by Chebyshev’s inequality, a constant fraction of the distribution lies within distance (say) <span class="math inline">\(2\sigma\)</span> of the expectation <span class="math inline">\(\mu\)</span>.</p>
<p><span><strong>Note:</strong></span> The above analysis shows that, by means of a simple origin shift and scaling, we can relate any normal distribution to the standard normal. This means that, when doing computations with normal distributions, it’s enough to do them for the standard normal. For this reason, books and online sources of mathematical formulas usually contain tables describing the density of the standard normal. From this, one can read off the corresponding information for any normal r.v. <span class="math inline">\(X\)</span> with parameters <span class="math inline">\(\mu,\sigma^2\)</span>, from the formula <span class="math display">\[{\mathbb{P}}[X\le a] = {\mathbb{P}}\!\left[Y\le \frac{a-\mu}{\sigma}\right],\]</span> where <span class="math inline">\(Y\)</span> is standard normal.</p>
<p>The normal distribution is ubiquitous throughout the sciences and the social sciences, because it is the standard model for any aggregate data that results from a large number of independent observations of the same random variable (such as the heights of females in the US population, or the observational error in a physical experiment). Such data, as is well known, tends to cluster around its mean in a “bell-shaped&quot; curve, with the correspondence becoming more accurate as the number of observations increases. A theoretical explanation of this phenomenon is the Central Limit Theorem, which we next discuss.</p>
<h3 id="sum-of-independent-normal-random-variables" class="unnumbered">Sum of Independent Normal Random Variables</h3>
<p>An important property of the normal distribution is that the sum of <span><em>independent</em></span> normal random variables is also normally distributed. We begin with the simple case when <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent standard normal random variables. In this case the result follows because the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is rotationally symmetric. The general case follows from the translation and scaling property of normal distribution in <a href="#lem:normalshift">Lemma 1</a>.</p>
<p><span id="thm:normalsum" class="pandoc-numbering-text thm"><strong>Theorem 4</strong></span></p>
<p><em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent standard normal random variables, and <span class="math inline">\(a,b \in \mathbb{R}\)</span> be constants. Then <span class="math inline">\(Z = aX + bY\)</span> is also a normal random variable with parameters <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = a^2 + b^2\)</span>.</em></p>
<p><em>Proof</em>.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Since <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, by <a href="#thm:jointdensity">Theorem 1</a> we know that the joint density of <span class="math inline">\((X,Y)\)</span> is <span class="math display">\[f(x,y) = f(x) \cdot f(y) = \frac{1}{2\pi} e^{-(x^2+y^2)/2}.\]</span> The key observation is that <span class="math inline">\(f(x,y)\)</span> is rotationally symmetric around the origin, i.e., <span class="math inline">\(f(x,y)\)</span> only depends on the value <span class="math inline">\(x^2 + y^2\)</span>, which is the distance of the point <span class="math inline">\((x,y)\)</span> from the origin <span class="math inline">\((0,0)\)</span>; see Figure <a href="#fig:jointnormal">3</a>.</p>
<div class="figure">
<img src="n20-jointnormal.png" alt="Figure 3: The joint density function f(x,y) = (2\pi)^{-1} e^{-(x^2+y^2)/2} is rotationally symmetric." id="fig:jointnormal" style="width:50.0%" />
<p class="caption">Figure 3: The joint density function <span class="math inline">\(f(x,y) = (2\pi)^{-1} e^{-(x^2+y^2)/2}\)</span> is rotationally symmetric.</p>
</div>
<p>Thus, <span class="math inline">\(f(T(x,y)) = f(x,y)\)</span> where <span class="math inline">\(T\)</span> is any rotation of the plane <span class="math inline">\(\mathbb{R}^2\)</span> about the origin. It follows that for any set <span class="math inline">\(A \subseteq \mathbb{R}^2\)</span>, <a name="eq:normalrotation"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
{\mathbb{P}}[(X,Y) \in A] = {\mathbb{P}}[(X,Y) \in T(A)]\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(7)</span></span>  where <span class="math inline">\(T\)</span> is a rotation of <span class="math inline">\(\mathbb{R}^2\)</span>. Now given any <span class="math inline">\(t \in \mathbb{R}\)</span>, we have <span class="math display">\[{\mathbb{P}}[Z \le t] ~=~ {\mathbb{P}}[aX + bY \le t] ~=~ {\mathbb{P}}[(X,Y) \in A]\]</span> where <span class="math inline">\(A\)</span> is the half-plane <span class="math inline">\(\{(x,y) \mid ax + by \le t\}\)</span>. The boundary line <span class="math inline">\(ax+by = t\)</span> lies at a distance <span class="math inline">\(d = t/\sqrt{a^2 + b^2}\)</span> from the origin. Therefore, as illustrated in Figure <a href="#fig:normalrotation">4</a>, the set <span class="math inline">\(A\)</span> can be rotated into the set <span class="math display">\[T(A) = \left\{(x,y) : x \le \frac{t}{\sqrt{a^2+b^2}} \right\}.\]</span></p>
<div class="figure">
<img src="n20-normalrotation.png" alt="Figure 4: The half plane ax+by \le t is rotated into the half plane x \le t/\sqrt{a^2+b^2}." id="fig:normalrotation" style="width:70.0%" />
<p class="caption">Figure 4: The half plane <span class="math inline">\(ax+by \le t\)</span> is rotated into the half plane <span class="math inline">\(x \le t/\sqrt{a^2+b^2}\)</span>.</p>
</div>
<p>By Equation <a href="#eq:normalrotation">7</a>, this rotation does not change the probability: <span class="math display">\[{\mathbb{P}}[Z \le t] ~=~ {\mathbb{P}}[(X,Y) \in A] ~=~ {\mathbb{P}}[(X,Y) \in T(A)] ~=~ {\mathbb{P}}\!\left[X \le \frac{t}{\sqrt{a^2+b^2}} \right] ~=~ {\mathbb{P}}\!\left[\sqrt{a^2+b^2} \, X \le t\right].\]</span> Since the equation above holds for all <span class="math inline">\(t \in {\mathbb{R}}\)</span>, we conclude that <span class="math inline">\(Z\)</span> has the same distribution as <span class="math inline">\(\sqrt{a^2+b^2} \, X\)</span>. Since <span class="math inline">\(X\)</span> has standard normal distribution, we know by <a href="#lem:normalshift">Lemma 1</a> that <span class="math inline">\(\sqrt{a^2 + b^2} \, X\)</span> has normal distribution with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(a^2+b^2\)</span>. Hence we conclude that <span class="math inline">\(Z = aX + bY\)</span> also has normal distribution with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(a^2+b^2\)</span>. <span class="math inline">\(\square\)</span></p>
<p>The general case now follows easily from <a href="#lem:normalshift">Lemma 1</a> and <a href="#thm:normalsum">Theorem 4</a>.</p>
<p><span id="corollary:1" class="pandoc-numbering-text corollary"><strong>Corollary 1</strong></span></p>
<p><em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent normal random variables with parameters <span class="math inline">\((\mu_X, \sigma^2_X)\)</span> and <span class="math inline">\((\mu_Y, \sigma^2_Y)\)</span>, respectively. Then for any constants <span class="math inline">\(a,b \in {\mathbb{R}}\)</span>, the random variable <span class="math inline">\(Z = aX + bY\)</span> is also normally distributed with mean <span class="math inline">\(\mu = a\mu_X + b \mu_Y\)</span> and variance <span class="math inline">\(\sigma^2 = a^2 \sigma_X^2 + b^2 \sigma_Y^2\)</span>.</em></p>
<p><em>Proof</em>. By <a href="#lem:normalshift">Lemma 1</a>, <span class="math inline">\(Z_1 = (X - \mu_X) / \sigma_X\)</span> and <span class="math inline">\(Z_2 = (Y - \mu_Y) / \sigma_Y\)</span> are independent standard normal random variables. We can write: <span class="math display">\[\begin{aligned}
Z ~&amp;=~ aX + bY
~=~ a(\mu_X + \sigma_X Z_1) + b(\mu_Y + \sigma_Y Z_2)
~=~ (a\mu_X + b\mu_Y) + (a\sigma_X Z_1 + b\sigma_Y Z_2).\end{aligned}\]</span> By <a href="#thm:normalsum">Theorem 4</a>, <span class="math inline">\(Z&#39; = a\sigma_X Z_1 + b\sigma_Y Z_2\)</span> is normally distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^2 = a^2 \sigma_X^2 + b^2 \sigma_Y^2\)</span>. Since <span class="math inline">\(\mu = a\mu_X + b\mu_Y\)</span> is a constant, by <a href="#lem:normalshift">Lemma 1</a> we conclude that <span class="math inline">\(Z = \mu + Z&#39;\)</span> is a normal random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, as desired. <span class="math inline">\(\square\)</span></p>
<h1 id="the-central-limit-theorem" class="unnumbered">The Central Limit Theorem</h1>
<p>Recall from Note 18 the Law of Large Numbers for i.i.d. random variables <span class="math inline">\(X_i\)</span>’s: it says that the probability of <span><em>any</em></span> deviation <span class="math inline">\(\alpha\)</span> of the sample average <span class="math inline">\(A_n := n^{-1} {\sum_{i=1}^n X_i}\)</span> from the mean, however small, tends to zero as the number of observations <span class="math inline">\(n\)</span> in our average tends to infinity. Thus by taking <span class="math inline">\(n\)</span> large enough, we can make the probability of any given deviation as small as we like.</p>
<p>Actually we can say something much stronger than the Law of Large Numbers: namely, the distribution of the sample average <span class="math inline">\(A_n\)</span>, for large enough <span class="math inline">\(n\)</span>, looks like a <span><em>normal distribution</em></span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2/n\)</span>. (Of course, we already know that these are the mean and variance of <span class="math inline">\(A_n\)</span>; the point is that the distribution becomes normal!) The fact that the standard deviation decreases with <span class="math inline">\(n\)</span> (specifically, as <span class="math inline">\(\sigma/\sqrt{n}\)</span>) means that the distribution approaches a sharp spike at <span class="math inline">\(\mu\)</span>.</p>
<p>Recall from the last section that the density of the normal distribution is a symmetrical bell-shaped curve centered around the mean <span class="math inline">\(\mu\)</span>. Its height and width are determined by the standard deviation <span class="math inline">\(\sigma\)</span> as follows: the height at the mean <span class="math inline">\(x = \mu\)</span> is <span class="math inline">\(1/\sqrt{2\pi \sigma^2} \approx 0.4/\sigma\)</span>; <span class="math inline">\(50\%\)</span> of the mass is contained in the interval of width <span class="math inline">\(0.67\sigma\)</span> either side of the mean, and <span class="math inline">\(99.7\%\)</span> in the interval of width <span class="math inline">\(3\sigma\)</span> either side of the mean. (Note that, to get the correct scale, deviations are on the order of <span class="math inline">\(\sigma\)</span> rather than <span class="math inline">\(\sigma^2\)</span>.)</p>
<p>To state the Central Limit Theorem precisely (so that the limiting distribution is a constant rather than something that depends on <span class="math inline">\(n\)</span>), we shift the mean of <span class="math inline">\(A_n\)</span> to <span class="math inline">\(0\)</span> and scale it so that its variance is <span class="math inline">\(1\)</span>, i.e., we replace <span class="math inline">\(A_n\)</span> by <span class="math display">\[A&#39;_n = \frac{(A_n-\mu)\sqrt{n}}{\sigma} = \frac{\sum_{i=1}^n X_i -n\mu}{\sigma\sqrt{n}}.\]</span> The Central Limit Theorem then says that the distribution of <span class="math inline">\(A&#39;_n\)</span> converges to the <span><em>standard normal</em></span> distribution.</p>
<p><span id="thm:clt" class="pandoc-numbering-text thm"><strong>Theorem 5</strong></span></p>
<p><em>Let <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> be i.i.d. random variables with common expectation <span class="math inline">\(\mu={\mathbb{E}}[X_i]\)</span> and variance <span class="math inline">\(\sigma^2={\operatorname{var}}(X_i)\)</span> (both assumed to be <span class="math inline">\(&lt;\infty\)</span>). Define <span class="math inline">\(A&#39;_n = (\sum_{i=1}^n X_i -n\mu)/(\sigma\sqrt{n})\)</span>. Then as <span class="math inline">\(n\to\infty\)</span>, the distribution of <span class="math inline">\(A&#39;_n\)</span> approaches the standard normal distribution in the sense that, for any real <span class="math inline">\(\alpha\)</span>,</em> <span class="math display">\[{\mathbb{P}}[A&#39;_n \le \alpha] \to \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\alpha e^{-x^2/2} \, {\mathrm{d}}x\quad\text{as $n\to\infty$}.\]</span></p>
<p>The Central Limit Theorem is a very striking fact. What it says is the following: If we take an average of <span class="math inline">\(n\)</span> observations of any arbitrary r.v. <span class="math inline">\(X\)</span>, then the distribution of that average will be a bell-shaped curve centered at <span class="math inline">\(\mu={\mathbb{E}}[X]\)</span>. Thus all trace of the distribution of <span class="math inline">\(X\)</span> disappears as <span class="math inline">\(n\)</span> gets large: all distributions, no matter how complex,<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> look like the normal distribution when they are averaged. The only effect of the original distribution is through the variance <span class="math inline">\(\sigma^2\)</span>, which determines the width of the curve for a given value of <span class="math inline">\(n\)</span>, and hence the rate at which the curve shrinks to a spike.</p>
<h1 id="optional-buffons-needle" class="unnumbered">Optional: Buffon’s Needle</h1>
<p>Here is a simple yet interesting application of continuous random variables to the analysis of a classical procedure for estimating the value of <span class="math inline">\(\pi\)</span> known as <span><em>Buffon’s needle</em></span>, after its 18th century inventor Georges-Louis Leclerc, Comte de Buffon.</p>
<p>Here we are given a needle of length <span class="math inline">\(\ell\)</span>, and a board ruled with horizontal lines at distance <span class="math inline">\(\ell\)</span> apart. The experiment consists of throwing the needle randomly onto the board and observing whether or not it crosses one of the lines. We shall see below that (assuming a perfectly random throw) the probability of this event is exactly <span class="math inline">\(2/\pi\)</span>. This means that, if we perform the experiment many times and record the <span><em>proportion</em></span> of throws on which the needle crosses a line, then the Law of Large Numbers tells us that we will get a good estimate of the quantity <span class="math inline">\(2/\pi\)</span>, and therefore also of <span class="math inline">\(\pi\)</span>; and we can use Chebyshev’s inequality as in the other estimation problems we considered in that same Note to determine how many throws we need in order to achieve specified accuracy and confidence.</p>
<p>To analyze the experiment, let’s consider what random variables are in play. Note that the position where the needle lands is completely specified by two random variables: the vertical distance <span class="math inline">\(Y\)</span> between the midpoint of the needle and the closest horizontal line, and the angle <span class="math inline">\(\Theta\)</span> between the needle and the vertical. The r.v. <span class="math inline">\(Y\)</span> ranges between <span class="math inline">\(0\)</span> and <span class="math inline">\(\ell/2\)</span>, while <span class="math inline">\(\Theta\)</span> ranges between <span class="math inline">\(-\pi/2\)</span> and <span class="math inline">\(\pi/2\)</span>. Since we assume a perfectly random throw, we may assume that their <span><em>joint distribution</em></span> has density <span class="math inline">\(f(y,\theta)\)</span> that is uniform over the rectangle <span class="math inline">\([0,\ell/2]\times[-\pi/2,\pi/2]\)</span>. Since this rectangle has area <span class="math inline">\(\pi \ell/2\)</span>, the density should be <a name="eq:ytheta"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   f(y,\theta) = \begin{cases}
       2/\pi\ell &amp; \text{for $(y,\theta)\in [0,\ell/2]\times[-\pi/2,\pi/2]$;}\cr
       0 &amp; \text{otherwise}.\cr
   \end{cases}\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(8)</span></span>  Equivalently, <span class="math inline">\(Y\)</span> and <span class="math inline">\(\Theta\)</span> are independent random variables, each uniformly distributed in their respective range. As a sanity check, let’s verify that the integral of this density over all possible values is indeed <span class="math inline">\(1\)</span>: <span class="math display">\[\int_{-\infty}^\infty \int_{-\infty}^\infty f(y,\theta) \, {\mathrm{d}}y \, {\mathrm{d}}\theta
        = \int_{-\pi/2}^{\pi/2}\int_0^{\ell/2} \frac{2}{\pi\ell} \, {\mathrm{d}}y \, {\mathrm{d}}\theta
        = \int_{-\pi/2}^{\pi/2} \left[\frac{2y}{\pi\ell}\right]_0^{\ell/2} \, {\mathrm{d}}\theta
        = \int_{-\pi/2}^{\pi/2} \frac{1}{\pi} \, {\mathrm{d}}\theta
        = \left[\frac{\theta}{\pi}\right]_{-\pi/2}^{\pi/2}
        = 1.\]</span> This is an analog of Equation <a href="#eq:total">1</a> for our joint distribution; rather than the area under the curve <span class="math inline">\(f(x)\)</span>, we are now computing the area under the “surface&quot; <span class="math inline">\(f(y,\theta)\)</span>.</p>
<p>Now let <span class="math inline">\(E\)</span> denote the event that the needle crosses a line. How can we express this event in terms of the values of <span class="math inline">\(Y\)</span> and <span class="math inline">\(\Theta\)</span>? Well, by elementary geometry the vertical distance of the endpoint of the needle from its midpoint is <span class="math inline">\((\ell/2)\cos\Theta\)</span>, so the needle will cross the line if and only if <span class="math inline">\(Y\le (\ell/2)\cos\Theta\)</span>. Therefore we have <span class="math display">\[{\mathbb{P}}[E] = {\mathbb{P}}\!\left[Y \le \frac{\ell}{2}\cos\Theta\right] =
\int_{-\pi/2}^{\pi/2}\int_0^{(\ell/2)\cos\theta}
f(y,\theta)\, {\mathrm{d}}y \, {\mathrm{d}}\theta.\]</span></p>
<p>Substituting the density <span class="math inline">\(f(y,\theta)\)</span> from Equation <a href="#eq:ytheta">8</a> and performing the integration we get <span class="math display">\[{\mathbb{P}}[E] = \int_{-\pi/2}^{\pi/2}\int_0^{(\ell/2)\cos\theta} \frac{2}{\pi\ell} \, {\mathrm{d}}y \, {\mathrm{d}}\theta
               = \int_{-\pi/2}^{\pi/2} \left[\frac{2y}{\pi\ell}\right]_0^{(\ell/2)\cos\theta} \, {\mathrm{d}}\theta
               = \frac{1}{\pi}\int_{-\pi/2}^{\pi/2} \cos\theta \, {\mathrm{d}}\theta
               = \frac{1}{\pi}\left[\sin\theta\right]_{-\pi/2}^{\pi/2}
               = \frac{2}{\pi}.\]</span></p>
<p>This is exactly what we claimed at the beginning of the section!</p>
<h2 id="buffons-needle-a-slick-approach" class="unnumbered">Buffon’s Needle: A Slick Approach</h2>
<div class="figure">
<img src="n20-needle.png" alt="Figure 5: Buffon’s Needle." id="fig:needle" style="width:40.0%" />
<p class="caption">Figure 5: Buffon’s Needle.</p>
</div>
<p>Recall that we toss a unit length needle on (an infinite) board ruled with horizontal lines spaced at unit length apart. We wish to calculate the chance that the needle intersects a horizontal line. That is, let <span class="math inline">\(I\)</span> be the event that the needle intersects a line. We will show that <span class="math inline">\({\mathbb{P}}[I] = 2/\pi\)</span>. Let <span class="math inline">\(X\)</span> be a random variable defined to be the number of intersections of such a needle: <span class="math display">\[X = \left\{ \begin{array}{cc}
1&amp;\mbox{ if the needle intersects a line }\\
0&amp;\mbox{ otherwise. }
\end{array}\right.\]</span></p>
<p>Since <span class="math inline">\(X\)</span> is an indicator random variable, <span class="math inline">\({\mathbb{E}}[X] = {\mathbb{P}}[I]\)</span> (here we are assuming the case in which the needle lies perfectly on two lines cannot happen, since the probability of this particular event is <span class="math inline">\(0\)</span>). Now, imagine that we were tossing a needle of two unit length, and we let <span class="math inline">\(Z\)</span> be the random variable representing the number of times such a needle intersects horizontal lines on the plane. We can “split&quot; the needle into two parts of unit length and get <span class="math inline">\(Z = X_1 + X_2\)</span>, where <span class="math inline">\(X_i\)</span> is <span class="math inline">\(1\)</span> if segment <span class="math inline">\(i\)</span> of the needle intersects and <span class="math inline">\(0\)</span> otherwise. Thus, <span class="math inline">\({\mathbb{E}}[Z] = {\mathbb{E}}[X_1 + X_2] = {\mathbb{E}}[X_1] + {\mathbb{E}}[X_2] = 2{\mathbb{E}}[X]\)</span>, since each segment is of unit length. A similar argument holds if we split a unit length needle into <span class="math inline">\(m\)</span> equal segments, so that <span class="math inline">\(X = X_1 + \cdots + X_m\)</span>, where <span class="math inline">\(X_i\)</span> is <span class="math inline">\(1\)</span> if segment <span class="math inline">\(i\)</span> (which has length <span class="math inline">\(1/m\)</span>) intersects a line and <span class="math inline">\(0\)</span> otherwise. We have that <span class="math inline">\({\mathbb{E}}[X] = {\mathbb{E}}[X_1 + \cdots + X_m] = {\mathbb{E}}[X_1] + \cdots + {\mathbb{E}}[X_m]\)</span>. But each of the <span class="math inline">\({\mathbb{E}}[X_i]\)</span>s are equal, so we get <span class="math inline">\({\mathbb{E}}[X] = m{\mathbb{E}}[X_i] \rightarrow {\mathbb{E}}[X_i] = (\text{length of segment
$i$}) \cdot {\mathbb{E}}[X]\)</span>. Thus, if we drop a needle of length <span class="math inline">\(l\)</span>, and we let <span class="math inline">\(Y\)</span> be the number of intersections, then <span class="math inline">\({\mathbb{E}}[Y] = l \cdot {\mathbb{E}}[X]\)</span>. Even if we have a needle which is arbitrarily short, say length <span class="math inline">\(\epsilon\)</span>, we still have <span class="math inline">\({\mathbb{E}}[Y] = \epsilon \cdot {\mathbb{E}}[X]\)</span>.</p>
<div class="figure">
<img src="n20-noodle.png" alt="Figure 6: “Noodle” consisting of 5 needles." id="fig:noodle" style="width:30.0%" />
<p class="caption">Figure 6: “Noodle” consisting of <span class="math inline">\(5\)</span> needles.</p>
</div>
<p>Now, consider a “noodle&quot; comprised of two needles of arbitrary lengths <span class="math inline">\(l_1\)</span> and <span class="math inline">\(l_2\)</span> (with corresponding random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>), where the needles are connected by a rotating joint, we can conclude by linearity of expectation that <span class="math inline">\({\mathbb{E}}[X_1 + X_2] = {\mathbb{E}}[X_1] + {\mathbb{E}}[X_2]\)</span>. In general, we can have a noodle comprised of <span class="math inline">\(n\)</span> needles of arbitrary length: where <span class="math inline">\({\mathbb{E}}[X_1 + \cdots + X_n] = {\mathbb{E}}[X_1] + \cdots + {\mathbb{E}}[X_n] = l_1{\mathbb{E}}[X] + \cdots + l_n{\mathbb{E}}[X]\)</span>. Factoring <span class="math inline">\({\mathbb{E}}[X]\)</span> out, we get that the expected number of intersections of a noodle is <span class="math inline">\((\text{length of noodle}) \cdot {\mathbb{E}}[X]\)</span>. In particular, since we are allowed to string together needles at connected rotating joints and since each needle can be arbitrarily short, we are free to pick any shape, but which one?</p>
<div class="figure">
<img src="n20-circle.png" alt="Figure 7: A circle always has two intersections." id="fig:circle" style="width:50.0%" />
<p class="caption">Figure 7: A circle always has two intersections.</p>
</div>
<p>Consider a circle with a unit length diameter (and therefore circumference <span class="math inline">\(\pi\)</span>). Such a shape must always intersect twice: which implies <span class="math inline">\({\mathbb{E}}[\text{number of circle intersections}] = \pi \cdot {\mathbb{E}}[X] = 2\)</span>, and thus <span class="math inline">\({\mathbb{E}}[X] = 2/\pi = {\mathbb{P}}[I]\)</span>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>A formal treatment of which events can be assigned a well-defined probability requires a discussion of <span><em>measure theory</em></span>, which is beyond the scope of this course.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Note that it does not matter whether or not we include the endpoints <span class="math inline">\(a,b\)</span>; since <span class="math inline">\({\mathbb{P}}[X=a]={\mathbb{P}}[X=b]=0\)</span>, we have <span class="math inline">\({\mathbb{P}}[a&lt; X&lt; b] = {\mathbb{P}}[a\le X\le b]\)</span>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>The following proof and figures are adapted from <span><em>“Why Is the Sum of Independent Normal Random Variables Normal?”</em></span> by B. Eisenberg and R. Sullivan, Mathematics Magazine, Vol. 81, No. 5.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>We do need to assume that the mean and variance of <span class="math inline">\(X\)</span> are finite.<a href="#fnref4">↩</a></p></li>
</ol>
</div>
</article>
</body>

<!-- Mirrored from www.eecs70.org/static/notes/n20.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:42:38 GMT -->
</html>
