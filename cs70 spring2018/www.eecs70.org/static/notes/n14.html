<!DOCTYPE html>
<!--==============================================================================
	           "GitHub HTML5 Pandoc Template" v1.2 — by Tristano Ajmone           
	==============================================================================
	(c) Tristano Ajmone, 2017, MIT License (MIT). Project's home repository:

	- https://github.com/tajmone/pandoc-goodies

	This template reuses source code taken from the following projects:

	- GitHub Markdown CSS: © Sindre Sorhus, MIT License (MIT):
	  https://github.com/sindresorhus/github-markdown-css

	- Primer CSS: © 2016 GitHub Inc., MIT License (MIT):
	  http://primercss.io/
	==============================================================================-->
<html>

<!-- Mirrored from www.eecs70.org/static/notes/n14.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:40:05 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Conditional Probability</title>
<style type="text/css">@font-face{font-family:octicons-link;src:url(data:font/woff;charset=utf-8;base64,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) format('woff')}.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;color:#24292e;font-family:-apple-system,system-ui,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:16px;line-height:1.5;word-wrap:break-word;box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto;padding:45px}.markdown-body .octicon{display:inline-block;fill:currentColor;vertical-align:text-bottom}.markdown-body a{background-color:transparent;-webkit-text-decoration-skip:objects;color:#0366d6;text-decoration:none}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body h1{margin:.67em 0}.markdown-body img{border-style:none}.markdown-body svg:not(:root){overflow:hidden}.markdown-body code,.markdown-body kbd,.markdown-body pre{font-family:monospace,monospace}.markdown-body input{font:inherit;margin:0;overflow:visible;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=checkbox]{box-sizing:border-box;padding:0}.markdown-body *{box-sizing:border-box}.markdown-body a:hover{text-decoration:underline}.markdown-body strong{font-weight:600}.markdown-body hr{box-sizing:content-box;overflow:hidden;background:0 0;border-bottom:1px solid #dfe2e5}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body table{border-spacing:0;border-collapse:collapse;display:block;width:100%;overflow:auto}.markdown-body td,.markdown-body th{padding:0}.markdown-body blockquote{margin:0}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body dd{margin-left:0}.markdown-body code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace}.markdown-body pre{font:12px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;word-wrap:normal}.markdown-body .pl-0{padding-left:0!important}.markdown-body .pl-1{padding-left:4px!important}.markdown-body .pl-2{padding-left:8px!important}.markdown-body .pl-3{padding-left:16px!important}.markdown-body .pl-4{padding-left:24px!important}.markdown-body .pl-5{padding-left:32px!important}.markdown-body .pl-6{padding-left:40px!important}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:0}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e1e4e8;border:0}.markdown-body blockquote{padding:0 1em;color:#6a737d;border-left:.25em solid #dfe2e5}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd{font-size:11px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#1b1f23;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1{padding-bottom:.3em;font-size:2em;border-bottom:1px solid #eaecef}.markdown-body h2{padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid #eaecef}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#6a737d}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #dfe2e5}.markdown-body table tr{background-color:#fff;border-top:1px solid #c6cbd1}.markdown-body table tr:nth-child(2n){background-color:#f6f8fa}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body code{padding:.2em 0;margin:0;font-size:85%;background-color:rgba(27,31,35,.05);border-radius:3px}.markdown-body code::after,.markdown-body code::before{letter-spacing:-.2em;content:"\00a0"}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f6f8fa;border-radius:3px}.markdown-body pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code::after,.markdown-body pre code::before{content:normal}.markdown-body .full-commit .btn-outline:not(:disabled):hover{color:#005cc5;border-color:#005cc5}.markdown-body kbd{display:inline-block;padding:3px 5px;font:11px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fcfcfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body :checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item+.task-list-item{margin-top:3px}.markdown-body .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown-body hr{border-bottom-color:#eee}.flash{position:relative;padding:16px;color:#246;background-color:#e2eef9;border:1px solid #bac6d3;border-radius:3px}.flash p:last-child{margin-bottom:0}.flash-messages{margin-bottom:24px}.flash-warn{color:#4c4a42;background-color:#fff9ea;border-color:#dfd8c2}.flash-error{color:#911;background-color:#fcdede;border-color:#d2b2b2}.flash-success{color:#22662c;background-color:#e2f9e5;border-color:#bad3be}.flash-plain{color:#4c4a42;background-color:#f5f5f5;border-color:#c1c1c1}.figure{text-align:center;}</style>
  <style type="text/css">code{white-space: pre;}</style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {equationNumbers: {autoNumber: "AMS"}}
    });
  </script>
  <script src="../../../cdn.mathjax.org/mathjax/latest/MathJax2ba6.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title">Conditional Probability</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#conditional-probability">Conditional Probability</a><ul>
<li><a href="#example-card-dealing">Example: Card Dealing</a></li>
</ul></li>
<li><a href="#bayesian-inference">Bayesian Inference</a></li>
<li><a href="#bayes-rule-and-total-probability-rule">Bayes Rule and Total Probability Rule</a><ul>
<li><a href="#example-tennis-match">Example: Tennis Match</a></li>
<li><a href="#example-balls-and-bins">Example: Balls and Bins</a></li>
</ul></li>
<li><a href="#combinations-of-events">Combinations of Events</a><ul>
<li><a href="#independent-events">Independent Events</a><ul>
<li><a href="#pairwise-independence-example">Pairwise Independence Example</a></li>
</ul></li>
<li><a href="#intersections-of-events">Intersections of Events</a></li>
<li><a href="#examples">Examples</a><ul>
<li><a href="#coin-tosses">Coin Tosses</a></li>
<li><a href="#monty-hall">Monty Hall</a></li>
<li><a href="#poker-hands">Poker Hands</a></li>
</ul></li>
<li><a href="#unions-of-events">Unions of Events</a></li>
</ul></li>
</ul>
</nav>
<h1 id="introduction" class="unnumbered">Introduction</h1>
<p>One of the key properties of coin flips is independence: if you flip a fair coin ten times and get ten <span class="math inline">\(T\)</span>’s, this does not make it more likely that the next coin flip will be <span class="math inline">\(H\)</span>’s. It still has exactly <span class="math inline">\(50\%\)</span> chance of being <span class="math inline">\(H\)</span>’s.</p>
<p>By contrast, suppose while dealing cards, the first ten cards are all red (hearts or diamonds). What is the chance that the next card is red? This is easy: we started with exactly <span class="math inline">\(26\)</span> red cards and <span class="math inline">\(26\)</span> black cards. But after dealing the first ten cards we know that the deck has <span class="math inline">\(16\)</span> red cards and <span class="math inline">\(26\)</span> black cards. So the chance that the next card is red is <span class="math inline">\(16/42\)</span>. So unlike the case of coin flips, now the chance of drawing a red card is no longer independent of the previous card that was dealt. This is the phenomenon we will explore in this note on conditional probability.</p>
<h1 id="conditional-probability" class="unnumbered">Conditional Probability</h1>
<p>Let’s consider an example with a smaller sample space. Suppose we toss <span class="math inline">\(m=3\)</span> balls into <span class="math inline">\(n=3\)</span> bins; this is a uniform sample space with <span class="math inline">\(3^3=27\)</span> points. We already know that the probability the first bin is empty is <span class="math inline">\((1-1/3)^3=(2/3)^3=8/27\)</span>. What is the probability of this event <span><em>given that</em></span> the second bin is empty? Call these events <span class="math inline">\(A,B\)</span> respectively. In the language of conditional probability we wish to compute the probability <span class="math inline">\({\mathbb{P}}[A\mid B]\)</span>, which we read to say probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>.</p>
<p>How should we compute <span class="math inline">\({\mathbb{P}}[A\mid B]\)</span>? Well, since event <span class="math inline">\(B\)</span> is guaranteed to happen, we need to look not at the whole sample space <span class="math inline">\(\Omega\)</span>, but at the smaller sample space consisting only of the sample points in <span class="math inline">\(B\)</span>. In terms of the picture below, we are no longer looking at the large oval, but only the oval labeled <span class="math inline">\(B\)</span>:</p>
<div class="figure">
<img src="n14-oval.png" style="width:30.0%" />

</div>
<p>What should the probabilities of these sample points be? If they all simply inherit their probabilities from <span class="math inline">\(\Omega\)</span>, then the sum of these probabilities will be <span class="math inline">\(\sum_{\omega\in B}{\mathbb{P}}[\omega]={\mathbb{P}}[B]\)</span>, which in general is less than <span class="math inline">\(1\)</span>. So we need to <span><em>scale</em></span> the probability of each sample point by <span class="math inline">\(1/{\mathbb{P}}[B]\)</span>. I.e., for each sample point <span class="math inline">\(\omega\in B\)</span>, the new probability becomes <span class="math display">\[{\mathbb{P}}[\omega\mid B]={{{\mathbb{P}}[\omega]}\over{{\mathbb{P}}[B]}}.\]</span> Now it is clear how to compute <span class="math inline">\({\mathbb{P}}[A\mid B]\)</span>: namely, we just sum up these scaled probabilities over all sample points that lie in both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>: <span class="math display">\[{\mathbb{P}}[A\mid B] = \sum_{\omega\in A\cap B} {\mathbb{P}}[\omega\mid B]
                 = \sum_{\omega\in A\cap B} {{{\mathbb{P}}[\omega]\over{{\mathbb{P}}[B]}}}
                 = {{{\mathbb{P}}[A\cap B]}\over{{\mathbb{P}}[B]}}.\]</span></p>
<p><span id="def:cp" class="pandoc-numbering-text def"><strong>Definition 1</strong> <em>(Conditional Probability)</em></span></p>
<p><em>For events <span class="math inline">\(A,B\)</span> in the same probability space, such that <span class="math inline">\({\mathbb{P}}[B]&gt;0\)</span>, the conditional probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is <span class="math display">\[{\mathbb{P}}[A\mid B] = {{{\mathbb{P}}[A\cap B]}\over{{\mathbb{P}}[B]}}.\]</span></em></p>
<p>Returning to our example, to compute <span class="math inline">\({\mathbb{P}}[A\mid B]\)</span> we need to figure out <span class="math inline">\({\mathbb{P}}[A\cap B]\)</span>. But <span class="math inline">\(A\cap B\)</span> is the event that both the first two bins are empty, i.e., all three balls fall in the third bin. So <span class="math inline">\({\mathbb{P}}[A\cap B]=1/27\)</span> (why?). Therefore, <span class="math display">\[{\mathbb{P}}[A\mid B] = {{{\mathbb{P}}[A\cap B]}\over{{\mathbb{P}}[B]}} = {{1/27}\over{8/27}} = {1\over 8}.\]</span> Not surprisingly, <span class="math inline">\(1/8\)</span> is quite a bit less than <span class="math inline">\(8/27\)</span>: knowing that bin <span class="math inline">\(2\)</span> is empty makes it significantly less likely that bin <span class="math inline">\(1\)</span> will be empty.</p>
<h2 id="example-card-dealing" class="unnumbered">Example: Card Dealing</h2>
<p>Let’s apply the ideas discussed above to compute the probability that, when dealing <span class="math inline">\(2\)</span> cards and the first card is known to be an ace, the second card is also an ace. Let <span class="math inline">\(B\)</span> be the event that the first card is an ace, and let <span class="math inline">\(A\)</span> be the event that the second card is an ace. Note that <span class="math inline">\({\mathbb{P}}[A] = {\mathbb{P}}[B] = 1/13\)</span>.</p>
<p>To compute <span class="math inline">\({\mathbb{P}}[A\mid B]\)</span>, we need to figure out <span class="math inline">\({\mathbb{P}}[A\cap B]\)</span>. This is the probability that both cards are aces. Note that there are <span class="math inline">\(52\cdot 51\)</span> sample points in the sample space, since each sample point is a sequence of two cards. A sample point is in <span class="math inline">\(A\cap B\)</span> if both cards are aces. This can happen in <span class="math inline">\(4\cdot 3 = 12\)</span> ways.</p>
<p>Since each sample point is equally likely, <span class="math inline">\({\mathbb{P}}[A\cap B] = 12/(52 \cdot 51)\)</span>. The probability of event <span class="math inline">\(B\)</span>, drawing an ace in the first trial, is <span class="math inline">\(4/52\)</span>. Therefore, <span class="math display">\[{\mathbb{P}}[A\mid B] = {{{\mathbb{P}}[A\cap B]}\over{{\mathbb{P}}[B]}} = {3\over 51}.\]</span></p>
<p>Note that this says that if the first card is an ace, it makes it less likely that the second card is also an ace.</p>
<h1 id="bayesian-inference" class="unnumbered">Bayesian Inference</h1>
<p>Now that we’ve introduced the notion of conditional probability, we can see how it is used in real world settings. Conditional probability is at the heart of a subject called <span><em>Bayesian inference</em></span>, used extensively in fields such as machine learning, communications and signal processing. Bayesian inference is a way to <span><em>update knowledge</em></span> after making an observation. For example, we may have an estimate of the probability of a given event <span class="math inline">\(A\)</span>. After event <span class="math inline">\(B\)</span> occurs, we can update this estimate to <span class="math inline">\({\mathbb{P}}[A \mid B]\)</span>. In this interpretation, <span class="math inline">\({\mathbb{P}}[A]\)</span> can be thought of as a <span><em>prior</em></span> probability: our assessment of the likelihood of an event of interest <span class="math inline">\(A\)</span> <span><em>before</em></span> making an observation. It reflects our prior knowledge. <span class="math inline">\({\mathbb{P}}[A \mid B]\)</span> can be interpreted as the <span><em>posterior</em></span> probability of <span class="math inline">\(A\)</span> after the observation. It reflects our new knowledge.</p>
<p>Here is an example of where we can apply such a technique. A pharmaceutical company is marketing a new test for a certain medical disorder. According to clinical trials, the test has the following properties:</p>
<ol style="list-style-type: decimal">
<li><p>When applied to an affected person, the test comes up positive in <span class="math inline">\(90\%\)</span> of cases, and negative in <span class="math inline">\(10\%\)</span> (these are called “false negatives”).</p></li>
<li><p>When applied to a healthy person, the test comes up negative in <span class="math inline">\(80\%\)</span> of cases, and positive in <span class="math inline">\(20\%\)</span> (these are called “false positives”).</p></li>
</ol>
<p>Suppose that the incidence of the disorder in the US population is <span class="math inline">\(5\%\)</span>; this is our prior knowledge. When a random person is tested and the test comes up positive, how can we update this probability? (Note that this is presumably <span><em>not</em></span> the same as the simple probability that a random person has the disorder, which is just <span class="math inline">\(1/20\)</span>.) The implicit probability space here is the entire US population with uniform probabilities.</p>
<div class="figure">
<img src="n14-pharmacy.png" style="width:30.0%" />

</div>
<p>The sample space here consists of all people in the US — denote their number by <span class="math inline">\(N\)</span> (so <span class="math inline">\(N\approx 250 \text{ million}\)</span>). Let <span class="math inline">\(A\)</span> be the event that a person chosen at random is affected, and <span class="math inline">\(B\)</span> be the event that a person chosen at random tests positive. Now we can rewrite the information above:</p>
<ul>
<li><p><span class="math inline">\({\mathbb{P}}[A] = 0.05\)</span> (<span class="math inline">\(5\%\)</span> of the U.S. population is affected)</p></li>
<li><p><span class="math inline">\({\mathbb{P}}[B \mid A] = 0.9\)</span> (<span class="math inline">\(90\%\)</span> of the affected people test positive)</p></li>
<li><p><span class="math inline">\({\mathbb{P}}[B \mid \overline{A}] =0.2\)</span> (<span class="math inline">\(20\%\)</span> of healthy people test positive)</p></li>
</ul>
<p>We want to calculate <span class="math inline">\({\mathbb{P}}[A \mid B]\)</span>. We can proceed as follows:</p>
<p><a name="eq:bayes"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
 {\mathbb{P}}[A \mid B] = \frac{{\mathbb{P}}[A\cap B]}{{\mathbb{P}}[B]} = \frac{{\mathbb{P}}[B \mid A]{\mathbb{P}}[A]}{{\mathbb{P}}[B]}\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(1)</span></span> </p>
<p>We obtained the second equality above by applying the definition of conditional probability: <span class="math display">\[{\mathbb{P}}[B \mid A] = \frac{{\mathbb{P}}[A\cap B]}{{\mathbb{P}}[A]}\]</span></p>
<p>Now we need to compute <span class="math inline">\({\mathbb{P}}[B]\)</span>. This is the probability that a random person tests positive. To compute this, we can sum two values: the probability that a healthy person tests positive, <span class="math inline">\({\mathbb{P}}[\bar{A}\cap B]\)</span> and the probability that an affected person tests positive, <span class="math inline">\({\mathbb{P}}[A\cap B]\)</span>. We can sum because the events <span class="math inline">\(\bar{A}\cap B\)</span> and <span class="math inline">\(A\cap B\)</span> do not intersect:</p>
<div class="figure">
<img src="n14-disorder.png" style="width:30.0%" />

</div>
<p>By again applying the definition of conditional probability we have: <a name="eq:total"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
 {\mathbb{P}}[B] = {\mathbb{P}}[A \cap B] + {\mathbb{P}}[\overline{A} \cap B] = {\mathbb{P}}[B \mid A]{\mathbb{P}}[A] +
{\mathbb{P}}[B \mid \overline{A}] (1-{\mathbb{P}}[A])\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(2)</span></span> </p>
<p>Combining Equation <a href="#eq:bayes">1</a> and Equation <a href="#eq:total">2</a>, we have expressed <span class="math inline">\({\mathbb{P}}[A \mid B]\)</span> in terms of <span class="math inline">\({\mathbb{P}}[A]\)</span>, <span class="math inline">\({\mathbb{P}}[B \mid A]\)</span>, and <span class="math inline">\({\mathbb{P}}[B \mid \overline{A}]\)</span>:</p>
<p><a name="eq:update"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
 {\mathbb{P}}[A \mid B] = \frac{{\mathbb{P}}[B \mid A] {\mathbb{P}}[A]}{{\mathbb{P}}[B \mid A]{\mathbb{P}}[A] +
{\mathbb{P}}[B \mid \overline{A}] (1-{\mathbb{P}}[A])}\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(3)</span></span> </p>
<p>By plugging in the values written above, we obtain <span class="math inline">\({\mathbb{P}}[A \mid B] = 9/47 \approx .19\)</span>.</p>
<p>Equation <a href="#eq:update">3</a> is useful for many inference problems. We are given <span class="math inline">\({\mathbb{P}}[A]\)</span>, which is the (unconditional) probability that the event of interest <span class="math inline">\(A\)</span> happens. We are given <span class="math inline">\({\mathbb{P}}[B \mid A]\)</span> and <span class="math inline">\({\mathbb{P}}[B \mid \overline{A}]\)</span>, which quantify how noisy the observation is. (If <span class="math inline">\({\mathbb{P}}[B \mid A] =1\)</span> and <span class="math inline">\({\mathbb{P}}[B \mid \overline{A}] = 0\)</span>, for example, the observation is completely noiseless.) Now we want to calculate <span class="math inline">\({\mathbb{P}}[A \mid B]\)</span>, the probability that the event of interest happens given we made the observation. Equation <a href="#eq:update">3</a> allows us to do just that.<br />
<br />
Of course, Equation <a href="#eq:bayes">1</a>, Equation <a href="#eq:total">2</a>, and Equation <a href="#eq:update">3</a> are derived from the basic axioms of probability and the definition of conditional probability, and are therefore true with or without the above Bayesian inference interpretation. However, this interpretation is very useful when we apply probability theory to study inference problems.</p>
<h1 id="bayes-rule-and-total-probability-rule" class="unnumbered">Bayes Rule and Total Probability Rule</h1>
<p>Equation <a href="#eq:bayes">1</a> and Equation <a href="#eq:total">2</a> are very useful in their own right. The first is called <span><strong>Bayes Rule</strong></span> and the second is called the <span><strong>Total Probability Rule</strong></span>. Bayes Rule is useful when one wants to calculate <span class="math inline">\({\mathbb{P}}[A \mid B]\)</span> but one is given <span class="math inline">\({\mathbb{P}}[B \mid A]\)</span> instead, i.e. it allows us to “flip” things around.</p>
<p>The Total Probability Rule is an application of the strategy of “dividing into cases” .There are two possibilities: either an event <span class="math inline">\(A\)</span> happens or <span class="math inline">\(A\)</span> does not happen. If <span class="math inline">\(A\)</span> happens the probability that <span class="math inline">\(B\)</span> happens is <span class="math inline">\({\mathbb{P}}[B \mid A]\)</span>. If <span class="math inline">\(A\)</span> does not happen, the probability that <span class="math inline">\(B\)</span> happens is <span class="math inline">\({\mathbb{P}}[B \mid \overline{A}]\)</span>. If we know or can easily calculate these two probabilities and also <span class="math inline">\({\mathbb{P}}[A]\)</span>, then the total probability rule yields the probability of event <span class="math inline">\(B\)</span>.</p>
<h2 id="example-tennis-match" class="unnumbered">Example: Tennis Match</h2>
<p>You are about to play a tennis match against a randomly chosen opponent and you wish to calculate your probability of winning. You know your opponent will be one of two people, <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>. If person <span class="math inline">\(X\)</span> is chosen, you will win with probability <span class="math inline">\(.7\)</span>. If person <span class="math inline">\(Y\)</span> is chosen, you will win with probability <span class="math inline">\(.3\)</span>. Your opponent is chosen by flipping a coin with bias <span class="math inline">\(.6\)</span> in favor of <span class="math inline">\(X\)</span>.</p>
<p>Let’s first determine which events we are interested in. Let <span class="math inline">\(A\)</span> be the event that you win. Let <span class="math inline">\(B_1\)</span> be the event that person <span class="math inline">\(X\)</span> is chosen, and let <span class="math inline">\(B_2\)</span> be the event that person <span class="math inline">\(Y\)</span> is chosen. We wish to calculate <span class="math inline">\({\mathbb{P}}[A]\)</span>. Here is what we know so far:</p>
<ul>
<li><p><span class="math inline">\({\mathbb{P}}[A \mid B_1] = 0.7\)</span>, (if person <span class="math inline">\(X\)</span> is chosen, you win with probability <span class="math inline">\(.7\)</span>)</p></li>
<li><p><span class="math inline">\({\mathbb{P}}[A \mid B_2] = 0.3\)</span> (if person <span class="math inline">\(Y\)</span> is chosen, you win with probability <span class="math inline">\(.3\)</span>)</p></li>
<li><p><span class="math inline">\({\mathbb{P}}[B_1] =0.6\)</span> (person <span class="math inline">\(X\)</span> is chosen with probability <span class="math inline">\(.6\)</span>)</p></li>
<li><p><span class="math inline">\({\mathbb{P}}[B_2] =0.4\)</span> (person <span class="math inline">\(Y\)</span> is chosen with probability <span class="math inline">\(.4\)</span>)</p></li>
</ul>
<p>By using the Total Probability rule, we have: <span class="math display">\[{\mathbb{P}}[A] = {\mathbb{P}}[A \mid B_1]{\mathbb{P}}[B_1] + {\mathbb{P}}[A \mid B_2]{\mathbb{P}}[B_2].\]</span></p>
<p>Now we can simply plug in the known values above to obtain <span class="math inline">\({\mathbb{P}}[A]\)</span>: <span class="math display">\[{\mathbb{P}}[A] = .7\times.6 + .3\times .4 = .54\]</span></p>
<h2 id="example-balls-and-bins" class="unnumbered">Example: Balls and Bins</h2>
<p>Imagine we have two bins containing black and white balls, and further suppose that we wanted to know what is the chance that we picked bin <span class="math inline">\(1\)</span> given that we picked a white ball, i.e., <span class="math inline">\({\mathbb{P}}[\text{Bin } 1 \mid \bigcirc]\)</span>. Assume that we are unbiased when choosing a bin so that each bin is chosen with probability <span class="math inline">\(1/2\)</span>.</p>
<div class="figure">
<img src="n14-bins.png" style="width:30.0%" />

</div>
<p>A wrong approach is to say that the answer is clearly <span class="math inline">\(2/3\)</span>, since we know there are a total of three white balls, two of which are in bin <span class="math inline">\(1\)</span>. However, this picture is misleading because the bins have equal “weight&quot;. Instead, what we should do is appropriately scale each sample point as the following picture shows:</p>
<div class="figure">
<img src="n14-binprob.png" style="width:30.0%" />

</div>
<p>This images shows that the sample space <span class="math inline">\(\Omega\)</span> is equal to the union of the events contained in bin <span class="math inline">\(1\)</span> (<span class="math inline">\(A_1\)</span>) and bin <span class="math inline">\(2\)</span> (<span class="math inline">\(A_2\)</span>), so <span class="math inline">\(\Omega = A_1\cup A_2\)</span>. We can use the definition of conditional probability to see that</p>
<p><span class="math display">\[{\mathbb{P}}[\text{Bin } 1 \mid \bigcirc] = \frac{1/10 + 1/10}{1/10 + 1/10 + 1/4} = \frac{2/10}{9/20} = \frac{4}{9}.\]</span></p>
<p>Let us try to achieve this probability using Bayes Rule. To apply Bayes Rule, we need to compute <span class="math inline">\({\mathbb{P}}[\bigcirc \mid \text{Bin } 1]\)</span>, <span class="math inline">\({\mathbb{P}}[\text{Bin } 1]\)</span>, and <span class="math inline">\({\mathbb{P}}[\bigcirc]\)</span>. <span class="math inline">\({\mathbb{P}}[\bigcirc \mid \text{Bin }1]\)</span> is the chance that we pick a white ball given that we picked bin <span class="math inline">\(1\)</span>, which is <span class="math inline">\(2/5\)</span>. <span class="math inline">\({\mathbb{P}}[\text{Bin }1]\)</span> is <span class="math inline">\(1/2\)</span> as given in the description of the problem. Finally, <span class="math inline">\({\mathbb{P}}[\bigcirc]\)</span> can be computed using the Total Probability Rule: <span class="math display">\[{\mathbb{P}}[\bigcirc] = {\mathbb{P}}[\bigcirc \mid \text{Bin } 1] \cdot {\mathbb{P}}[\text{Bin } 1] + {\mathbb{P}}[\bigcirc \mid \text{Bin } 2] \cdot {\mathbb{P}}[\text{Bin } 2] = \frac{2}{5} \cdot \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2} = \frac{9}{20}.\]</span> Observe that we can apply the Total Probability Rule here because <span class="math inline">\({\mathbb{P}}[\text{Bin } 1]\)</span> is the complement of <span class="math inline">\({\mathbb{P}}[\text{Bin } 2]\)</span>. Finally, if we plug the above values into Bayes Rule we obtain the probability that we picked bin <span class="math inline">\(1\)</span> given that we picked a white ball: <span class="math display">\[{\mathbb{P}}[\text{Bin } 1 \mid \bigcirc] = \frac{(2/5) \cdot (1/2)}{9/20} = \frac{2/10}{9/20} = \frac{4}{9}\]</span> All we have done above is combined Bayes Rule and the Total Probability Rule; this is also how we obtained Equation <a href="#eq:update">3</a>. We could equivalently have plugged in the appropriate values to Equation <a href="#eq:update">3</a>.</p>
<h1 id="combinations-of-events" class="unnumbered">Combinations of Events</h1>
<p>In most applications of probability in Computer Science, we are interested in things like <span class="math inline">\({\mathbb{P}}[\bigcup_{i=1}^n A_i]\)</span> and <span class="math inline">\({\mathbb{P}}[\bigcap_{i=1}^n A_i]\)</span>, where the <span class="math inline">\(A_i\)</span> are simple events (i.e., we know, or can easily compute, the <span class="math inline">\({\mathbb{P}}[A_i]\)</span>). The intersection <span class="math inline">\(\bigcap_{i} A_i\)</span> corresponds to the logical <code>AND</code> of the events <span class="math inline">\(A_i\)</span>, while the union <span class="math inline">\(\bigcup_i A_i\)</span> corresponds to their logical <code>OR</code>. As an example, if <span class="math inline">\(A_i\)</span> denotes the event that a failure of type <span class="math inline">\(i\)</span> happens in a certain system, then <span class="math inline">\(\bigcup_i A_i\)</span> is the event that the system fails.</p>
<p>In general, computing the probabilities of such combinations can be very difficult. In this section, we discuss some situations where it can be done. Let’s start with independent events, for which intersections are quite simple to compute.</p>
<h2 id="independent-events" class="unnumbered">Independent Events</h2>
<p><span id="def:independence" class="pandoc-numbering-text def"><strong>Definition 2</strong> <em>(Independence)</em></span></p>
<p><em>Two events <span class="math inline">\(A,B\)</span> in the same probability space are independent if <span class="math inline">\({\mathbb{P}}[A\cap B]={\mathbb{P}}[A]\times {\mathbb{P}}[B]\)</span>.</em></p>
<p>The intuition behind this definition is the following. Suppose that <span class="math inline">\({\mathbb{P}}[B]&gt;0\)</span>. Then we have <span class="math display">\[{\mathbb{P}}[A\mid B] = \frac{{\mathbb{P}}[A\cap B]}{{\mathbb{P}}[B]} = \frac{{\mathbb{P}}[A]\times {\mathbb{P}}[B]}{{\mathbb{P}}[B]} = {\mathbb{P}}[A].\]</span> Thus independence has the natural meaning that “the probability of <span class="math inline">\(A\)</span> is not affected by whether or not <span class="math inline">\(B\)</span> occurs.” (By a symmetrical argument, we also have <span class="math inline">\({\mathbb{P}}[B\mid A] = {\mathbb{P}}[B]\)</span> provided <span class="math inline">\({\mathbb{P}}[A]&gt;0\)</span>.) For events <span class="math inline">\(A,B\)</span> such that <span class="math inline">\({\mathbb{P}}[B]&gt;0\)</span>, the condition <span class="math inline">\({\mathbb{P}}[A\mid B]={\mathbb{P}}[A]\)</span> is actually <span><em>equivalent</em></span> to the definition of independence.</p>
<p>Several of our previously mentioned random experiments consist of independent events. For example, if we flip a coin twice, the event of obtaining heads in the first trial is independent to the event of obtaining heads in the second trial. The same applies for two rolls of a die; the outcomes of each trial are independent.</p>
<p>The above definition generalizes to any finite set of events:</p>
<p><span id="def:mutual-independence" class="pandoc-numbering-text def"><strong>Definition 3</strong> <em>(Mutual Independence)</em></span></p>
<p><em>Events <span class="math inline">\(A_1,\ldots,A_n\)</span> are mutually independent if for every subset <span class="math inline">\(I\subseteq\{1,\ldots,n\}\)</span>,</em> <span class="math display">\[{\textstyle{\mathbb{P}}[\bigcap_{i\in I}A_i] = \prod_{i\in I}{\mathbb{P}}[A_i]}.\]</span></p>
<p>Note that we need this property to hold for <span><em>every</em></span> subset <span class="math inline">\(I\)</span>.</p>
<p>For mutually independent events <span class="math inline">\(A_1,\ldots,A_n\)</span>, it is not hard to check from the definition of conditional probability that, for any <span class="math inline">\(1\le i\le n\)</span> and any subset <span class="math inline">\(I\subseteq\{1,\ldots,n\}\setminus\{i\}\)</span>, we have <span class="math display">\[{\textstyle{\mathbb{P}}[A_i\mid \bigcap_{j\in I}A_j] = {\mathbb{P}}[A_i]}.\]</span></p>
<p>Note that the independence of every pair of events (so-called <span><em>pairwise independence</em></span>) does <span><em>not</em></span> necessarily imply mutual independence. For example, it is possible to construct three events <span class="math inline">\(A,B,C\)</span> such that each <span><em>pair</em></span> is independent but the triple <span class="math inline">\(A,B,C\)</span> is <span><em>not</em></span> mutually independent.</p>
<h3 id="pairwise-independence-example" class="unnumbered">Pairwise Independence Example</h3>
<p>Suppose you toss a fair coin twice and let <span class="math inline">\(A\)</span> be the event that the first flip is <span class="math inline">\(H\)</span> and <span class="math inline">\(B\)</span> be the event that the second flip is <span class="math inline">\(H\)</span>. Now let <span class="math inline">\(C\)</span> be the event that both flips are the same (i.e. both <span class="math inline">\(H\)</span> or both <span class="math inline">\(T\)</span>). Of course <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent. What is more interesting is that so are <span class="math inline">\(A\)</span> and <span class="math inline">\(C\)</span>: given that the first toss came up <span class="math inline">\(H\)</span>, there is still an even chance that the second flip is the same as the first. Another way of saying this is that <span class="math inline">\({\mathbb{P}}[A \cap C] = {\mathbb{P}}[A]{\mathbb{P}}[C] = 1/4\)</span> since <span class="math inline">\(A \cap C\)</span> is the event that the first flip is <span class="math inline">\(H\)</span> and the second is also <span class="math inline">\(H\)</span>. By the same reasoning <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> are also independent. On the other hand, <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> are not mutually independent. For example if we are given that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occurred then the probability that <span class="math inline">\(C\)</span> occurs is <span class="math inline">\(1\)</span>. So even though <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> are not mutually independent, every pair of them is independent. In other words, <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> are pairwise independent but not mutually independent.</p>
<h2 id="intersections-of-events" class="unnumbered">Intersections of Events</h2>
<p>Computing intersections of independent events is easy; it follows from the definition. We simply multiply the probabilities of each event. How do we compute intersections for events which may not be independent? From the definition of conditional probability, we immediately have the following <em>product rule</em> (sometimes also called the <em>chain rule</em>) for computing the probability of an intersection of events.</p>
<p><span id="thm:pr" class="pandoc-numbering-text thm"><strong>Theorem 1</strong> <em>(Product Rule)</em></span></p>
<p><em>For any events <span class="math inline">\(A,B\)</span>, we have <span class="math display">\[{\mathbb{P}}[A\cap B]= {\mathbb{P}}[A]{\mathbb{P}}[B\mid A].\]</span> More generally, for any events <span class="math inline">\(A_1,\ldots,A_n\)</span>, <span class="math display">\[{\textstyle{\mathbb{P}}[\bigcap_{i=1}^n A_i] = {\mathbb{P}}[A_1]\times{\mathbb{P}}[A_2\mid A_1]\times
                                      {\mathbb{P}}[A_3\mid A_1\cap A_2]\times
                        \cdots\times{\mathbb{P}}[A_n\mid\bigcap_{i=1}^{n-1}A_i]}.\]</span></em></p>
<p><em>Proof</em>. The first assertion follows directly from the definition of <span class="math inline">\({\mathbb{P}}[B\mid A]\)</span> (and is in fact a special case of the second assertion with <span class="math inline">\(n=2\)</span>).</p>
<p>To prove the second assertion, we will use induction on <span class="math inline">\(n\)</span> (the number of events). The base case is <span class="math inline">\(n=1\)</span>, and corresponds to the statement that <span class="math inline">\({\mathbb{P}}[A]={\mathbb{P}}[A]\)</span>, which is trivially true. For the inductive step, let <span class="math inline">\(n&gt;1\)</span> and assume (the inductive hypothesis) that <span class="math display">\[{\mathbb{P}}[{\textstyle\bigcap_{i=1}^{n-1} A_i}] = {\mathbb{P}}[A_1]\times{\mathbb{P}}[A_2\mid A_1]\times\cdots\times{\mathbb{P}}[A_{n-1}\mid{\textstyle\bigcap_{i=1}^{n-2} A_i}].\]</span> Now we can apply the definition of conditional probability to the two events <span class="math inline">\(A_n\)</span> and <span class="math inline">\(\bigcap_{i=1}^{n-1} A_i\)</span> to deduce that <span class="math display">\[\begin{aligned}{\mathbb{P}}[{\textstyle\bigcap_{i=1}^n A_i}] = {\mathbb{P}}[A_n\cap({\textstyle\bigcap_{i=1}^{n-1} A_i})]
                            &amp;= {\mathbb{P}}[A_n\mid{\textstyle\bigcap_{i=1}^{n-1} A_i}]
                                      \times{\mathbb{P}}[{\textstyle\bigcap_{i=1}^{n-1} A_i}] \\
                            &amp;= {\mathbb{P}}[A_n\mid{\textstyle\bigcap_{i=1}^{n-1} A_i}]
                                      \times{\mathbb{P}}[A_1]\times{\mathbb{P}}[A_2\mid A_1]\times\cdots\times{\mathbb{P}}[A_{n-1}\mid{\textstyle\bigcap_{i=1}^{n-2} A_i}],\end{aligned}\]</span> where in the last line we have used the inductive hypothesis. This completes the proof by induction. <span class="math inline">\(\square\)</span></p>
<p>The product rule is particularly useful when we can view our sample space as a sequence of choices. The next few examples illustrate this point.</p>
<h2 id="examples" class="unnumbered">Examples</h2>
<h3 id="coin-tosses" class="unnumbered">Coin Tosses</h3>
<p>Toss a fair coin three times. Let <span class="math inline">\(A\)</span> be the event that all three tosses are heads. Then <span class="math inline">\(A=A_1\cap A_2\cap A_3\)</span>, where <span class="math inline">\(A_i\)</span> is the event that the <span class="math inline">\(i\)</span>th toss comes up heads. We have <span class="math display">\[\begin{aligned} {\mathbb{P}}[A]&amp;={\mathbb{P}}[A_1]\times{\mathbb{P}}[A_2\mid A_1]\times{\mathbb{P}}[A_3\mid A_1\cap A_2] \\
               &amp;={\mathbb{P}}[A_1]\times{\mathbb{P}}[A_2]\times{\mathbb{P}}[A_3] \\
               &amp;=\frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{2} ={1\over 8}. \end{aligned}\]</span> The second line here follows from the fact that the tosses are mutually independent. Of course, we already know that <span class="math inline">\({\mathbb{P}}[A]=1/8\)</span> from our definition of the probability space in the previous lecture note. Another way of looking at this calculation is that it justifies our definition of the probability space, and shows that it was consistent with assuming that the coin flips are mutually independent.</p>
<p>If the coin is biased with heads probability <span class="math inline">\(p\)</span>, we get, again using independence, <span class="math display">\[{\mathbb{P}}[A] = {\mathbb{P}}[A_1]\times {\mathbb{P}}[A_2]\times {\mathbb{P}}[A_3] = p^3.\]</span> And more generally, the probability of any sequence of <span class="math inline">\(n\)</span> tosses containing <span class="math inline">\(r\)</span> heads and <span class="math inline">\(n-r\)</span> tails is <span class="math inline">\(p^r(1-p)^{n-r}\)</span>. This is in fact the reason we defined the probability space this way in the previous lecture note: we defined the sample point probabilities so that the coin tosses would behave independently.</p>
<h3 id="monty-hall" class="unnumbered">Monty Hall</h3>
<p>Recall the Monty Hall problem from the last lecture: there are three doors and the probability that the prize is behind any given door is <span class="math inline">\(1/3\)</span>. There are goats behind the other two doors. The contestant picks a door randomly, and the host opens one of the other two doors, revealing a goat. How do we calculate intersections in this setting? For example, what is the probability that the contestant chooses door <span class="math inline">\(1\)</span>, the prize is behind door <span class="math inline">\(2\)</span>, and the host chooses door <span class="math inline">\(3\)</span>?</p>
<p>Let <span class="math inline">\(A_1\)</span> be the event that the contestant chooses door <span class="math inline">\(1\)</span>, let <span class="math inline">\(A_2\)</span> be the event that the prize is behind door <span class="math inline">\(2\)</span>, and let <span class="math inline">\(A_3\)</span> be the event that the host chooses door <span class="math inline">\(3\)</span>. We would like to compute <span class="math inline">\({\mathbb{P}}[A_1\cap A_2\cap A_3]\)</span>. By the product rule: <span class="math display">\[{\mathbb{P}}[A_1\cap A_2\cap A_3] = {\mathbb{P}}[A_1]\times {\mathbb{P}}[A_2 \mid A_1] \times {\mathbb{P}}[A_3 \mid A_1\cap A_2]\]</span></p>
<p>The probability of <span class="math inline">\(A_1\)</span> is <span class="math inline">\(1/3\)</span>, since the contestant is choosing the door at random. The probability <span class="math inline">\(A_2\)</span> given <span class="math inline">\(A_1\)</span> is still <span class="math inline">\(1/3\)</span> since they are independent. The probability of the host choosing door <span class="math inline">\(3\)</span> given events <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> is <span class="math inline">\(1\)</span>; the host cannot choose door <span class="math inline">\(1\)</span>, since the contestant has already opened it, and the host cannot choose door <span class="math inline">\(2\)</span>, since the host must reveal a goat (and not the prize). Therefore, <span class="math display">\[{\mathbb{P}}[A_1\cap A_2\cap A_3] = \frac{1}{3}\times\frac{1}{3}\times 1 = \frac{1}{9}.\]</span></p>
<p>Observe that we did need conditional probability in this setting; had we simply multiplied the probabilities of each event, we would have obtained <span class="math inline">\(1/27\)</span> since the probability of <span class="math inline">\(A_3\)</span> is also <span class="math inline">\(1/3\)</span> (can you figure out why?). What if we changed the situation, and instead asked for the probability that the contestant chooses door <span class="math inline">\(1\)</span>, the prize is behind door <span class="math inline">\(1\)</span>, and the host chooses door <span class="math inline">\(2\)</span>? We can use the same technique as above, but our final answer will be different. This is left as an exercise.</p>
<h3 id="poker-hands" class="unnumbered">Poker Hands</h3>
<p>Let’s use the product rule to compute the probability of a flush in a different way. This is equal to <span class="math inline">\(4\times{\mathbb{P}}[A]\)</span>, where <span class="math inline">\(A\)</span> is the probability of a Hearts flush. Intuitively, this should be clear since there are <span class="math inline">\(4\)</span> suits; we’ll see why this is formally true in the next section. We can write <span class="math inline">\(A=\bigcap_{i=1}^5 A_i\)</span>, where <span class="math inline">\(A_i\)</span> is the event that the <span class="math inline">\(i\)</span>th card we pick is a Heart. So we have <span class="math display">\[{\textstyle{\mathbb{P}}[A]={\mathbb{P}}[A_1]\times{\mathbb{P}}[A_2\mid A_1]\times\cdots\times
                {\mathbb{P}}[A_5\mid\bigcap_{i=1}^4 A_i]}.\]</span> Clearly <span class="math inline">\({\mathbb{P}}[A_1]=13/52 = 1/4\)</span>. What about <span class="math inline">\({\mathbb{P}}[A_2\mid A_1]\)</span>? Well, since we are conditioning on <span class="math inline">\(A_1\)</span> (the first card is a Heart), there are only <span class="math inline">\(51\)</span> remaining possibilities for the second card, <span class="math inline">\(12\)</span> of which are Hearts. So <span class="math inline">\({\mathbb{P}}[A_2\mid A_1]=12/51\)</span>. Similarly, <span class="math inline">\({\mathbb{P}}[A_3\mid A_1\cap A_2]=11/50\)</span>, and so on. So we get <span class="math display">\[4\times{\mathbb{P}}[A]=4\times{{13}\over{52}}\times{{12}\over{51}}\times{{11}\over{50}}\times{{10}\over{49}}\times{{9}\over{48}},\]</span> which is exactly the same fraction we computed in the previous lecture note.</p>
<p>So now we have two methods of computing probabilities in many of our sample spaces. It is useful to keep these different methods around, both as a check on your answers and because in some cases one of the methods is easier to use than the other.</p>
<h2 id="unions-of-events" class="unnumbered">Unions of Events</h2>
<p>You are in Las Vegas, and you spy a new game with the following rules. You pick a number between <span class="math inline">\(1\)</span> and <span class="math inline">\(6\)</span>. Then three dice are thrown. You win if and only if your number comes up on at least one of the dice.</p>
<p>The casino claims that your odds of winning are <span class="math inline">\(50\%\)</span>, using the following argument. Let <span class="math inline">\(A\)</span> be the event that you win. We can write <span class="math inline">\(A=A_1\cup A_2\cup A_3\)</span>, where <span class="math inline">\(A_i\)</span> is the event that your number comes up on die <span class="math inline">\(i\)</span>. Clearly <span class="math inline">\({\mathbb{P}}[A_i]=1/6\)</span> for each <span class="math inline">\(i\)</span>. Therefore, <span class="math display">\[{\mathbb{P}}[A]={\mathbb{P}}[A_1\cup A_2\cup A_3] = {\mathbb{P}}[A_1]+{\mathbb{P}}[A_2]+{\mathbb{P}}[A_3]
                       = 3\times{1\over 6} = {1\over 2}.\]</span> Is this calculation correct? Well, suppose instead that the casino rolled six dice, and again you win iff your number comes up at least once. Then the analogous calculation would say that you win with probability <span class="math inline">\(6\times (1/6)=1\)</span>, i.e., certainly! The situation becomes even more ridiculous when the number of dice gets bigger than <span class="math inline">\(6\)</span>.</p>
<p>The problem is that the events <span class="math inline">\(A_i\)</span> are <span><em>not disjoint</em></span>: i.e., there are some sample points that lie in more than one of the <span class="math inline">\(A_i\)</span>. (We could get really lucky and our number could come up on two of the dice, or all three.) So if we add up the <span class="math inline">\({\mathbb{P}}[A_i]\)</span> we are counting some sample points more than once.</p>
<p>Fortunately, there is a formula for this, known as the <em>Principle of Inclusion/Exclusion</em>:</p>
<p><span id="thm:inclusion-exclusion" class="pandoc-numbering-text thm"><strong>Theorem 2</strong> <em>(Inclusion/Exclusion)</em></span></p>
<p><em>For events <span class="math inline">\(A_1,\ldots,A_n\)</span> in some probability space, we have <span class="math display">\[{\mathbb{P}}[{\textstyle\bigcup_{i=1}^n A_i}] = \sum_{i=1}^n {\mathbb{P}}[A_i]
                             -\sum_{\{i,j\}} {\mathbb{P}}[A_i\cap A_j]
                             +\sum_{\{i,j,k\}} {\mathbb{P}}[A_i\cap A_j\cap A_k]
                             -\cdots\pm {\mathbb{P}}[{\textstyle\bigcap_{i=1}^n A_i}].\]</span> [In the above summations, <span class="math inline">\(\{i,j\}\)</span> denotes all unordered pairs with <span class="math inline">\(i\ne j\)</span>, <span class="math inline">\(\{i,j,k\}\)</span> denotes all unordered triples of distinct elements, and so on.]</em></p>
<p>In other words, to compute <span class="math inline">\({\mathbb{P}}[\bigcup_i A_i]\)</span>, we start by summing the event probabilities <span class="math inline">\({\mathbb{P}}[A_i]\)</span>, then we <span><em>subtract</em></span> the probabilities of all pairwise intersections, then we <span><em>add</em></span> back in the probabilities of all three-way intersections, and so on.</p>
<p>We won’t prove this formula here; but you might like to verify it for the special case <span class="math inline">\(n=3\)</span> by drawing a Venn diagram and checking that every sample point in <span class="math inline">\(A_1\cup A_2\cup A_3\)</span> is counted exactly once by the formula. You might also like to prove the formula for general <span class="math inline">\(n\)</span> by induction (in similar fashion to the proof of the Product Rule above).</p>
<p>Taking the formula on faith, what is the probability we get lucky in the new game in Vegas? <span class="math display">\[{\mathbb{P}}[A_1\cup A_2\cup A_3] = {\mathbb{P}}[A_1]+{\mathbb{P}}[A_2]+{\mathbb{P}}[A_3]
                  -{\mathbb{P}}[A_1\cap A_2] -{\mathbb{P}}[A_1\cap A_3] -{\mathbb{P}}[A_2\cap A_3]
                                +{\mathbb{P}}[A_1\cap A_2\cap A_3].\]</span> Now the nice thing here is that the events <span class="math inline">\(A_i\)</span> are mutually independent (the outcome of any die does not depend on that of the others), so <span class="math inline">\({\mathbb{P}}[A_i\cap A_j]={\mathbb{P}}[A_i]{\mathbb{P}}[A_j] = (1/6)^2=1/36\)</span>, and similarly <span class="math inline">\({\mathbb{P}}[A_1\cap A_2\cap A_3]=(1/6)^3=1/216\)</span>. So we get <span class="math display">\[{\mathbb{P}}[A_1\cup A_2\cup A_3] = \left(3\times{1\over 6}\right) -
                                         \left(3\times{1\over {36}}\right) +
                                         {1\over{216}} =
                                             {{91}\over{216}}\approx 0.42.\]</span> So your odds are quite a bit worse than the casino is claiming!</p>
<p>When <span class="math inline">\(n\)</span> is large (i.e., we are interested in the union of many events), the Inclusion/Exclusion formula is essentially useless because it involves computing the probability of the intersection of every non-empty subset of the events: and there are <span class="math inline">\(2^n-1\)</span> of these! Sometimes we can just look at the first few terms of it and forget the rest: note that successive terms actually give us an overestimate and then an underestimate of the answer, and these estimates both get better as we go along.</p>
<p>However, in many situations we can get a long way by just looking at the term:</p>
<ol style="list-style-type: decimal">
<li><p><span><strong>Disjoint events.</strong></span> If the events <span class="math inline">\(A_i\)</span> are all <span><em>disjoint</em></span> (i.e., no pair of them contain a common sample point — such events are also called <span><em>mutually exclusive</em></span>), then <span class="math display">\[{\mathbb{P}}[{\textstyle\bigcup_{i=1}^n A_i}] = \sum_{i=1}^n {\mathbb{P}}[A_i].\]</span> [Note that we have already used this fact several times in our examples, e.g., in claiming that the probability of a flush is four times the probability of a Hearts flush — clearly flushes in different suits are disjoint events.]</p></li>
<li><p><span><strong>Union bound.</strong></span> Always, it is the case that <span class="math display">\[{\mathbb{P}}[{\textstyle\bigcup_{i=1}^n A_i}] \le \sum_{i=1}^n {\mathbb{P}}[A_i].\]</span> This merely says that adding up the <span class="math inline">\({\mathbb{P}}[A_i]\)</span> can only <span><em>over</em></span>estimate the probability of the union. Crude as it may seem, in the next lecture note we’ll see how to use the union bound effectively in a Computer Science example.</p></li>
</ol>
</article>
</body>

<!-- Mirrored from www.eecs70.org/static/notes/n14.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:40:16 GMT -->
</html>
