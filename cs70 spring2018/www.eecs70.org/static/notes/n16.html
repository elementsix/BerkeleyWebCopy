<!DOCTYPE html>
<!--==============================================================================
	           "GitHub HTML5 Pandoc Template" v1.2 — by Tristano Ajmone           
	==============================================================================
	(c) Tristano Ajmone, 2017, MIT License (MIT). Project's home repository:

	- https://github.com/tajmone/pandoc-goodies

	This template reuses source code taken from the following projects:

	- GitHub Markdown CSS: © Sindre Sorhus, MIT License (MIT):
	  https://github.com/sindresorhus/github-markdown-css

	- Primer CSS: © 2016 GitHub Inc., MIT License (MIT):
	  http://primercss.io/
	==============================================================================-->
<html>

<!-- Mirrored from www.eecs70.org/static/notes/n16.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:40:16 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Random Variables - Distribution and Expectation</title>
<style type="text/css">@font-face{font-family:octicons-link;src:url(data:font/woff;charset=utf-8;base64,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) format('woff')}.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;color:#24292e;font-family:-apple-system,system-ui,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:16px;line-height:1.5;word-wrap:break-word;box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto;padding:45px}.markdown-body .octicon{display:inline-block;fill:currentColor;vertical-align:text-bottom}.markdown-body a{background-color:transparent;-webkit-text-decoration-skip:objects;color:#0366d6;text-decoration:none}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body h1{margin:.67em 0}.markdown-body img{border-style:none}.markdown-body svg:not(:root){overflow:hidden}.markdown-body code,.markdown-body kbd,.markdown-body pre{font-family:monospace,monospace}.markdown-body input{font:inherit;margin:0;overflow:visible;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=checkbox]{box-sizing:border-box;padding:0}.markdown-body *{box-sizing:border-box}.markdown-body a:hover{text-decoration:underline}.markdown-body strong{font-weight:600}.markdown-body hr{box-sizing:content-box;overflow:hidden;background:0 0;border-bottom:1px solid #dfe2e5}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body table{border-spacing:0;border-collapse:collapse;display:block;width:100%;overflow:auto}.markdown-body td,.markdown-body th{padding:0}.markdown-body blockquote{margin:0}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body dd{margin-left:0}.markdown-body code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace}.markdown-body pre{font:12px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;word-wrap:normal}.markdown-body .pl-0{padding-left:0!important}.markdown-body .pl-1{padding-left:4px!important}.markdown-body .pl-2{padding-left:8px!important}.markdown-body .pl-3{padding-left:16px!important}.markdown-body .pl-4{padding-left:24px!important}.markdown-body .pl-5{padding-left:32px!important}.markdown-body .pl-6{padding-left:40px!important}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:0}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e1e4e8;border:0}.markdown-body blockquote{padding:0 1em;color:#6a737d;border-left:.25em solid #dfe2e5}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd{font-size:11px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#1b1f23;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1{padding-bottom:.3em;font-size:2em;border-bottom:1px solid #eaecef}.markdown-body h2{padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid #eaecef}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#6a737d}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #dfe2e5}.markdown-body table tr{background-color:#fff;border-top:1px solid #c6cbd1}.markdown-body table tr:nth-child(2n){background-color:#f6f8fa}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body code{padding:.2em 0;margin:0;font-size:85%;background-color:rgba(27,31,35,.05);border-radius:3px}.markdown-body code::after,.markdown-body code::before{letter-spacing:-.2em;content:"\00a0"}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f6f8fa;border-radius:3px}.markdown-body pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code::after,.markdown-body pre code::before{content:normal}.markdown-body .full-commit .btn-outline:not(:disabled):hover{color:#005cc5;border-color:#005cc5}.markdown-body kbd{display:inline-block;padding:3px 5px;font:11px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fcfcfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body :checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item+.task-list-item{margin-top:3px}.markdown-body .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown-body hr{border-bottom-color:#eee}.flash{position:relative;padding:16px;color:#246;background-color:#e2eef9;border:1px solid #bac6d3;border-radius:3px}.flash p:last-child{margin-bottom:0}.flash-messages{margin-bottom:24px}.flash-warn{color:#4c4a42;background-color:#fff9ea;border-color:#dfd8c2}.flash-error{color:#911;background-color:#fcdede;border-color:#d2b2b2}.flash-success{color:#22662c;background-color:#e2f9e5;border-color:#bad3be}.flash-plain{color:#4c4a42;background-color:#f5f5f5;border-color:#c1c1c1}.figure{text-align:center;}</style>
  <style type="text/css">code{white-space: pre;}</style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {equationNumbers: {autoNumber: "AMS"}}
    });
  </script>
  <script src="../../../cdn.mathjax.org/mathjax/latest/MathJax2ba6.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title">Random Variables - Distribution and Expectation</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#random-variables-distribution-and-expectation">Random Variables: Distribution and Expectation</a><ul>
<li><a href="#random-variables">Random Variables</a></li>
<li><a href="#distribution">Distribution</a><ul>
<li><a href="#example-the-binomial-distribution">Example: The Binomial Distribution</a></li>
</ul></li>
<li><a href="#expectation">Expectation</a><ul>
<li><a href="#examples">Examples</a></li>
</ul></li>
<li><a href="#linearity-of-expectation">Linearity of Expectation</a><ul>
<li><a href="#examples-1">Examples</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<h1 id="random-variables-distribution-and-expectation" class="unnumbered">Random Variables: Distribution and Expectation</h1>
<p><span><strong>Example: Coin Flips</strong></span></p>
<p>Recall our setup of a probabilistic experiment as a procedure of drawing a sample from a set of possible values, and assigning a probability for each possible outcome of the experiment. For example, if we toss a fair coin <span class="math inline">\(n\)</span> times, then there are <span class="math inline">\(2^n\)</span> possible outcomes, each of which is equally likely and has probability <span class="math inline">\(2^{-n}\)</span>.</p>
<p>Now suppose we want to make a measurement in our experiment. For example, we can ask what is the number of heads in <span class="math inline">\(n\)</span> coin tosses; call this number <span class="math inline">\(X\)</span>. Of course, <span class="math inline">\(X\)</span> is not a fixed number, but it depends on the actual sequence of coin flips that we obtain. For example, if <span class="math inline">\(n = 4\)</span> and we observe the outcome <span class="math inline">\(\omega = HTHH\)</span>, then the number of heads is <span class="math inline">\(X = 3\)</span>; whereas if we observe the outcome <span class="math inline">\(\omega = HTHT\)</span>, then the number of heads is <span class="math inline">\(X = 2\)</span>. In this example of <span class="math inline">\(n\)</span> coin tosses we only know that <span class="math inline">\(X\)</span> is an integer between <span class="math inline">\(0\)</span> and <span class="math inline">\(n\)</span>, but we do not know what its exact value is until we observe which outcome of <span class="math inline">\(n\)</span> coin flips is realized and count how many heads there are. Because every possible outcome is assigned a probability, the value <span class="math inline">\(X\)</span> also carries with it a probability for each possible value it can take. The table below lists all the possible values <span class="math inline">\(X\)</span> can take in the example of <span class="math inline">\(n = 4\)</span> coin tosses, along with their respective probabilities.</p>
<table>
<thead>
<tr class="header">
<th align="center">outcomes <span class="math inline">\(\omega\)</span></th>
<th align="center">value of <span class="math inline">\(X\)</span> (number of heads)</th>
<th align="center">probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(TTTT\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(1/16\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(HTTT, THTT, TTHT, TTTH\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(4/16\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(HHTT, HTHT, HTTH, THHT, THTH, TTHH\)</span></td>
<td align="center"><span class="math inline">\(2\)</span></td>
<td align="center"><span class="math inline">\(6/16\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(HHHT, HHTH, HTHH, THHH\)</span></td>
<td align="center"><span class="math inline">\(3\)</span></td>
<td align="center"><span class="math inline">\(4/16\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(HHHH\)</span></td>
<td align="center"><span class="math inline">\(4\)</span></td>
<td align="center"><span class="math inline">\(1/16\)</span></td>
</tr>
</tbody>
</table>
<p>Such a value <span class="math inline">\(X\)</span> that depends on the outcome of the probabilistic experiment is called a <span><em>random variable</em></span> (or <span><em>r.v.</em></span>). As we see from the example above, <span class="math inline">\(X\)</span> does not have a definitive value, but instead only has a probability <span><em>distribution</em></span> over the set of possible values <span class="math inline">\(X\)</span> can take, which is why it is called random. So the question “What is the number of heads in <span class="math inline">\(n\)</span> coin tosses?” does not exactly make sense because the answer <span class="math inline">\(X\)</span> is a random variable. But the question “What is the <span><em>typical</em></span> number of heads in <span class="math inline">\(n\)</span> coin tosses?” makes sense: it is asking what is the average value of <span class="math inline">\(X\)</span> (the number of heads) if we repeat the experiment of tossing <span class="math inline">\(n\)</span> coins multiple times. This average value is called the <span><em>expectation</em></span> of <span class="math inline">\(X\)</span>, and is one of the most useful summary (also called <span><em>statistics</em></span>) of an experiment.</p>
<p><span><strong>Example: Permutations</strong></span></p>
<p>Before we formalize all these notions, let us consider another example to enforce our conceptual understanding of a random variable. Suppose we collect the homeworks of <span class="math inline">\(n\)</span> students, randomly shuffle them, and return them to the students. How many students receive their own homework?</p>
<p>Here the probability space consists of all <span class="math inline">\(n!\)</span> permutations of the homeworks, each with equal probability <span class="math inline">\(1/n!\)</span>. If we label the homeworks as <span class="math inline">\(1,2,\dots,n\)</span>, then each sample point is a permutation <span class="math inline">\(\pi = (\pi_1,\dots,\pi_n)\)</span> where <span class="math inline">\(\pi_i\)</span> is the homework that is returned to the <span class="math inline">\(i\)</span>-th student. Note that <span class="math inline">\(\pi_1,\dots,\pi_n \in \{1,2,\dots,n\}\)</span> are all distinct, so each element in <span class="math inline">\(\{1,\dots,n\}\)</span> appears exactly once in the permutation <span class="math inline">\(\pi\)</span>.</p>
<p>In this setting, the <span class="math inline">\(i\)</span>-th student receives her own homework if and only if <span class="math inline">\(\pi_i = i\)</span>. Then the question “How many students receive their own homework?” translates into the question of how many indices <span class="math inline">\(i\)</span>’s satisfy <span class="math inline">\(\pi_i = i\)</span>. These are known as <em>fixed points</em> of the permutation. As in the coin flipping case above, our question does not have a simple numerical answer (such as <span class="math inline">\(4\)</span>), because the number depends on the particular permutation we choose (i.e., on the sample point). Let’s call the number of fixed points <span class="math inline">\(X\)</span>, which is a random variable.</p>
<p>To illustrate the idea concretely, let’s consider the example <span class="math inline">\(n = 3\)</span>. The following table gives a complete listing of the sample space (of size <span class="math inline">\(3!=6\)</span>), together with the corresponding value of <span class="math inline">\(X\)</span> for each sample point. Here we see that <span class="math inline">\(X\)</span> takes on values <span class="math inline">\(0\)</span>, <span class="math inline">\(1\)</span>, or <span class="math inline">\(3\)</span>, depending on the sample point. You should check that you agree with this table.</p>
<table>
<thead>
<tr class="header">
<th align="center">permutation <span class="math inline">\(\pi\)</span></th>
<th align="center">value of <span class="math inline">\(X\)</span> (number of fixed points)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\((1, 2, 3)\)</span></td>
<td align="center"><span class="math inline">\(3\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\((1, 3, 2)\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\((2, 1, 3)\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\((2, 3, 1)\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\((3, 1, 2)\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\((3, 2, 1)\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
<h2 id="random-variables" class="unnumbered">Random Variables</h2>
<p>Let us formalize the concepts discussed above.</p>
<p><span id="def:rv" class="pandoc-numbering-text def"><strong>Definition 1</strong> <em>(Random Variable)</em></span></p>
<p><em>A random variable <span class="math inline">\(X\)</span> on a sample space <span class="math inline">\(\Omega\)</span> is a function <span class="math inline">\(X : \Omega \to \mathbb{R}\)</span> that assigns to each sample point <span class="math inline">\(\omega\in\Omega\)</span> a real number <span class="math inline">\(X(\omega)\)</span>.</em></p>
<p>Until further notice, we will restrict out attention to random variables that are <em>discrete</em>, i.e., they take values in a range that is finite or countably infinite. This means even though we define <span class="math inline">\(X\)</span> to map <span class="math inline">\(\Omega\)</span> to <span class="math inline">\(\mathbb{R}\)</span>, the actual set of values <span class="math inline">\(\{X(\omega) : \omega \in \Omega\}\)</span> that <span class="math inline">\(X\)</span> takes is a discrete subset of <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>A random variable can be visualized in general by the picture in Figure <a href="#fig:rv">1</a>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Note that the term “random variable” is really something of a misnomer: it is a function so there is nothing random about it and it is definitely not a variable! What is random is which sample point of the experiment is realized and hence the value that the random variable maps the sample point to.</p>
<div class="figure">
<img src="n16-rv.png" alt="Figure 1: Visualization of how a random variable is defined on the sample space." id="fig:rv" style="width:50.0%" />
<p class="caption">Figure 1: Visualization of how a random variable is defined on the sample space.</p>
</div>
<h2 id="distribution" class="unnumbered">Distribution</h2>
<p>When we introduced the basic probability space in Note 13, we defined two things:</p>
<ol style="list-style-type: decimal">
<li><p>the sample space <span class="math inline">\(\Omega\)</span> consisting of all the possible outcomes (sample points) of the experiment;</p></li>
<li><p>the probability of each of the sample points.</p></li>
</ol>
<p>Analogously, there are two things important about any random variable:</p>
<ol style="list-style-type: decimal">
<li><p>the set of values that it can take;</p></li>
<li><p>the probabilities with which it takes on the values.</p></li>
</ol>
<p>Since a random variable is defined on a probability space, we can calculate these probabilities given the probabilities of the sample points. Let <span class="math inline">\(a\)</span> be any number in the range of a random variable <span class="math inline">\(X\)</span>. Then the set <span class="math display">\[\{\omega\in\Omega:X(\omega)=a\}\]</span> is an <span><em>event</em></span> in the sample space (simply because it is a subset of <span class="math inline">\(\Omega\)</span>). We usually abbreviate this event to simply “<span class="math inline">\(X=a\)</span>”. Since <span class="math inline">\(X=a\)</span> is an event, we can talk about its probability, <span class="math inline">\({\mathbb{P}}[X=a]\)</span>. The collection of these probabilities, for all possible values of <span class="math inline">\(a\)</span>, is known as the <span><em>distribution</em></span> of the random variable <span class="math inline">\(X\)</span>.</p>
<p><span id="def:distribution" class="pandoc-numbering-text def"><strong>Definition 2</strong> <em>(Distribution)</em></span></p>
<p><em>The distribution of a discrete random variable <span class="math inline">\(X\)</span> is the collection of values <span class="math inline">\(\{(a,{\mathbb{P}}[X=a]):a\in\mathcal{A}\}\)</span>, where <span class="math inline">\(\mathcal{A}\)</span> is the set of all possible values taken by <span class="math inline">\(X\)</span>.</em></p>
<p>Thus, the distribution of the random variable <span class="math inline">\(X\)</span> in our permutation example above is: <span class="math display">\[{\mathbb{P}}[X=0]={1\over 3};\qquad{\mathbb{P}}[X=1]={1\over 2};\qquad{\mathbb{P}}[X=3]={1\over 6};\]</span> and <span class="math inline">\({\mathbb{P}}[X=a]=0\)</span> for all other values of <span class="math inline">\(a\)</span>.</p>
<p>The distribution of a random variable can be visualized as a bar diagram, shown in Figure <a href="#fig:dist">2</a>. The <span class="math inline">\(x\)</span>-axis represents the values that the random variable can take on. The height of the bar at a value <span class="math inline">\(a\)</span> is the probability <span class="math inline">\({\mathbb{P}}[X=a]\)</span>. Each of these probabilities can be computed by looking at the probability of the corresponding event in the sample space.</p>
<div class="figure">
<img src="n16-distribution.png" alt="Figure 2: Visualization of how the distribution of a random variable is defined." id="fig:dist" style="width:50.0%" />
<p class="caption">Figure 2: Visualization of how the distribution of a random variable is defined.</p>
</div>
<p>Note that the collection of events <span class="math inline">\(X=a\)</span>, <span class="math inline">\(a \in \mathcal{A}\)</span>, satisfy two important properties:</p>
<ul>
<li><p>Any two events <span class="math inline">\(X=a_1\)</span> and <span class="math inline">\(X= a_2\)</span> with <span class="math inline">\(a_1 \not = a_2\)</span> are disjoint.</p></li>
<li><p>The union of all these events is equal to the entire sample space <span class="math inline">\(\Omega\)</span>.</p></li>
</ul>
<p>The collection of events thus form a <span><em>partition</em></span> of the sample space (see Figure <a href="#fig:dist">2</a>). Both properties follow directly from the fact that <span class="math inline">\(X\)</span> is a function defined on <span class="math inline">\(\Omega\)</span>, i.e., <span class="math inline">\(X\)</span> assigns a unique value to each and every possible sample point in <span class="math inline">\(\Omega\)</span>. As a consequence, the sum of the probabilities <span class="math inline">\({\mathbb{P}}[X=a]\)</span> over all possible values of <span class="math inline">\(a\)</span> is exactly <span class="math inline">\(1\)</span>. So when we sum up the probabilities of the events <span class="math inline">\(X=a\)</span>, we are really summing up the probabilities of all the sample points.</p>
<h3 id="example-the-binomial-distribution" class="unnumbered">Example: The Binomial Distribution</h3>
<p>Let’s return to our coin tossing example above, where we defined our random variable <span class="math inline">\(X\)</span> to be the number of heads. More formally, consider the random experiment consisting of <span class="math inline">\(n\)</span> independent tosses of a biased coin which lands on heads with probability <span class="math inline">\(p\)</span>. Each sample point <span class="math inline">\(\omega\)</span> is a sequence of tosses. <span class="math inline">\(X(\omega)\)</span> is defined to be the number of heads in <span class="math inline">\(\omega\)</span>. For example, when <span class="math inline">\(n = 3\)</span>, <span class="math inline">\(X(THH) = 2\)</span>.</p>
<p>To compute the distribution of <span class="math inline">\(X\)</span>, we first enumerate the possible values <span class="math inline">\(X\)</span> can take on. They are simply <span class="math inline">\(0, 1, \ldots, n\)</span>. Then we compute the probability of each event <span class="math inline">\(X=i\)</span> for <span class="math inline">\(i =0, 1, \ldots, n\)</span>. The probability of the event <span class="math inline">\(X=i\)</span> is the sum of the probabilities of all the sample points with exactly <span class="math inline">\(i\)</span> heads (for example, if <span class="math inline">\(n=3\)</span> and <span class="math inline">\(i=2\)</span>, there would be three such sample points <span class="math inline">\(\{HHT, HTH, THH\}\)</span>). Any such sample point has a probability <span class="math inline">\(p^i(1-p)^{n-i}\)</span>, since the coin flips are independent. There are exactly <span class="math inline">\(n \choose i\)</span> of these sample points. So <span class="math display">\[{\mathbb{P}}[X=i] = {n \choose i} p^i(1-p)^{n-i} \qquad \text{ for } ~ i =0,1, \ldots, n.\]</span></p>
<p>This distribution, called the <span><em>binomial</em></span> distribution, is one of the most important distributions in probability. A random variable with this distribution is called a <span><em>binomial</em></span> random variable, written as <span class="math display">\[X \sim {\operatorname{Bin}}(n,p)\]</span> An example of a binomial distribution is shown in Figure <a href="#fig:binomial">3</a>. Notice that due to the properties of <span class="math inline">\(X\)</span> mentioned above, it must be the case that <span class="math inline">\(\sum_{i=0}^{n}{\mathbb{P}}[X=i] = 1\)</span>, which implies that <span class="math inline">\(\sum_{i=0}^{n}{n \choose i}p^i(1-p)^{n-i} = 1\)</span>. This provides a probabilistic proof of the Binomial Theorem!</p>
<div class="figure">
<img src="n16-binomial.png" alt="Figure 3: The binomial distributions for two choices of (n,p)." id="fig:binomial" style="width:80.0%" />
<p class="caption">Figure 3: The binomial distributions for two choices of <span class="math inline">\((n,p)\)</span>.</p>
</div>
<p>Although we define the binomial distribution in terms of an experiment involving tossing coins, this distribution is useful for modeling many real-world problems. Consider for example the error correction problem studied in Note 9. Recall that we wanted to encode <span class="math inline">\(n\)</span> packets into <span class="math inline">\(n+k\)</span> packets such that the recipient can reconstruct the original <span class="math inline">\(n\)</span> packets from any <span class="math inline">\(n\)</span> packets received. But in practice, the number of packet losses is random, so how do we choose <span class="math inline">\(k\)</span>, the amount of redundancy? If we model each packet getting lost with probability <span class="math inline">\(p\)</span> and the losses are independent, then if we transmit <span class="math inline">\(n+k\)</span> packets, the number of packets received is a random variable <span class="math inline">\(X\)</span> with binomial distribution: <span class="math inline">\(X \sim {\operatorname{Bin}}(n+k,1-p)\)</span> (we are tossing a coin <span class="math inline">\(n+k\)</span> times, and each coin turns out to be a head (packet received) with probability <span class="math inline">\(1-p\)</span>). So the probability of successfully decoding the original data is: <span class="math display">\[{\mathbb{P}}[X \ge n] ~=~ \sum_{i=n}^{n+k} {\mathbb{P}}[X = i] ~=~ \sum_{i=n}^{n+k} {n+k \choose i} (1-p)^i p^{n+k-i}.\]</span> Given fixed <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, we can choose <span class="math inline">\(k\)</span> such that this probability is no less than, say, <span class="math inline">\(0.99\)</span>.</p>
<h2 id="expectation" class="unnumbered">Expectation</h2>
<p>The distribution of a r.v. contains <span><em>all</em></span> the probabilistic information about the r.v. In most applications, however, the complete distribution of a r.v. is very hard to calculate. For example, consider the homework permutation example with <span class="math inline">\(n = 20\)</span>. In principle, we’d have to enumerate <span class="math inline">\(20!\approx 2.4\times10^{18}\)</span> sample points, compute the value of <span class="math inline">\(X\)</span> at each one, and count the number of points at which <span class="math inline">\(X\)</span> takes on each of its possible values! (Though in practice we could streamline this calculation a bit.) Moreover, even when we can compute the complete distribution of a r.v., it is often not very informative.</p>
<p>For these reasons, we seek to <span><em>compress</em></span> the distribution into a more compact, convenient form that is also easier to compute. The most widely used such form is the <span><em>expectation</em></span> (or <span><em>mean</em></span> or <span><em>average</em></span>) of the r.v.</p>
<p><span id="def:expectation" class="pandoc-numbering-text def"><strong>Definition 3</strong> <em>(Expectation)</em></span></p>
<p><em>The expectation of a discrete random variable <span class="math inline">\(X\)</span> is defined as <span class="math display">\[{\mathbb{E}}[X]=\sum_{a\in\mathcal{A}} a\times{\mathbb{P}}[X=a],\]</span> where the sum is over all possible values taken by the r.v.</em></p>
<p>For our simpler permutation example with only <span class="math inline">\(3\)</span> students, the expectation is <span class="math display">\[{\mathbb{E}}[X]=\left( 0\times{1\over 3}\right) +
          \left( 1\times{1\over 2}\right) +
          \left( 3\times{1\over 6}\right) =
          0+{1\over 2}+{1\over 2} = 1.\]</span> That is, the expected number of fixed points in a permutation of three items is exactly <span class="math inline">\(1\)</span>.</p>
<p>The expectation can be seen in some sense as a “typical” value of the r.v. (though note that it may not actually be a value that the r.v. ever takes on). The question of how typical the expectation is for a given r.v. is a very important one that we shall return to in a later lecture.</p>
<p>Here is a physical interpretation of the expectation of a random variable: imagine carving out a wooden cutout figure of the probability distribution as in Figure <a href="#fig:gravity">4</a>. Then the expected value of the distribution is the balance point (directly below the center of gravity) of this object.</p>
<div class="figure">
<img src="n16-gravity.png" alt="Figure 4: Physical interpretation of expected value as the balance point." id="fig:gravity" style="width:50.0%" />
<p class="caption">Figure 4: Physical interpretation of expected value as the balance point.</p>
</div>
<h3 id="examples" class="unnumbered">Examples</h3>
<ol style="list-style-type: decimal">
<li><p><span><strong>Single die.</strong></span> Throw one fair die. Let <span class="math inline">\(X\)</span> be the number that comes up. Then <span class="math inline">\(X\)</span> takes on values <span class="math inline">\(1,2,\ldots,6\)</span> each with probability <span class="math inline">\(1/6\)</span>, so <span class="math display">\[{\mathbb{E}}[X]={1\over 6}(1+2+3+4+5+6)={{21}\over 6} = {7\over 2}.\]</span> Note that <span class="math inline">\(X\)</span> never actually takes on its expected value <span class="math inline">\(7/2\)</span>.</p></li>
<li><p><span><strong>Two dice.</strong></span> Throw two fair dice. Let <span class="math inline">\(X\)</span> be the sum of their scores. Then the distribution of <span class="math inline">\(X\)</span> is: <span class="math display">\[\begin{array}{|c|ccccccccccc|}
\hline
a &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; 11 &amp; 12 \\
\hline
{\mathbb{P}}[X = a] &amp; 1/36 &amp; 1/18 &amp; 1/12 &amp; 1/9 &amp; 5/36 &amp; 1/6 &amp; 5/36 &amp; 1/9 &amp; 1/12 &amp; 1/18 &amp; 1/36 \\
\hline
\end{array}\]</span> The expectation is therefore <span class="math display">\[{\mathbb{E}}[X]=\left(2\times{1\over{36}}\right)+\left(3\times{1\over{18}}\right)
          +\left(4\times{1\over{12}}\right)+\cdots+\left(12\times{1\over{36}}\right)=7.\]</span></p></li>
<li><p><span><strong>Roulette.</strong></span> A roulette wheel is spun (recall that a roulette wheel has <span class="math inline">\(38\)</span> slots: the numbers <span class="math inline">\(1,2,\ldots,36\)</span>, half of which are red and half black, plus <span class="math inline">\(0\)</span> and <span class="math inline">\(00\)</span>, which are green). You bet <span class="math inline">\(\$1\)</span> on Black. If a black number comes up, you receive your stake plus <span class="math inline">\(\$1\)</span>; otherwise you lose your stake. Let <span class="math inline">\(X\)</span> be your net winnings in one game. Then <span class="math inline">\(X\)</span> can take on the values <span class="math inline">\(+1\)</span> and <span class="math inline">\(-1\)</span>, and <span class="math inline">\({\mathbb{P}}[X=1]=18/38\)</span>, <span class="math inline">\({\mathbb{P}}[X=-1]=20/38\)</span>. Thus, <span class="math display">\[{\mathbb{E}}[X] = \left(1\times{{18}\over{38}}\right) + \left(-1\times{{20}\over{38}}\right) = -{1\over{19}};\]</span> i.e., you expect to lose about a nickel per game. Notice how the zeros tip the balance in favor of the casino!</p></li>
</ol>
<h2 id="linearity-of-expectation" class="unnumbered">Linearity of Expectation</h2>
<p>So far, we’ve computed expectations by brute force: i.e., we have written down the whole distribution and then added up the contributions for all possible values of the r.v. The real power of expectations is that in many real-life examples they can be computed much more easily using a simple shortcut. The shortcut is the following:</p>
<p><span id="thm:lin" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></p>
<p><em>For any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> on the same probability space, we have <span class="math display">\[{\mathbb{E}}[X+Y]={\mathbb{E}}[X]+{\mathbb{E}}[Y].\]</span> Also, for any constant <span class="math inline">\(c\)</span>, we have <span class="math display">\[{\mathbb{E}}[cX] = c{\mathbb{E}}[X].\]</span></em></p>
<p><em>Proof</em>. To make the proof easier, we will first rewrite the definition of expectation in a more convenient form. Recall from <a href="#def:expectation">Definition 3</a> that <span class="math display">\[{\mathbb{E}}[X]=\sum_{a\in\mathcal{A}} a\times{\mathbb{P}}[X=a].\]</span> Let’s look at one term <span class="math inline">\(a\times{\mathbb{P}}[X=a]\)</span> in the above sum. Notice that <span class="math inline">\({\mathbb{P}}[X=a]\)</span>, by definition, is the sum of <span class="math inline">\({\mathbb{P}}[\omega]\)</span> over those sample points <span class="math inline">\(\omega\)</span> for which <span class="math inline">\(X(\omega)=a\)</span>. And we know that every sample point <span class="math inline">\(\omega\in\Omega\)</span> is in exactly one of these events <span class="math inline">\(X=a\)</span>. This means we can write out the above definition in a more long-winded form as <a name="eq:newexp"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   {\mathbb{E}}[X]=\sum_{\omega\in\Omega} X(\omega)\times{\mathbb{P}}[\omega].\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(1)</span></span>  This equivalent definition of expectation will make the present proof much easier (though it is usually less convenient for actual calculations).</p>
<p>Now let’s write out <span class="math inline">\({\mathbb{E}}[X+Y]\)</span> using Equation <a href="#eq:newexp">1</a>: <span class="math display">\[\begin{aligned} {\mathbb{E}}[X+Y]&amp;=\sum_{\omega\in\Omega} (X+Y)(\omega)\times{\mathbb{P}}[\omega]\cr
                 &amp;=\sum_{\omega\in\Omega} (X(\omega)+Y(\omega))\times{\mathbb{P}}[\omega]\cr
                 &amp;=\sum_{\omega\in\Omega} \Bigl(X(\omega)\times{\mathbb{P}}[\omega]\Bigr) + \sum_{\omega\in\Omega} \Bigl(Y(\omega)\times{\mathbb{P}}[\omega]\Bigr)\cr
                 &amp;= {\mathbb{E}}[X] + {\mathbb{E}}[Y].\end{aligned}\]</span> In the last step, we used Equation <a href="#eq:newexp">1</a> twice.</p>
<p>This completes the proof of the first equality. The proof of the second equality is much simpler and is left as an exercise. <span class="math inline">\(\square\)</span></p>
<p><a href="#thm:lin">Theorem 1</a> is very powerful: it says that the expectation of a sum of r.v.’s is the sum of their expectations, <em>with no assumptions about the r.v.'s</em>. We can use <a href="#thm:lin">Theorem 1</a> to conclude things like <span class="math inline">\({\mathbb{E}}[3X-5Y]=3{\mathbb{E}}[X]-5{\mathbb{E}}[Y]\)</span>. This property is known as <em>linearity of expectation</em>.</p>
<p><span><em>Important caveat: <a href="#thm:lin">Theorem 1</a> does <span><strong>not</strong></span> say that <span class="math inline">\({\mathbb{E}}[XY]={\mathbb{E}}[X]{\mathbb{E}}[Y]\)</span>, or that <span class="math inline">\({\mathbb{E}}[1/X]=1/{\mathbb{E}}[X]\)</span>, etc. These claims are not true in general. It is only <span>sums</span> and <span>differences</span> and <span>constant multiples</span> of random variables that behave so nicely.</em></span></p>
<h3 id="examples-1" class="unnumbered">Examples</h3>
<p>Now let’s see some examples of <a href="#thm:lin">Theorem 1</a> in action.</p>
<ol style="list-style-type: decimal">
<li><p><span><strong>Two dice again.</strong></span> Here’s a much less painful way of computing <span class="math inline">\({\mathbb{E}}[X]\)</span>, where <span class="math inline">\(X\)</span> is the sum of the scores of the two dice. Note that <span class="math inline">\(X=X_1+X_2\)</span>, where <span class="math inline">\(X_i\)</span> is the score on die <span class="math inline">\(i\)</span>. We know from example 1 above that <span class="math inline">\({\mathbb{E}}[X_1]={\mathbb{E}}[X_2]=7/2\)</span>. So by <a href="#thm:lin">Theorem 1</a> we have <span class="math inline">\({\mathbb{E}}[X]={\mathbb{E}}[X_1]+{\mathbb{E}}[X_2]=7\)</span>.</p></li>
<li><p><span><strong>More roulette.</strong></span> Suppose we play the above roulette game not once, but <span class="math inline">\(1000\)</span> times. Let <span class="math inline">\(X\)</span> be our expected net winnings. Then <span class="math inline">\(X=X_1+X_2+\cdots+X_{1000}\)</span>, where <span class="math inline">\(X_i\)</span> is our net winnings in the <span class="math inline">\(i\)</span>th play. We know from earlier that <span class="math inline">\({\mathbb{E}}[X_i]=-1/19\)</span> for each <span class="math inline">\(i\)</span>. Therefore, by <a href="#thm:lin">Theorem 1</a>, <span class="math inline">\({\mathbb{E}}[X]={\mathbb{E}}[X_1]+{\mathbb{E}}[X_2]+\cdots+{\mathbb{E}}[X_{1000}]=1000\times(-1/19) =-1000/19\approx -53\)</span>. So if you play <span class="math inline">\(1000\)</span> games, you expect to lose about <span class="math inline">\(\$53\)</span>.</p></li>
<li><p><span><strong>Homeworks.</strong></span> Let’s go back to our homework permutation example with <span class="math inline">\(n = 20\)</span> students. Recall that the r.v. <span class="math inline">\(X\)</span> is the number of students who receive their own homework after shuffling (or equivalently, the number of fixed points). To take advantage of <a href="#thm:lin">Theorem 1</a>, we need to write <span class="math inline">\(X\)</span> as the <span><em>sum</em></span> of simpler r.v.’s. But since <span class="math inline">\(X\)</span> <span><em>counts</em></span> the number of times something happens, we can write it as a sum using the following trick: <a name="eq:hw"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   X=X_1+X_2+\cdots+X_{20},\qquad\text{where }
                  X_i=\begin{cases} 1 &amp; \text{if student $i$ gets her own hw}; \\
                       0 &amp; \text{otherwise}. \end{cases}\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(2)</span></span>  [You should think about this equation for a moment. Remember that all the <span class="math inline">\(X\)</span>’s are random variables. What does an equation involving random variables mean? What we mean is that, <span><em>at every sample point</em></span> <span class="math inline">\(\omega\)</span>, we have <span class="math inline">\(X(\omega)=X_1(\omega)+X_2(\omega)+\cdots+X_{20}(\omega)\)</span>. Why is this true?]</p>
<p>A <span class="math inline">\(\{0,1\}\)</span>-valued random variable such as <span class="math inline">\(X_i\)</span> is called an <em>indicator</em> random variable of the corresponding event (in this case, the event that student <span class="math inline">\(i\)</span> gets her own hw). For indicator r.v.’s, the expectation is particularly easy to calculate. Namely, <span class="math display">\[{\mathbb{E}}[X_i]=(0\times{\mathbb{P}}[X_i=0]) + (1\times{\mathbb{P}}[X_i=1]) = {\mathbb{P}}[X_i=1].\]</span> But in our case, we have <span class="math display">\[{\mathbb{P}}[X_i=1] = {\mathbb{P}}[\text{student $i$ gets her own hw}] = {1\over{20}}.\]</span> Now we can apply <a href="#thm:lin">Theorem 1</a> to Equation <a href="#eq:hw">2</a>, to get <span class="math display">\[{\mathbb{E}}[X]={\mathbb{E}}[X_1]+{\mathbb{E}}[X_2]+\cdots+{\mathbb{E}}[X_{20}]=20\times \frac{1}{20} = 1.\]</span> So we see that the expected number of students who get their own homeworks in a class of size <span class="math inline">\(20\)</span> is <span class="math inline">\(1\)</span>. But this is exactly the same answer as we got for a class of size <span class="math inline">\(3\)</span>! And indeed, we can easily see from the above calculation that we would get <span class="math inline">\({\mathbb{E}}[X]=1\)</span> for <span><em>any</em></span> class size <span class="math inline">\(n\)</span>: this is because we can write <span class="math inline">\(X=X_1+X_2+\cdots+X_n\)</span>, and <span class="math inline">\({\mathbb{E}}[X_i]=1/n\)</span> for each <span class="math inline">\(i\)</span>.</p>
<p>So <em>the expected number of fixed points in a random permutation of <span class="math inline">\(n\)</span> items is always <span class="math inline">\(1\)</span></em>, regardless of <span class="math inline">\(n\)</span>. Amazing, but true.</p></li>
<li><p><span><strong>Coin tosses.</strong></span> Toss a fair coin <span class="math inline">\(100\)</span> times. Let the r.v. <span class="math inline">\(X\)</span> be the number of Heads. As in the previous example, to take advantage of <a href="#thm:lin">Theorem 1</a> we write <span class="math display">\[X=X_1+X_2+\cdots+X_{100},\]</span> where <span class="math inline">\(X_i\)</span> is the indicator r.v. of the event that the <span class="math inline">\(i\)</span>-th toss is Heads. Since the coin is fair, we have <span class="math display">\[{\mathbb{E}}[X_i]={\mathbb{P}}[X_i=1]={\mathbb{P}}[\text{$i$-th toss is Heads}]=\frac{1}{2}.\]</span> Using <a href="#thm:lin">Theorem 1</a>, we therefore get <span class="math display">\[{\mathbb{E}}[X] = \sum_{i=1}^{100} \frac{1}{2} = 100\times \frac{1}{2} = 50.\]</span> More generally, the expected number of Heads in <span class="math inline">\(n\)</span> tosses of a fair coin is <span class="math inline">\(n/2\)</span>. And in <span class="math inline">\(n\)</span> tosses of a biased coin with Heads probability <span class="math inline">\(p\)</span>, it is <span class="math inline">\(np\)</span> (why?). So the expectation of a binomial r.v. <span class="math inline">\(X \sim {\operatorname{Bin}}(n,p)\)</span> is equal to <span class="math inline">\(np\)</span>. Note that it would have been harder to reach the same conclusion by computing this directly from definition of expectation.</p></li>
<li><p><span><strong>Balls and bins.</strong></span> Throw <span class="math inline">\(m\)</span> balls into <span class="math inline">\(n\)</span> bins. Let the r.v. <span class="math inline">\(X\)</span> be the number of balls that land in the first bin. Then <span class="math inline">\(X\)</span> behaves exactly like the number of Heads in <span class="math inline">\(m\)</span> tosses of a biased coin, with Heads probability <span class="math inline">\(1/n\)</span> (why?). So we get <span class="math inline">\({\mathbb{E}}[X]=m/n\)</span>.</p>
<p>In the special case <span class="math inline">\(m=n\)</span>, the expected number of balls in any bin is <span class="math inline">\(1\)</span>. If we wanted to compute this directly from the distribution of <span class="math inline">\(X\)</span>, we’d get into a messy calculation involving binomial coefficients.</p>
<p>Here’s another example on the same sample space. Let the r.v. <span class="math inline">\(Y\)</span> be the number of empty bins. The distribution of <span class="math inline">\(Y\)</span> is horrible to contemplate: to get a feel for this, you might like to write it down for <span class="math inline">\(m=n=3\)</span> (<span class="math inline">\(3\)</span> balls, <span class="math inline">\(3\)</span> bins). However, computing the expectation <span class="math inline">\({\mathbb{E}}[Y]\)</span> is easy using <a href="#thm:lin">Theorem 1</a>. As usual, let’s write <a name="eq:bbapprox"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[
   Y=Y_1+Y_2+\cdots+Y_n,\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(3)</span></span>  where <span class="math inline">\(Y_i\)</span> is the indicator r.v. of the event “bin <span class="math inline">\(i\)</span> is empty”. Again as usual, the expectation of <span class="math inline">\(Y_i\)</span> is easy: <span class="math display">\[{\mathbb{E}}[Y_i] = {\mathbb{P}}[Y_i=1] = {\mathbb{P}}[\text{bin $i$ is empty}] =
                            \left(1-{1\over n}\right)^m;\]</span> recall that we computed this probability (quite easily) in an earlier lecture. Applying <a href="#thm:lin">Theorem 1</a> to Equation <a href="#eq:bbapprox">3</a> we therefore have <span class="math display">\[{\mathbb{E}}[Y] = \sum_{i=1}^n{\mathbb{E}}[Y_i]=n\left(1-{1\over n}\right)^m,\]</span> a very simple formula, very easily derived.</p>
<p>Let’s see how it behaves in the special case <span class="math inline">\(m=n\)</span> (same number of balls as bins). In this case we get <span class="math inline">\({\mathbb{E}}[Y]=n(1-1/n)^n\)</span>. Now the quantity <span class="math inline">\((1-1/n)^n\)</span> can be approximated (for large enough values of <span class="math inline">\(n\)</span>) by the number <span class="math inline">\(1/e\)</span>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> So we see that <span class="math display">\[{\mathbb{E}}[Y]\to{n\over{e}}\approx 0.368n\quad\text{as $n\to\infty$}.\]</span></p>
<p>The bottom line is that, if we throw (say) <span class="math inline">\(1000\)</span> balls into <span class="math inline">\(1000\)</span> bins, the expected number of empty bins is about <span class="math inline">\(368\)</span>.</p></li>
</ol>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The figures in this note are inspired by figures in Chapter 2 of “Introduction to Probability” by D. Bertsekas and J. Tsitsiklis.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>More generally, it is a standard fact that for any constant <span class="math inline">\(c\)</span>, <span class="math display">\[\left(1+\frac{c}{n}\right)^n\to e^c\quad\text{as $n\to\infty$}.\]</span> We just used this fact in the special case <span class="math inline">\(c=-1\)</span>. The approximation is actually very good even for quite small values of <span class="math inline">\(n\)</span> — try it yourself! For example, for <span class="math inline">\(n=20\)</span> we already get <span class="math inline">\((1-1/n)^n=0.358\)</span>, which is very close to <span class="math inline">\(1/e=0.367\ldots\)</span>. The approximation gets better and better for larger <span class="math inline">\(n\)</span>.<a href="#fnref2">↩</a></p></li>
</ol>
</div>
</article>
</body>

<!-- Mirrored from www.eecs70.org/static/notes/n16.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 17 May 2018 00:40:58 GMT -->
</html>
